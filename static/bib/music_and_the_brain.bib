
@article{bartlettOrganizationPhysiologyAuditory2013a,
  title = {The Organization and Physiology of the Auditory Thalamus and Its Role in Processing Acoustic Features Important for Speech Perception.},
  volume = {126},
  issn = {10902155},
  abstract = {The auditory thalamus, or medial geniculate body (MGB), is the primary sensory input to auditory cortex. Therefore, it plays a critical role in the complex auditory processing necessary for robust speech perception. This review will describe the functional organization of the thalamus as it relates to processing acoustic features important for speech perception, focusing on thalamic nuclei that relate to auditory representations of language sounds. The MGB can be divided into three main subdivisions, the ventral, dorsal, and medial subdivisions, each with different connectivity, auditory response properties, neuronal properties, and synaptic properties. Together, the MGB subdivisions actively and dynamically shape complex auditory processing and form ongoing communication loops with auditory cortex and subcortical structures. Copyright \textcopyright{} 2013 Elsevier Inc. All rights reserved.},
  number = {1},
  journal = {Brain and language},
  doi = {10.1016/j.bandl.2013.03.003},
  url = {http://dx.doi.org/10.1016/j.bandl.2013.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0093934X13000722},
  author = {Bartlett, Edward L.},
  month = jul,
  year = {2013},
  pages = {29--48},
  file = {/Users/jonny/Papers/BartlettE/2013/Bartlett_2013_The organization and physiology of the auditory thalamus and its role in2.pdf},
  pmid = {23725661}
}

@article{Wang2013a,
  title = {The Harmonic Organization of Auditory Cortex.},
  volume = {7},
  issn = {1662-5137},
  abstract = {A fundamental structure of sounds encountered in the natural environment is the harmonicity. Harmonicity is an essential component of music found in all cultures. It is also a unique feature of vocal communication sounds such as human speech and animal vocalizations. Harmonics in sounds are produced by a variety of acoustic generators and reflectors in the natural environment, including vocal apparatuses of humans and animal species as well as music instruments of many types. We live in an acoustic world full of harmonicity. Given the widespread existence of the harmonicity in many aspects of the hearing environment, it is natural to expect that it be reflected in the evolution and development of the auditory systems of both humans and animals, in particular the auditory cortex. Recent neuroimaging and neurophysiology experiments have identified regions of non-primary auditory cortex in humans and non-human primates that have selective responses to harmonic pitches. Accumulating evidence has also shown that neurons in many regions of the auditory cortex exhibit characteristic responses to harmonically related frequencies beyond the range of pitch. Together, these findings suggest that a fundamental organizational principle of auditory cortex is based on the harmonicity. Such an organization likely plays an important role in music processing by the brain. It may also form the basis of the preference for particular classes of music and voice sounds.},
  language = {English},
  number = {December},
  urldate = {2016-03-17},
  journal = {Frontiers in systems neuroscience},
  doi = {10.3389/fnsys.2013.00114},
  url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3865599\&tool=pmcentrez\&rendertype=abstract},
  author = {Wang, Xiaoqin},
  month = dec,
  year = {2013},
  keywords = {auditory cortex,pitch,music,a harmonic is a,component frequency of a,environment,for example,fundamental frequency,harmonicity,harmonicity; auditory cortex; marmoset; pitch; mus,harmonics in the hearing,if the,integer multiple of a,marmoset,sound that is an},
  pages = {114},
  pmid = {24381544}
}

@article{bendixenRegularityExtractionNonAdjacent2012a,
  title = {Regularity {{Extraction}} from {{Non}}-{{Adjacent Sounds}}},
  volume = {3},
  issn = {1664-1078},
  language = {en},
  urldate = {2018-11-08},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2012.00143},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00143/abstract},
  author = {Bendixen, Alexandra and Schr{\"o}ger, Erich and Ritter, Walter and Winkler, Istv{\'a}n},
  year = {2012},
  file = {/Users/jonny/Papers/BendixenA/2012/Bendixen_2012_Regularity Extraction from Non-Adjacent Sounds.pdf}
}

@article{winerAuditoryThalamocorticalTransformation2005b,
  title = {Auditory Thalamocortical Transformation: Structure and Function},
  volume = {28},
  issn = {01662236},
  shorttitle = {Auditory Thalamocortical Transformation},
  language = {en},
  number = {5},
  urldate = {2018-11-09},
  journal = {Trends in Neurosciences},
  doi = {10.1016/j.tins.2005.03.009},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0166223605000822},
  author = {Winer, Jeffery A. and Miller, Lee M. and Lee, Charles C. and Schreiner, Christoph E.},
  month = may,
  year = {2005},
  pages = {255-263},
  file = {/Users/jonny/Papers/WinerJ/2005/Winer_2005_Auditory thalamocortical transformation.pdf}
}

@article{winklerModelingAuditoryScene2009b,
  title = {Modeling the Auditory Scene: Predictive Regularity Representations and Perceptual Objects},
  volume = {13},
  issn = {13646613},
  shorttitle = {Modeling the Auditory Scene},
  language = {en},
  number = {12},
  urldate = {2018-11-09},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2009.09.003},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661309002095},
  author = {Winkler, Istv{\'a}n and Denham, Susan L. and Nelken, Israel},
  month = dec,
  year = {2009},
  pages = {532-540},
  file = {/Users/jonny/Papers/WinklerI/2009/Winkler_2009_Modeling the auditory scene.pdf}
}

@article{zatorreWhenBrainPlays2007a,
  title = {When the Brain Plays Music: Auditory\textendash{}Motor Interactions in Music Perception and Production},
  volume = {8},
  issn = {1471-003X, 1471-0048},
  shorttitle = {When the Brain Plays Music},
  abstract = {Music performance is both a natural human activity, present in all societies, and one of the most complex and demanding cognitive challenges that the human mind can undertake. Unlike most other sensory\textendash{}motor activities, music performance requires precise timing of several hierarchically organized actions, as well as precise control over pitch interval production, implemented through diverse effectors according to the instrument involved. We review the cognitive neuroscience literature of both motor and auditory domains, highlighting the value of studying interactions between these systems in a musical context, and propose some ideas concerning the role of the premotor cortex in integration of higher order features of music with appropriately timed and organized actions.},
  language = {en},
  number = {7},
  urldate = {2018-11-09},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn2152},
  url = {http://www.nature.com/articles/nrn2152},
  author = {Zatorre, Robert J. and Chen, Joyce L. and Penhune, Virginia B.},
  month = jul,
  year = {2007},
  pages = {547-558},
  file = {/Users/jonny/Papers/ZatorreR/2007/Zatorre_2007_When the brain plays music.pdf}
}

@article{gamezAmplitudePeriodicNeural2019,
  title = {The Amplitude in Periodic Neural State Trajectories Underlies the Tempo of Rhythmic Tapping},
  volume = {17},
  issn = {1545-7885},
  abstract = {Our motor commands can be exquisitely timed according to the demands of the environment, and the ability to generate rhythms of different tempos is a hallmark of musical cognition. Yet, the neuronal underpinnings behind rhythmic tapping remain elusive. Here, we found that the activity of hundreds of primate medial premotor cortices (MPCs; pre-supplementary motor area [preSMA] and supplementary motor area [SMA]) neurons show a strong periodic pattern that becomes evident when their responses are projected into a state space using dimensionality reduction analysis. We show that different tapping tempos are encoded by circular trajectories that travelled at a constant speed but with different radii, and that this neuronal code is highly resilient to the number of participating neurons. Crucially, the changes in the amplitude of the oscillatory dynamics in neuronal state space are a signature of duration encoding during rhythmic timing, regardless of whether it is guided by an external metronome or is internally controlled and is not the result of repetitive motor commands. This dynamic state signal predicted the duration of the rhythmically produced intervals on a trial-by-trial basis. Furthermore, the increase in variability of the neural trajectories accounted for the scalar property, a hallmark feature of temporal processing across tasks and species. Finally, we found that the interval-dependent increments in the radius of periodic neural trajectories are the result of a larger number of neurons engaged in the production of longer intervals. Our results support the notion that rhythmic timing during tapping behaviors is encoded in the radial curvature of periodic MPC neural population trajectories.},
  language = {en},
  number = {4},
  urldate = {2019-06-12},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.3000054},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000054},
  author = {G{\'a}mez, Jorge and Mendoza, Germ{\'a}n and Prado, Luis and Betancourt, Abraham and Merchant, Hugo},
  month = apr,
  year = {2019},
  keywords = {Neurons,Population dynamics,Neural networks,Kinematics,Monkeys,Music perception,Principal component analysis,Tangents},
  pages = {e3000054},
  file = {/Users/jonny/Zotero/storage/PIS3MPJ2/GÃ¡mez et al. - 2019 - The amplitude in periodic neural state trajectorie.pdf;/Users/jonny/Zotero/storage/3YIU3NLX/article.html}
}

@article{liuNeuralCorrelatesLyrical2012,
  title = {Neural {{Correlates}} of {{Lyrical Improvisation}}: {{An fMRI Study}} of {{Freestyle Rap}}},
  volume = {2},
  copyright = {2012 Nature Publishing Group},
  issn = {2045-2322},
  shorttitle = {Neural {{Correlates}} of {{Lyrical Improvisation}}},
  abstract = {The neural correlates of creativity are poorly understood. Freestyle rap provides a unique opportunity to study spontaneous lyrical improvisation, a multidimensional form of creativity at the interface of music and language. Here we use functional magnetic resonance imaging to characterize this process. Task contrast analyses indicate that improvised performance is characterized by dissociated activity in medial and dorsolateral prefrontal cortices, providing a context in which stimulus-independent behaviors may unfold in the absence of conscious monitoring and volitional control. Connectivity analyses reveal widespread improvisation-related correlations between medial prefrontal, cingulate motor, perisylvian cortices and amygdala, suggesting the emergence of a network linking motivation, language, affect and movement. Lyrical improvisation appears to be characterized by altered relationships between regions coupling intention and action, in which conventional executive control may be bypassed and motor control directed by cingulate motor mechanisms. These functional reorganizations may facilitate the initial improvisatory phase of creative behavior.},
  language = {en},
  urldate = {2019-06-18},
  journal = {Scientific Reports},
  doi = {10.1038/srep00834},
  url = {https://www.nature.com/articles/srep00834},
  author = {Liu, Siyuan and Chow, Ho Ming and Xu, Yisheng and Erkkinen, Michael G. and Swett, Katherine E. and Eagle, Michael W. and {Rizik-Baer}, Daniel A. and Braun, Allen R.},
  month = nov,
  year = {2012},
  pages = {834},
  file = {/Users/jonny/Papers/LiuS/2012/Liu_2012_Neural Correlates of Lyrical Improvisation.pdf;/Users/jonny/Zotero/storage/CNLL9X93/srep00834.html}
}

@article{trainorOriginsMusicAuditory2015b,
  title = {The Origins of Music in Auditory Scene Analysis and the Roles of Evolution and Culture in Musical Creation},
  volume = {370},
  issn = {0962-8436, 1471-2970},
  language = {en},
  number = {1664},
  urldate = {2019-06-22},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  doi = {10.1098/rstb.2014.0089},
  url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2014.0089},
  author = {Trainor, L. J.},
  month = feb,
  year = {2015},
  pages = {20140089-20140089},
  file = {/Users/jonny/Zotero/storage/YKC9EA5S/Trainor - 2015 - The origins of music in auditory scene analysis an.pdf}
}

@article{fitchBiologyEvolutionMusic2006a,
  title = {The Biology and Evolution of Music: {{A}} Comparative Perspective},
  volume = {100},
  issn = {00100277},
  shorttitle = {The Biology and Evolution of Music},
  language = {en},
  number = {1},
  urldate = {2019-06-23},
  journal = {Cognition},
  doi = {10.1016/j.cognition.2005.11.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027705002258},
  author = {Fitch, W. Tecumseh},
  month = may,
  year = {2006},
  pages = {173-215},
  file = {/Users/jonny/Zotero/storage/CMZY6BP2/Fitch - 2006 - The biology and evolution of music A comparative .pdf}
}

@misc{feynmanFeynmanLecturesPhysics1963,
  title = {The {{Feynman Lectures}} on {{Physics Vol}}. {{I Ch}}. 47: {{Sound}}. {{The}} Wave Equation},
  urldate = {2019-06-23},
  url = {http://www.feynmanlectures.caltech.edu/I_47.html},
  author = {Feynman, Richard},
  year = {1963},
  file = {/Users/jonny/Zotero/storage/KR8MIXDN/I_47.html}
}

@incollection{pulkkiCommunicationAcousticsChapter2015,
  title = {Communication {{Acoustics}}, {{Chapter}} 2: {{Physics}} of {{Sound}}},
  shorttitle = {Communication {{Acoustics}}},
  abstract = {In communication acoustics, the communication channel consists of a sound source, a channel (acoustic and/or electric) and finally the receiver: the human auditory system, a complex and intricate system that shapes the way sound is heard. Thus, when developing techniques in communication acoustics, such as in speech, audio and aided hearing, it is important to understand the timefrequencyspace resolution of hearing.  This book facilitates the readers understanding and development of speech and audio techniques based on our knowledge of the auditory perceptual mechanisms by introducing the physical, signal-processing and psychophysical background to communication acoustics. It then provides a detailed explanation of sound technologies where a human listener is involved, including audio and speech techniques, sound quality measurement, hearing aids and audiology.  Key features:    Explains perceptually-based audio: the authors take a detailed but accessible engineering perspective on sound and hearing with a focus on the human place in the audio communications signal chain, from psychoacoustics and audiology to optimizing digital signal processing for human listening.  Presents a wide overview of speech, from the human production of speech sounds and basics of phonetics to major speech technologies, recognition and synthesis of speech and methods for speech quality evaluation.  Includes MATLAB examples that serve as an excellent basis for the readers own investigations into communication acoustics interaction schemes which intuitively combine touch, vision and voice for lifelike interactions.},
  language = {en-us},
  urldate = {2019-06-23},
  booktitle = {Wiley.Com},
  url = {https://www.wiley.com/en-us/Communication+Acoustics\%3A+An+Introduction+to+Speech\%2C+Audio+and+Psychoacoustics-p-9781118866542},
  author = {Pulkki, Ville},
  year = {2015},
  pages = {15-42},
  file = {/Users/jonny/Zotero/storage/FPNFNQJ3/Communication+Acoustics+An+Introduction+to+Speech,+Audio+and+Psychoacoustics-p-9781118866542.html}
}

@misc{feynmanFeynmanLecturesPhysics1963a,
  title = {The {{Feynman Lectures}} on {{Physics Vol}}. {{I Ch}}. 49: {{Modes}}},
  urldate = {2019-06-23},
  url = {http://www.feynmanlectures.caltech.edu/I_49.html},
  author = {Feynman, Richard},
  year = {1963},
  file = {/Users/jonny/Zotero/storage/IPXB3VVT/I_49.html}
}

@article{iyerNovelRepresentationRhythmic1997,
  title = {A Novel Representation for Rhythmic Structure},
  journal = {Proceedings of the 23rd International Computer Music Conference},
  url = {http://melodi.ee.washington.edu/~bilmes/mypubs/iyer1997-rhythm.pdf},
  author = {Iyer, Vijay and Bilmes, Jeff and Wright, Matt and Wessel, David},
  year = {1997},
  pages = {97--100},
  file = {/Users/jonny/Papers/IyerV/1997/Iyer_1997_A novel representation for rhythmic structure.pdf}
}

@article{laudanskiStructuralTheoryPitch2014a,
  title = {A {{Structural Theory}} of {{Pitch}}},
  volume = {1},
  issn = {2373-2822},
  abstract = {Musical notes can be ordered from low to high along a perceptual dimension called ``pitch''. A characteristic property of these sounds is their periodic waveform, and periodicity generally correlates with pitch. Thus, pitch is often described as the perceptual correlate of the periodicity of the sound's waveform. However, the existence and salience of pitch also depends in a complex way on other factors, in particular harmonic content. For example, periodic sounds made of high-order harmonics tend to have a weaker pitch than those made of low-order harmonics. Here we examine the theoretical proposition that pitch is the perceptual correlate of the regularity structure of the vibration pattern of the basilar membrane, across place and time\textemdash{}a generalization of the traditional view on pitch. While this proposition also attributes pitch to periodic sounds, we show that it predicts differences between resolved and unresolved harmonic complexes and a complex domain of existence of pitch, in agreement with psychophysical experiments. We also present a possible neural mechanism for pitch estimation based on coincidence detection, which does not require long delays, in contrast with standard temporal models of pitch.},
  language = {en},
  number = {1},
  urldate = {2019-06-23},
  journal = {eNeuro},
  doi = {10.1523/ENEURO.0033-14.2014},
  url = {http://eneuro.org/cgi/doi/10.1523/ENEURO.0033-14.2014},
  author = {Laudanski, J. and Zheng, Y. and Brette, R.},
  month = dec,
  year = {2014},
  pages = {ENEURO.0033-14.2014},
  file = {/Users/jonny/Zotero/storage/QDGXB9XD/Laudanski et al. - 2014 - A Structural Theory of Pitch.pdf}
}

@article{ekdaleFormFunctionMammalian2016,
  title = {Form and Function of the Mammalian Inner Ear},
  volume = {228},
  issn = {0021-8782},
  abstract = {The inner ear of mammals consists of the cochlea, which is involved with the sense of hearing, and the vestibule and three semicircular canals, which are involved with the sense of balance. Although different regions of the inner ear contribute to different functions, the bony chambers and membranous ducts are morphologically continuous. The gross anatomy of the cochlea that has been related to auditory physiologies includes overall size of the structure, including volume and total spiral length, development of internal cochlear structures, including the primary and secondary bony laminae, morphology of the spiral nerve ganglion, and the nature of cochlear coiling, including total number of turns completed by the cochlear canal and the relative diameters of the basal and apical turns. The overall sizes, shapes, and orientations of the semicircular canals are related to sensitivity to head rotations and possibly locomotor behaviors. Intraspecific variation, primarily in the shape and orientation of the semicircular canals, may provide additional clues to help us better understand form and function of the inner ear.},
  number = {2},
  urldate = {2019-06-23},
  journal = {Journal of Anatomy},
  doi = {10.1111/joa.12308},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4718163/},
  author = {Ekdale, Eric G.},
  month = feb,
  year = {2016},
  pages = {324-337},
  file = {/Users/jonny/Papers/EkdaleE/2016/Ekdale_2016_Form and function of the mammalian inner ear.pdf},
  pmid = {25911945},
  pmcid = {PMC4718163}
}

@article{koelschBrainCorrelatesMusicevoked2014,
  title = {Brain Correlates of Music-Evoked Emotions},
  volume = {15},
  copyright = {2014 Nature Publishing Group},
  issn = {1471-0048},
  abstract = {Music is a universal feature of human societies, partly owing to its power to evoke strong emotions and influence moods. During the past decade, the investigation of the neural correlates of music-evoked emotions has been invaluable for the understanding of human emotion. Functional neuroimaging studies on music and emotion show that music can modulate activity in brain structures that are known to be crucially involved in emotion, such as the amygdala, nucleus accumbens, hypothalamus, hippocampus, insula, cingulate cortex and orbitofrontal cortex. The potential of music to modulate activity in these structures has important implications for the use of music in the treatment of psychiatric and neurological disorders.},
  language = {en},
  number = {3},
  urldate = {2019-06-23},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn3666},
  url = {https://www.nature.com/articles/nrn3666},
  author = {Koelsch, Stefan},
  month = mar,
  year = {2014},
  pages = {170-180},
  file = {/Users/jonny/Zotero/storage/AE5LLJ4R/nrn3666.html}
}

@article{hickokCorticalOrganizationSpeech2007a,
  title = {The Cortical Organization of Speech Processing},
  volume = {8},
  copyright = {2007 Nature Publishing Group},
  issn = {1471-0048},
  abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized \textemdash{} although there are important computational differences between the left- and right-hemisphere systems \textemdash{} and that the dorsal stream is strongly left-hemisphere dominant.},
  language = {en},
  number = {5},
  urldate = {2019-06-23},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn2113},
  url = {https://www.nature.com/articles/nrn2113},
  author = {Hickok, Gregory and Poeppel, David},
  month = may,
  year = {2007},
  pages = {393-402},
  file = {/Users/jonny/Zotero/storage/Q2I6XTRY/nrn2113.html}
}

@article{pachitariuStateDependentPopulationCoding2015,
  title = {State-{{Dependent Population Coding}} in {{Primary Auditory Cortex}}},
  volume = {35},
  copyright = {Copyright \textcopyright{} 2015 Pachitariu et al.. This article is freely available online through the J Neurosci Author Open Choice option.},
  issn = {0270-6474, 1529-2401},
  abstract = {Sensory function is mediated by interactions between external stimuli and intrinsic cortical dynamics that are evident in the modulation of evoked responses by cortical state. A number of recent studies across different modalities have demonstrated that the patterns of activity in neuronal populations can vary strongly between synchronized and desynchronized cortical states, i.e., in the presence or absence of intrinsically generated up and down states. Here we investigated the impact of cortical state on the population coding of tones and speech in the primary auditory cortex (A1) of gerbils, and found that responses were qualitatively different in synchronized and desynchronized cortical states. Activity in synchronized A1 was only weakly modulated by sensory input, and the spike patterns evoked by tones and speech were unreliable and constrained to a small range of patterns. In contrast, responses to tones and speech in desynchronized A1 were temporally precise and reliable across trials, and different speech tokens evoked diverse spike patterns with extremely weak noise correlations, allowing responses to be decoded with nearly perfect accuracy. Restricting the analysis of synchronized A1 to activity within up states yielded similar results, suggesting that up states are not equivalent to brief periods of desynchronization. These findings demonstrate that the representational capacity of A1 depends strongly on cortical state, and suggest that cortical state should be considered as an explicit variable in all studies of sensory processing.},
  language = {en},
  number = {5},
  urldate = {2019-06-23},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.3318-14.2015},
  url = {http://www.jneurosci.org/content/35/5/2058},
  author = {Pachitariu, Marius and Lyamzin, Dmitry R. and Sahani, Maneesh and Lesica, Nicholas A.},
  month = feb,
  year = {2015},
  keywords = {cortex,population code,brain state,noise correlations},
  pages = {2058-2073},
  file = {/Users/jonny/Papers/PachitariuM/2015/Pachitariu_2015_State-Dependent Population Coding in Primary Auditory Cortex.pdf;/Users/jonny/Zotero/storage/2LHZ87ZB/2058.html},
  pmid = {25653363}
}

@article{kayserRhythmicAuditoryCortex2015,
  title = {Rhythmic {{Auditory Cortex Activity}} at {{Multiple Timescales Shapes Stimulus}}\textendash{{Response Gain}} and {{Background Firing}}},
  volume = {35},
  copyright = {Copyright \textcopyright{} 2015 Kayser et al.. This article is freely available online through the J Neurosci Author Open Choice option.},
  issn = {0270-6474, 1529-2401},
  abstract = {The phase of low-frequency network activity in the auditory cortex captures changes in neural excitability, entrains to the temporal structure of natural sounds, and correlates with the perceptual performance in acoustic tasks. Although these observations suggest a causal link between network rhythms and perception, it remains unknown how precisely they affect the processes by which neural populations encode sounds. We addressed this question by analyzing neural responses in the auditory cortex of anesthetized rats using stimulus\textendash{}response models. These models included a parametric dependence on the phase of local field potential rhythms in both stimulus-unrelated background activity and the stimulus\textendash{}response transfer function. We found that phase-dependent models better reproduced the observed responses than static models, during both stimulation with a series of natural sounds and epochs of silence. This was attributable to two factors: (1) phase-dependent variations in background firing (most prominent for delta; 1\textendash{}4 Hz); and (2) modulations of response gain that rhythmically amplify and attenuate the responses at specific phases of the rhythm (prominent for frequencies between 2 and 12 Hz). These results provide a quantitative characterization of how slow auditory cortical rhythms shape sound encoding and suggest a differential contribution of network activity at different timescales. In addition, they highlight a putative mechanism that may implement the selective amplification of appropriately timed sound tokens relative to the phase of rhythmic auditory cortex activity.},
  language = {en},
  number = {20},
  urldate = {2019-06-23},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.0268-15.2015},
  url = {http://www.jneurosci.org/content/35/20/7750},
  author = {Kayser, Christoph and Wilson, Caroline and Safaai, Houman and Sakata, Shuzo and Panzeri, Stefano},
  month = may,
  year = {2015},
  keywords = {neural coding,delta rhythm,information coding,LNP models,network state,receptive fields},
  pages = {7750-7762},
  file = {/Users/jonny/Papers/KayserC/2015/Kayser_2015_Rhythmic Auditory Cortex Activity at Multiple Timescales Shapes.pdf;/Users/jonny/Zotero/storage/RUXSLUJA/7750.html}
}

@article{peretzBrainOrganizationMusic2005b,
  title = {Brain {{Organization}} for {{Music Processing}}},
  volume = {56},
  abstract = {Research on how the brain processes music is emerging as a rich and stimulating area of investigation of perception, memory, emotion, and performance. Results emanating from both lesion studies and neuroimaging techniques are reviewed and integrated for each of these musical functions. We focus our attention on the common core of musical abilities shared by musicians and nonmusicians alike. Hence, the effect of musical training on brain plasticity is examined in a separate section, after a review of the available data regarding music playing and reading skills that are typically cultivated by musicians. Finally, we address a currently debated issue regarding the putative existence of music-specific neural networks. Unfortunately, due to scarcity of research on the macrostructure of music organization and on cultural differences, the musical material under focus is at the level of the musical phrase, as typically used in Western popular music.},
  number = {1},
  urldate = {2019-06-23},
  journal = {Annual Review of Psychology},
  doi = {10.1146/annurev.psych.56.091103.070225},
  url = {https://doi.org/10.1146/annurev.psych.56.091103.070225},
  author = {Peretz, Isabelle and Zatorre, Robert J.},
  year = {2005},
  pages = {89-114},
  pmid = {15709930}
}

@article{giraudCorticalOscillationsSpeech2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  volume = {15},
  copyright = {2012 Nature Publishing Group},
  issn = {1546-1726},
  shorttitle = {Cortical Oscillations and Speech Processing},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, 'packaging' incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  language = {en},
  number = {4},
  urldate = {2019-06-23},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.3063},
  url = {https://www.nature.com/articles/nn.3063},
  author = {Giraud, Anne-Lise and Poeppel, David},
  month = apr,
  year = {2012},
  pages = {511-517},
  file = {/Users/jonny/Papers/GiraudA/2012/Giraud_2012_Cortical oscillations and speech processing.pdf;/Users/jonny/Zotero/storage/P96RG9YI/nn.html}
}

@article{rothschildFunctionalOrganizationPopulation2010c,
  title = {Functional Organization and Population Dynamics in the Mouse Primary Auditory Cortex},
  volume = {13},
  copyright = {2010 Nature Publishing Group},
  issn = {1546-1726},
  abstract = {Cortical processing of auditory stimuli involves large populations of neurons with distinct individual response profiles. However, the functional organization and dynamics of local populations in the auditory cortex have remained largely unknown. Using in vivo two-photon calcium imaging, we examined the response profiles and network dynamics of layer 2/3 neurons in the primary auditory cortex (A1) of mice in response to pure tones. We found that local populations in A1 were highly heterogeneous in the large-scale tonotopic organization. Despite the spatial heterogeneity, the tendency of neurons to respond together (measured as noise correlation) was high on average. This functional organization and high levels of noise correlations are consistent with the existence of partially overlapping cortical subnetworks. Our findings may account for apparent discrepancies between ordered large-scale organization and local heterogeneity.},
  language = {en},
  number = {3},
  urldate = {2019-06-23},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.2484},
  url = {https://www.nature.com/articles/nn.2484},
  author = {Rothschild, Gideon and Nelken, Israel and Mizrahi, Adi},
  month = mar,
  year = {2010},
  pages = {353-360},
  file = {/Users/jonny/Zotero/storage/7IIBN74E/nn.html}
}

@article{limFunctionalStructureOrgan1986,
  title = {Functional Structure of the Organ of {{Corti}}: A Review},
  volume = {22},
  issn = {0378-5955},
  shorttitle = {Functional Structure of the Organ of {{Corti}}},
  abstract = {The mammalian auditory organs have a dual sensory system (inner vs. outer hair cells) with distinctly different cellular organizations and innervation patterns. However, the inner (IHCs) and outer (OHCs) hair cells are mechanoreceptors sharing similar general characteristics such as organization of stereocilia (including linkage system) and a gradation of stereociliary height along the length of the cochlea. This gradation of stereociliary height may be the single most important anatomic feature in the tuning capability of the sensory cell. Several lines of evidence suggest that the stereociliary stiffness may be modulated by the sensory cells themselves, most likely via the cuticular plate-rootlet complex. The stereociliary bundles of both types of hair cell are organized in a `W' formation with a steplike arrangment. In the OHCs, the `W' formation is sharply angulated and slanted toward the apex, coinciding with the slanted fiber arrangment of the overlying tectorial membrane, which is firmly coupled to the tips of the tallest row of the stereociliary bundles. However, in the IHCs, the `W' formation is wide and its long axis is linear and arranged at a right angle to the radial axis of the organ of Corti; also, the ciliary bundles are freestanding (with a few exceptions in the basal turn). This arrangement in the IHCs would be best suited for deflection by the radial flow of the endolymph. Present evidence suggests that the subtectorial fluid space exists, is filled with endolymph, and freely communicates with endolymph. Because of the discovery of the phenomenon of `cochlear emission', the possible motility of the sensory cells, particularly of the OHCs, has drawn intense interest in recent years. Recent investigations with dissociated sensory cells (OHCs) indicate some motile capability under various experimental conditions, although it has not been established that this motility is present in vivo. For this reason, the specialized cellular organization for motility and localization of contractile and cytoskeletal proteins have been investigated. These results support the possibility that the OHCs may have cellular facilities for this function. The most striking cellular features of the OHCs that distinguish them from the IHCs are the cell shape (cylindrical in OHCs vs. wine-bottle shaped in IHCs), endoplasmic reticular organization (well developed in OHCs vs. poorly developed in IHCs), extra cell wall membrane (well developed in the OHCs vs. poorly developed or absent in IHCs), mitochondrial organization (closely associated with ER in OHCs vs. dispersed in IHCs), and specialization of postsynaptic apparatus (well-developed subsynaptic cistern in OHCs vs. poorly developed subsynaptic cistern in IHCs). The well-developed ER system in the OHCs resembles the sarcoplasmic reticulum of the muscles. The ER system of the OHCs includes apical cistern, subsurface cistern, Hensen's body, and subsynaptic cistern, and these structures are interconnected. Owing to this connection, and because the subsynaptic cistern directly apposes the efferent nerve endings, the ER system of the OHCs is under efferent neural influence. The nerve ending sides of both cells have accumulations of neurotransmitter vesicles that are being released and recycled via exo- and endocytosis. This finding implies that both sensory cells are capable of transmitting afferent neural impulses via neurotransmitter release. The current morphological evidence is compatible with the concept that the IHCs are passive mechanoreceptors and the OHCs are bidirectional mechanoreceptors that can be passive as well as active.},
  number = {1},
  urldate = {2019-06-23},
  journal = {Hearing Research},
  doi = {10.1016/0378-5955(86)90089-4},
  url = {http://www.sciencedirect.com/science/article/pii/0378595586900894},
  author = {Lim, David J.},
  month = jan,
  year = {1986},
  pages = {117-146},
  file = {/Users/jonny/Papers/LimD/1986/Lim_1986_Functional structure of the organ of Corti.pdf;/Users/jonny/Zotero/storage/563JE2XF/0378595586900894.html}
}

@article{streitOriginVertebrateInner2001,
  title = {Origin of the Vertebrate Inner Ear: Evolution and Induction of the Otic Placode},
  volume = {199},
  issn = {1553-0795, 0002-9106},
  shorttitle = {Origin of the Vertebrate Inner Ear},
  abstract = {The vertebrate inner ear forms a highly complex sensory structure responsible for the detection of sound and balance. Some new aspects on the evolutionary and developmental origin of the inner ear are summarised here. Recent molecular data have challenged the longstanding view that special sense organs such as the inner ear have evolved with the appearance of vertebrates. In addition, it has remained unclear whether the ear originally arose through a modification of the amphibian mechanosensory lateral line system or whether both evolved independently. A comparison of the developmental mechanisms giving rise to both sensory systems in different species should help to clarify some of these controversies. During embryonic development, the inner ear arises from a simple epithelium adjacent to the hindbrain, the otic placode, that is specified through inductive interactions with surrounding tissues. This review summarises the embryological evidence showing that the induction of the otic placode is a multistep process which requires sequential interaction of different tissues with the future otic ectoderm and the recent progress that has been made to identify some of the molecular players involved. Finally, the hypothesis is discussed that induction of all sensory placodes initially shares a common molecular pathway, which may have been responsible to generate an `ancestral placode' during evolution.},
  language = {en},
  number = {1-2},
  urldate = {2019-06-23},
  journal = {The Journal of Anatomy},
  doi = {10.1017/S0021878201008263},
  url = {https://www.cambridge.org/core/journals/journal-of-anatomy/article/origin-of-the-vertebrate-inner-ear-evolution-and-induction-of-the-otic-placode/18F9D93001840F470DBBC9BB8DEBD782},
  author = {Streit, Andrea},
  month = aug,
  year = {2001},
  keywords = {Chordates,placode field,sensory placode},
  pages = {99-103},
  file = {/Users/jonny/Papers/StreitA/2001/Streit_2001_Origin of the vertebrate inner ear.pdf;/Users/jonny/Zotero/storage/9732YTQ9/18F9D93001840F470DBBC9BB8DEBD782.html}
}

@article{bissingerStructuralAcousticsGood2008,
  title = {Structural Acoustics of Good and Bad Violins},
  volume = {124},
  issn = {0001-4966},
  number = {3},
  urldate = {2019-06-23},
  journal = {The Journal of the Acoustical Society of America},
  doi = {10.1121/1.2956478},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.2956478},
  author = {Bissinger, George},
  month = sep,
  year = {2008},
  pages = {1764-1773},
  file = {/Users/jonny/Zotero/storage/DRUZMF3C/1.html}
}

@article{bendixenRegularityExtractionNonAdjacent2012b,
  title = {Regularity {{Extraction}} from {{Non}}-{{Adjacent Sounds}}},
  volume = {3},
  issn = {1664-1078},
  language = {en},
  urldate = {2019-06-23},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2012.00143},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00143/abstract},
  author = {Bendixen, Alexandra and Schr{\"o}ger, Erich and Ritter, Walter and Winkler, Istv{\'a}n},
  year = {2012},
  file = {/Users/jonny/Zotero/storage/Z4A7Z4AW/Bendixen et al. - 2012 - Regularity Extraction from Non-Adjacent Sounds.pdf}
}

@inproceedings{ekmanLocalizationCuesAffect2009,
  title = {Localization {{Cues Affect Emotional Judgments}} \textendash{} {{Results}} from a {{User Study}} on {{Scary Sound}}},
  abstract = {The current paradigm for creating emotional impact in game sound is to carefully choose which sounds to play. This paper takes an alternative approach, suggesting that emotional impact of sounds can be affected by choosing how to play those sounds. We describe a novel concept for emotional sound design - emotional fine-tuning - and show how it is possible to systematically influence the emotional impact of a single sound sample. A controlled user study with 8 subjects confirmed that changing...},
  language = {English},
  urldate = {2019-06-23},
  booktitle = {Audio {{Engineering Society Conference}}: 35th {{International Conference}}: {{Audio}} for {{Games}}},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15177},
  author = {Ekman, Inger and Kajastila, Raine},
  month = feb,
  year = {2009},
  file = {/Users/jonny/Zotero/storage/D3JRY4TL/browse.html}
}

@techreport{bispingEmotionalEffectCar1995,
  address = {{Warrendale, PA}},
  type = {{{SAE Technical Paper}}},
  title = {Emotional {{Effect}} of {{Car Interior Sounds}}: {{Pleasantness}} and {{Power}} and {{Their Relation}} to {{Acoustic Key Features}}},
  shorttitle = {Emotional {{Effect}} of {{Car Interior Sounds}}},
  abstract = {Factor analysis of attributes describing various aspects of car sounds quality reliably shows that the emotional meaning and reinforcing properties of car interior sounds are mainly related to two categories: pleasantness and power. They form the four quadrants of an emotional space with one axis de},
  language = {English},
  number = {951284},
  urldate = {2019-06-23},
  institution = {{SAE International}},
  url = {https://www.sae.org/publications/technical-papers/content/951284/},
  author = {Bisping, Rudolf},
  month = may,
  year = {1995},
  file = {/Users/jonny/Zotero/storage/SHFVHNQD/951284.html},
  doi = {10.4271/951284}
}

@article{mustonenReviewbasedConceptualAnalysis2008,
  title = {A Review-Based Conceptual Analysis of Auditory Signs and Their Design},
  abstract = {The research frames of auditory display have traditionally mainly focused on the evaluation of different applications and devices, whereas the theoretical development has had a minor role. In order to reach the goal of functional and intuitive auditory signs, the theoretical basis must be on a robust basis. User interface sound types have been traditionally divided into two exclusionary sound types: earcons and auditory icons. However, when approaching the issues from the viewpoints of for example human communication or semiotics, one can see that the current definitions and practices in auditory display as a scientific discipline are not pragmatic. It is recommended to define auditory signs to include different levels of meaning, as was originally proposed. Following current theoretical concepts leaves the full potential of auditory signs unexposed. In this paper, I introduce important viewpoints and approaches for more practical theoretical approaches for the design of auditory signs in order to develop a theoretical basis for usable syntax.},
  language = {en\_US},
  urldate = {2019-06-23},
  url = {https://smartech.gatech.edu/handle/1853/49882},
  author = {Mustonen, Manne-Sakari},
  month = jun,
  year = {2008},
  file = {/Users/jonny/Papers/MustonenM/2008/Mustonen_2008_A review-based conceptual analysis of auditory signs and their design.pdf;/Users/jonny/Zotero/storage/BWJQHBGA/49882.html}
}

@article{blattnerEarconsIconsTheir1989,
  title = {Earcons and {{Icons}}: {{Their Structure}} and {{Common Design Principles}}},
  volume = {4},
  issn = {0737-0024},
  shorttitle = {Earcons and {{Icons}}},
  abstract = {In this article we examine earcons, which are audio messages used in the user-computer interface to provide information and feedback to the user about computer entities. (Earcons include messages and functions, as well as states and labels.) We identify some design principles that are common to both visual symbols and auditory messages, and discuss the use of representational and abstract icons and earcons. We give some examples of audio patterns that may be used to design modules for earcons, which then may be assembled into larger groupings called families. The modules are single pitches or rhythmicized sequences of pitches called motives. The families are constructed about related motives that serve to identify a family of related messages. Issues concerned with learning and remembering earcons are discussed.},
  number = {1},
  urldate = {2019-06-23},
  journal = {Human\textendash{}Computer Interaction},
  doi = {10.1207/s15327051hci0401_1},
  url = {https://www.tandfonline.com/doi/abs/10.1207/s15327051hci0401_1},
  author = {Blattner, Meera M. and Sumikawa, Denise A. and Greenberg, Robert M.},
  month = mar,
  year = {1989},
  pages = {11-44},
  file = {/Users/jonny/Papers/BlattnerM/1989/Blattner_1989_Earcons and Icons.pdf;/Users/jonny/Zotero/storage/U7WWXSMK/s15327051hci0401_1.html}
}

@article{mcdermottSoundTexturePerception2011,
  title = {Sound {{Texture Perception}} via {{Statistics}} of the {{Auditory Periphery}}: {{Evidence}} from {{Sound Synthesis}}},
  volume = {71},
  issn = {0896-6273},
  shorttitle = {Sound {{Texture Perception}} via {{Statistics}} of the {{Auditory Periphery}}},
  abstract = {Summary
Rainstorms, insect swarms, and galloping horses produce ``sound textures''\textemdash{}the collective result of many similar acoustic events. Sound textures are distinguished by temporal homogeneity, suggesting they could be recognized with time-averaged statistics. To test this hypothesis, we processed real-world textures with an auditory model containing filters tuned for sound frequencies and their modulations, and measured statistics of the resulting decomposition. We then assessed the realism and recognizability of novel sounds synthesized to have matching statistics. Statistics of individual frequency channels, capturing spectral power and sparsity, generally failed to produce compelling synthetic textures; however, combining them with correlations between channels produced identifiable and natural-sounding textures. Synthesis quality declined if statistics were computed from biologically implausible auditory models. The results suggest that sound texture perception is mediated by relatively simple statistics of~early auditory representations, presumably computed by downstream neural populations. The synthesis methodology offers a powerful tool for their further investigation.},
  number = {5},
  urldate = {2019-06-23},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2011.06.032},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627311005629},
  author = {McDermott, Josh H. and Simoncelli, Eero P.},
  month = sep,
  year = {2011},
  pages = {926-940},
  file = {/Users/jonny/Papers/McDermottJ/2011/McDermott_2011_Sound Texture Perception via Statistics of the Auditory Periphery.pdf;/Users/jonny/Zotero/storage/TXV27WL4/S0896627311005629.html}
}

@article{mcdermottSummaryStatisticsAuditory2013a,
  title = {Summary Statistics in Auditory Perception},
  volume = {16},
  copyright = {2013 Nature Publishing Group},
  issn = {1546-1726},
  abstract = {Sensory signals are transduced at high resolution, but their structure must be stored in a more compact format. Here we provide evidence that the auditory system summarizes the temporal details of sounds using time-averaged statistics. We measured discrimination of 'sound textures' that were characterized by particular statistical properties, as normally result from the superposition of many acoustic features in auditory scenes. When listeners discriminated examples of different textures, performance improved with excerpt duration. In contrast, when listeners discriminated different examples of the same texture, performance declined with duration, a paradoxical result given that the information available for discrimination grows with duration. These results indicate that once these sounds are of moderate length, the brain's representation is limited to time-averaged statistics, which, for different examples of the same texture, converge to the same values with increasing duration. Such statistical representations produce good categorical discrimination, but limit the ability to discern temporal detail.},
  language = {en},
  number = {4},
  urldate = {2019-06-23},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.3347},
  url = {https://www.nature.com/articles/nn.3347},
  author = {McDermott, Josh H. and Schemitsch, Michael and Simoncelli, Eero P.},
  month = apr,
  year = {2013},
  pages = {493-498},
  file = {/Users/jonny/Papers/McDermottJ/2013/McDermott_2013_Summary statistics in auditory perception2.pdf;/Users/jonny/Zotero/storage/E354M6YF/nn.html}
}

@article{daviesPerceptionSoundscapesInterdisciplinary2013,
  series = {Applied {{Soundscapes}}: {{Recent Advances}} in {{Soundscape Research}}},
  title = {Perception of Soundscapes: {{An}} Interdisciplinary Approach},
  volume = {74},
  issn = {0003-682X},
  shorttitle = {Perception of Soundscapes},
  abstract = {This paper takes an overall view of findings from the Positive Soundscape Project, a large inter-disciplinary soundscapes study. Qualitative fieldwork (soundwalks and focus groups) have found that soundscape perception is influenced by cognitive effects such as the meaning of a soundscape and its components, and how information is conveyed by a soundscape, for example on the behaviour of people within the soundscape. Three significant clusters were found in the language people use to describe soundscapes: sound sources, sound descriptors and soundscape descriptors. Results from listening tests and soundwalks have been integrated to show that the two principal dimensions of soundscape emotional response seem to be calmness and vibrancy. Further, vibrancy seems to have two aspects: organisation of sounds and changes over time. The possible application of the results to soundscape assessment and design are briefly discussed.},
  number = {2},
  urldate = {2019-06-23},
  journal = {Applied Acoustics},
  doi = {10.1016/j.apacoust.2012.05.010},
  url = {http://www.sciencedirect.com/science/article/pii/S0003682X12001545},
  author = {Davies, William J. and Adams, Mags D. and Bruce, Neil S. and Cain, Rebecca and Carlyle, Angus and Cusack, Peter and Hall, Deborah A. and Hume, Ken I. and Irwin, Amy and Jennings, Paul and Marselle, Melissa and Plack, Christopher J. and Poxon, John},
  month = feb,
  year = {2013},
  keywords = {Emotion,Noise,Perception,Soundscape,Soundwalk},
  pages = {224-231},
  file = {/Users/jonny/Papers/DaviesW/2013/Davies_2013_Perception of soundscapes.pdf;/Users/jonny/Zotero/storage/8N8XC8YH/S0003682X12001545.html}
}

@article{mooreHumanAuditorySystem2009,
  title = {The Human Auditory System: {{A}} Timeline of Development},
  volume = {46},
  number = {9},
  urldate = {2019-06-23},
  journal = {International Journal of Audiology},
  doi = {https://doi.org/10.1080/14992020701383019},
  url = {https://www.tandfonline.com/doi/abs/10.1080/14992020701383019},
  author = {Moore, Jean K. and Linthicum, Fred H.},
  month = jul,
  year = {2009},
  file = {/Users/jonny/Zotero/storage/LEYDVTQ7/14992020701383019.html}
}

@article{keuroghlianAdaptiveAuditoryPlasticity2007,
  title = {Adaptive Auditory Plasticity in Developing and Adult Animals},
  volume = {82},
  issn = {0301-0082},
  abstract = {Enormous progress has been made in our understanding of adaptive plasticity in the central auditory system. Experiments on a range of species demonstrate that, in adults, the animal must attend to (i.e., respond to) a stimulus in order for plasticity to be induced, and the plasticity that is induced is specific for the acoustic feature to which the animal has attended. The requirement that an adult animal must attend to a stimulus in order for adaptive plasticity to occur suggests an essential role of neuromodulatory systems in gating plasticity in adults. Indeed, neuromodulators, particularly acetylcholine (ACh), that are associated with the processes of attention, have been shown to enable adaptive plasticity in adults. In juvenile animals, attention may facilitate plasticity, but it is not always required: during sensitive periods, mere exposure of an animal to an atypical auditory environment can result in large functional changes in certain auditory circuits. Thus, in both the developing and mature auditory systems substantial experience-dependent plasticity can occur, but the conditions under which it occurs are far more stringent in adults. We review experimental results that demonstrate experience-dependent plasticity in the central auditory representations of sound frequency, level and temporal sequence, as well as in the representations of binaural localization cues in both developing and adult animals.},
  number = {3},
  urldate = {2019-06-23},
  journal = {Progress in Neurobiology},
  doi = {10.1016/j.pneurobio.2007.03.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0301008207000731},
  author = {Keuroghlian, Alex S. and Knudsen, Eric I.},
  month = jun,
  year = {2007},
  keywords = {Attention,Plasticity,Auditory system,Adult,Sensitive period,Juvenile,Experience,Neuromodulators},
  pages = {109-121},
  file = {/Users/jonny/Papers/KeuroghlianA/2007/Keuroghlian_2007_Adaptive auditory plasticity in developing and adult animals.pdf;/Users/jonny/Zotero/storage/2NQVD3DE/S0301008207000731.html}
}

@misc{WhatWhereHow,
  title = {The What, Where and How of Auditory-Object Perception | {{Nature Reviews Neuroscience}}},
  urldate = {2019-06-23},
  url = {https://www.nature.com/articles/nrn3565},
  file = {/Users/jonny/Zotero/storage/QEPUK8QS/nrn3565.html}
}

@article{bizleyWhatWhereHow2013a,
  title = {The What, Where and How of Auditory-Object Perception},
  volume = {14},
  copyright = {2013 Nature Publishing Group},
  issn = {1471-0048},
  abstract = {The fundamental perceptual unit in hearing is the 'auditory object'. Similar to visual objects, auditory objects are the computational result of the auditory system's capacity to detect, extract, segregate and group spectrotemporal regularities in the acoustic environment; the multitude of acoustic stimuli around us together form the auditory scene. However, unlike the visual scene, resolving the component objects within the auditory scene crucially depends on their temporal structure. Neural correlates of auditory objects are found throughout the auditory system. However, neural responses do not become correlated with a listener's perceptual reports until the level of the cortex. The roles of different neural structures and the contribution of different cognitive states to the perception of auditory objects are not yet fully understood.},
  language = {en},
  number = {10},
  urldate = {2019-06-23},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn3565},
  url = {https://www.nature.com/articles/nrn3565},
  author = {Bizley, Jennifer K. and Cohen, Yale E.},
  month = oct,
  year = {2013},
  pages = {693-707},
  file = {/Users/jonny/Papers/BizleyJ/2013/Bizley_2013_The what, where and how of auditory-object perception3.pdf}
}

@article{denhamPredictiveCodingAuditory2018,
  title = {Predictive Coding in Auditory Perception: Challenges and Unresolved Questions},
  copyright = {\textcopyright{} 2017 Federation of European Neuroscience Societies and John Wiley \& Sons Ltd},
  issn = {1460-9568},
  shorttitle = {Predictive Coding in Auditory Perception},
  abstract = {Predictive coding is arguably the currently dominant theoretical framework for the study of perception. It has been employed to explain important auditory perceptual phenomena, and it has inspired theoretical, experimental and computational modelling efforts aimed at describing how the auditory system parses the complex sound input into meaningful units (auditory scene analysis). These efforts have uncovered some vital questions, addressing which could help to further specify predictive coding and clarify some of its basic assumptions. The goal of the current review is to motivate these questions and show how unresolved issues in explaining some auditory phenomena lead to general questions of the theoretical framework. We focus on experimental and computational modelling issues related to sequential grouping in auditory scene analysis (auditory pattern detection and bistable perception), as we believe that this is the research topic where predictive coding has the highest potential for advancing our understanding. In addition to specific questions, our analysis led us to identify three more general questions that require further clarification: (1) What exactly is meant by prediction in predictive coding? (2) What governs which generative models make the predictions? and (3) What (if it exists) is the correlate of perceptual experience within the predictive coding framework?},
  language = {en},
  urldate = {2019-06-23},
  journal = {European Journal of Neuroscience},
  doi = {10.1111/ejn.13802},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.13802},
  author = {Denham, Susan L. and Winkler, Istv{\'a}n},
  year = {2018},
  keywords = {auditory scene analysis,auditory object representation,computational modelling,pattern detection},
  file = {/Users/jonny/Papers/DenhamS/undefined/Denham_Predictive coding in auditory perception.pdf;/Users/jonny/Zotero/storage/6NWSIKVB/ejn.html}
}

@article{bratticoNeuroaestheticsMusic2013,
  title = {The Neuroaesthetics of Music},
  volume = {7},
  issn = {1931-390X(Electronic),1931-3896(Print)},
  abstract = {The increasingly intensive study of music by neuroscientists over the past two decades has established the neurosciences of music as a subdiscipline of cognitive neuroscience, responsible for investigating the neural basis for music perception, cognition, and emotion. In this endeavor, music perception and cognition have often been compared with language processing and understanding, while music-induced emotions are compared with emotions induced by visual stimuli. Here, we review research that is beginning to define a new field of study called neuroaesthetics of music. According to this fresh perspective, music is viewed primarily as an expressive art rather than as a cognitive domain. The goal of this emerging field is to understand the neural mechanisms and structures involved in the perceptual, affective and cognitive processes that generate the three principal aesthetic responses: emotions, judgments, and preference. Although much is known about the frontotemporal brain mechanisms underlying perceptual and cognitive musical processes, and about the limbic and paralimbic networks responsible for musical affect, there is a great deal of work to be done in understanding the neural chronometry and structures determining aesthetic responses to music. Research has only recently begun to delineate the modulatory effects of the listener, listening situation, and the properties of the music itself on a musical aesthetic experience. This article offers a review and synthesis of our current understanding of the perceptual, cognitive, and affective processes involved in an aesthetic musical experience and introduces a novel framework to coordinate future endeavors in an emerging field. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  number = {1},
  journal = {Psychology of Aesthetics, Creativity, and the Arts},
  doi = {10.1037/a0031624},
  author = {Brattico, Elvira and Pearce, Marcus},
  year = {2013},
  keywords = {Auditory Cortex,Pitch Perception,Music,Rhyme,Aesthetics,Cognitive Neuroscience,Music Perception},
  pages = {48-61},
  file = {/Users/jonny/Zotero/storage/SYI6N67G/2013-06417-005.html}
}

@article{zatorreWhyWeLove2018,
  title = {Why {{Do We Love Music}}?},
  volume = {2018},
  issn = {1524-6205},
  abstract = {While the human brain is hardwired to feel pleasure for basic survival necessities, such as eating and sex, music\textemdash{}although obviously pleasurable\textemdash{}doesn't offer the same evolutionary advantages. So why do we respond to patterns of sounds that disappear in an instant? Why do we belt music from the top of our lungs, learn to play instruments, and empty our bank accounts to see Bruce Springsteen on Broadway? Our author offers some valuable insights.},
  urldate = {2019-06-23},
  journal = {Cerebrum: the Dana Forum on Brain Science},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6353111/},
  author = {Zatorre, Robert J.},
  month = nov,
  year = {2018},
  file = {/Users/jonny/Papers/ZatorreR/2018/Zatorre_2018_Why Do We Love Music.pdf},
  pmid = {30746026},
  pmcid = {PMC6353111}
}

@article{ferreriDopamineModulatesReward2019,
  title = {Dopamine Modulates the Reward Experiences Elicited by Music},
  volume = {116},
  copyright = {\textcopyright{} 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  abstract = {Understanding how the brain translates a structured sequence of sounds, such as music, into a pleasant and rewarding experience is a fascinating question which may be crucial to better understand the processing of abstract rewards in humans. Previous neuroimaging findings point to a challenging role of the dopaminergic system in music-evoked pleasure. However, there is a lack of direct evidence showing that dopamine function is causally related to the pleasure we experience from music. We addressed this problem through a double blind within-subject pharmacological design in which we directly manipulated dopaminergic synaptic availability while healthy participants (n = 27) were engaged in music listening. We orally administrated to each participant a dopamine precursor (levodopa), a dopamine antagonist (risperidone), and a placebo (lactose) in three different sessions. We demonstrate that levodopa and risperidone led to opposite effects in measures of musical pleasure and motivation: while the dopamine precursor levodopa, compared with placebo, increased the hedonic experience and music-related motivational responses, risperidone led to a reduction of both. This study shows a causal role of dopamine in musical pleasure and indicates that dopaminergic transmission might play different or additive roles than the ones postulated in affective processing so far, particularly in abstract cognitive activities.},
  language = {en},
  number = {9},
  urldate = {2019-06-23},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1811878116},
  url = {https://www.pnas.org/content/116/9/3793},
  author = {Ferreri, Laura and {Mas-Herrero}, Ernest and Zatorre, Robert J. and Ripoll{\'e}s, Pablo and {Gomez-Andres}, Alba and Alicart, Helena and Oliv{\'e}, Guillem and {Marco-Pallar{\'e}s}, Josep and Antonijoan, Rosa M. and Valle, Marta and Riba, Jordi and {Rodriguez-Fornells}, Antoni},
  month = feb,
  year = {2019},
  keywords = {reward,dopamine,music,motivation,pleasure},
  pages = {3793-3798},
  file = {/Users/jonny/Papers/FerreriL/2019/Ferreri_2019_Dopamine modulates the reward experiences elicited by music.pdf;/Users/jonny/Zotero/storage/F64KJRAZ/3793.html},
  pmid = {30670642}
}

@misc{BrainCorrelatesMusicevoked,
  title = {Brain Correlates of Music-Evoked Emotions | {{Nature Reviews Neuroscience}}},
  urldate = {2019-06-23},
  url = {https://www.nature.com/articles/nrn3666},
  file = {/Users/jonny/Zotero/storage/6F2NL9FL/nrn3666.html}
}

@misc{WhyMusicalMemory,
  title = {Why Musical Memory Can Be Preserved in Advanced {{Alzheimer}}'s Disease | {{Brain}} | {{Oxford Academic}}},
  urldate = {2019-06-23},
  url = {https://academic.oup.com/brain/article/138/8/2438/330016},
  file = {/Users/jonny/Zotero/storage/JAMZ5X43/330016.html}
}

@article{kaelenLSDEnhancesEmotional2015,
  title = {{{LSD}} Enhances the Emotional Response to Music},
  volume = {232},
  issn = {1432-2072},
  abstract = {RationaleThere is renewed interest in the therapeutic potential of psychedelic drugs such as lysergic acid diethylamide (LSD). LSD was used extensively in the 1950s and 1960s as an adjunct in psychotherapy, reportedly enhancing emotionality. Music is an effective tool to evoke and study emotion and is considered an important element in psychedelic-assisted psychotherapy; however, the hypothesis that psychedelics enhance the emotional response to music has yet to be investigated in a modern placebo-controlled study.ObjectivesThe present study sought to test the hypothesis that music-evoked emotions are enhanced under LSD.MethodsTen healthy volunteers listened to five different tracks of instrumental music during each of two study days, a placebo day followed by an LSD day, separated by 5\textendash{}7 days. Subjective ratings were completed after each music track and included a visual analogue scale (VAS) and the nine-item Geneva Emotional Music Scale (GEMS-9).ResultsResults demonstrated that the emotional response to music is enhanced by LSD, especially the emotions ``wonder'', ``transcendence'', ``power'' and ``tenderness''.ConclusionsThese findings reinforce the long-held assumption that psychedelics enhance music-evoked emotion, and provide tentative and indirect support for the notion that this effect can be harnessed in the context of psychedelic-assisted psychotherapy. Further research is required to test this link directly.},
  language = {en},
  number = {19},
  urldate = {2019-06-23},
  journal = {Psychopharmacology},
  doi = {10.1007/s00213-015-4014-y},
  url = {https://doi.org/10.1007/s00213-015-4014-y},
  author = {Kaelen, M. and Barrett, F. S. and Roseman, L. and Lorenz, R. and Family, N. and Bolstridge, M. and Curran, H. V. and Feilding, A. and Nutt, D. J. and {Carhart-Harris}, R. L.},
  month = oct,
  year = {2015},
  keywords = {Emotion,Music,LSD,Psychedelic,Psychotherapy,Serotonin 2A receptor},
  pages = {3607-3614}
}

@article{fruhholzSoundEmotionsUnifying2016,
  title = {The Sound of Emotions\textemdash{{Towards}} a Unifying Neural Network Perspective of Affective Sound Processing},
  volume = {68},
  issn = {0149-7634},
  abstract = {Affective sounds are an integral part of the natural and social environment that shape and influence behavior across a multitude of species. In human primates, these affective sounds span a repertoire of environmental and human sounds when we vocalize or produce music. In terms of neural processing, cortical and subcortical brain areas constitute a distributed network that supports our listening experience to these affective sounds. Taking an exhaustive cross-domain view, we accordingly suggest a common neural network that facilitates the decoding of the emotional meaning from a wide source of sounds rather than a traditional view that postulates distinct neural systems for specific affective sound types. This new integrative neural network view unifies the decoding of affective valence in sounds, and ascribes differential as well as complementary functional roles to specific nodes within a common neural network. It also highlights the importance of an extended brain network beyond the central limbic and auditory brain systems engaged in the processing of affective sounds.},
  urldate = {2019-06-23},
  journal = {Neuroscience \& Biobehavioral Reviews},
  doi = {10.1016/j.neubiorev.2016.05.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0149763416300082},
  author = {Fr{\"u}hholz, Sascha and Trost, Wiebke and Kotz, Sonja A.},
  month = sep,
  year = {2016},
  keywords = {Auditory cortex,Music,Sound,Basal ganglia,Affect,Cerebellum,Voice,Limbic system},
  pages = {96-110},
  file = {/Users/jonny/Papers/FrÃ¼hholzS/2016/FrÃ¼hholz_2016_The sound of emotionsâTowards a unifying neural network perspective of.pdf;/Users/jonny/Zotero/storage/8BX5QIE6/S0149763416300082.html}
}

@article{kamiokaEffectivenessMusicTherapy2014,
  title = {Effectiveness of Music Therapy: A Summary of Systematic Reviews Based on Randomized Controlled Trials of Music Interventions},
  volume = {8},
  issn = {1177-889X},
  shorttitle = {Effectiveness of Music Therapy},
  abstract = {OBJECTIVE: The objective of this review was to summarize evidence for the effectiveness of music therapy (MT) and to assess the quality of systematic reviews (SRs) based on randomized controlled trials (RCTs).
STUDY DESIGN: An SR of SRs based on RCTs.
METHODS: Studies were eligible if they were RCTs. Studies included were those with at least one treatment group in which MT was applied. We searched the following databases from 1995 to October 1, 2012: MEDLINE via PubMed, CINAHL (Cumulative Index of Nursing and Allied Health Literature), Web of Science, Global Health Library, and Ichushi-Web. We also searched all Cochrane Database and Campbell Systematic Reviews up to October 1, 2012. Based on the International Classification of Diseases, 10th revision, we identified a disease targeted for each article.
RESULTS: Twenty-one studies met all inclusion criteria. This study included 16 Cochrane reviews. As a whole, the quality of the articles was very good. Eight studies were about "Mental and behavioural disorders (F00-99)"; there were two studies on "Diseases of the nervous system (G00-99)" and "Diseases of the respiratory system (J00-99)"; and there was one study each for "Endocrine, nutritional and metabolic diseases (E00-90)", "Diseases of the circulatory system (I00-99)", and "Pregnancy, childbirth and the puerperium (O60)". MT treatment improved the following: global and social functioning in schizophrenia and/or serious mental disorders, gait and related activities in Parkinson's disease, depressive symptoms, and sleep quality.
CONCLUSION: THIS COMPREHENSIVE SUMMARY OF SRS DEMONSTRATED THAT MT TREATMENT IMPROVED THE FOLLOWING: global and social functioning in schizophrenia and/or serious mental disorders, gait and related activities in Parkinson's disease, depressive symptoms, and sleep quality. MT may have the potential for improving other diseases, but there is not enough evidence at present. Most importantly, no specific adverse effect or harmful phenomenon occurred in any of the studies, and MT was well tolerated by almost all patients.},
  language = {eng},
  journal = {Patient Preference and Adherence},
  doi = {10.2147/PPA.S61340},
  author = {Kamioka, Hiroharu and Tsutani, Kiichiro and Yamada, Minoru and Park, Hyuntae and Okuizumi, Hiroyasu and Tsuruoka, Koki and Honda, Takuya and Okada, Shinpei and Park, Sang-Jun and Kitayuguchi, Jun and Abe, Takafumi and Handa, Shuichi and Oshio, Takuya and Mutoh, Yoshiteru},
  year = {2014},
  keywords = {depression,schizophrenia,ICD-10,mental disorders,Parkinsonâs disease,sleep},
  pages = {727-754},
  file = {/Users/jonny/Papers/KamiokaH/2014/Kamioka_2014_Effectiveness of music therapy.pdf},
  pmid = {24876768},
  pmcid = {PMC4036702}
}

@article{bratticoNeuralChronometryAesthetic2013,
  title = {Toward a {{Neural Chronometry}} for the {{Aesthetic Experience}} of {{Music}}},
  volume = {4},
  issn = {1664-1078},
  abstract = {Music is often studied as a cognitive domain alongside language. The emotional aspects of music have also been shown to be important, but views on their nature diverge. For instance, the specific emotions that music induces and how they relate to emotional expression are still under debate. Here we propose a mental and neural chronometry of the aesthetic experience of music initiated and mediated by external and internal contexts such as intentionality, background mood, attention, and expertise. The initial stages necessary for an aesthetic experience of music are feature analysis, integration across modalities, and cognitive processing on the basis of long-term knowledge. These stages are common to individuals belonging to the same musical culture. The initial emotional reactions to music include the startle reflex, core `liking', and arousal. Subsequently, discrete emotions are perceived and induced. Presumably somatomotor processes synchronizing the body with the music also come into play here. The subsequent stages, in which cognitive, affective, and decisional processes intermingle, require controlled cross-modal neural processes to result in aesthetic emotions, aesthetic judgments, and conscious liking. These latter aesthetic stages often require attention, intentionality, and expertise for their full actualization.},
  language = {English},
  urldate = {2019-06-23},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2013.00206},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00206/full},
  author = {Brattico, Elvira and Bogert, Brigitte and Jacobsen, Thomas},
  year = {2013},
  keywords = {Brain,aesthetics,appraisal,Judgment,liking,music cognition,Music emotion,preference},
  file = {/Users/jonny/Papers/BratticoE/2013/Brattico_2013_Toward a Neural Chronometry for the Aesthetic Experience of Music.pdf}
}

@article{fontaineBrianHearsOnline2011,
  title = {Brian {{Hears}}: {{Online Auditory Processing Using Vectorization Over Channels}}},
  volume = {5},
  issn = {1662-5196},
  shorttitle = {Brian {{Hears}}},
  abstract = {The human cochlea includes about 3000 inner hair cells which filter sounds at frequencies between 20 Hz and 20 kHz. This massively parallel frequency analysis is reflected in models of auditory processing, which are often based on banks of filters. However, existing implementations do not exploit this parallelism. Here we propose algorithms to simulate these models by vectorising computation over frequency channels, which are implemented in ``Brian Hears'', a library for the spiking neural network simulator package ``Brian''. This approach allows us to use high-level programming languages such as Python, as the cost of interpretation becomes negligible. This makes it possible to define and simulate complex models in a simple way, while all previous implementations were model-specific. In addition, we show that these algorithms can be naturally parallelised using graphics processing units, yielding substantial speed improvements. We demonstrate these algorithms with several state-of-the-art cochlear models, and show that they compare favorably with existing, less flexible, implementations.},
  language = {English},
  urldate = {2019-06-23},
  journal = {Frontiers in Neuroinformatics},
  doi = {10.3389/fninf.2011.00009},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2011.00009/full},
  author = {Fontaine, Bertrand and Goodman, Dan F. M. and Benichoux, Victor and Brette, Romain},
  year = {2011},
  keywords = {auditory filter,brian,gpu,python,vectorisation},
  file = {/Users/jonny/Papers/FontaineB/2011/Fontaine_2011_Brian Hears.pdf}
}

@article{martinez-molinaNeuralCorrelatesSpecific2016,
  title = {Neural Correlates of Specific Musical Anhedonia},
  volume = {113},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  issn = {0027-8424, 1091-6490},
  abstract = {Although music is ubiquitous in human societies, there are some people for whom music holds no reward value despite normal perceptual ability and preserved reward-related responses in other domains. The study of these individuals with specific musical anhedonia may be crucial to understand better the neural correlates underlying musical reward. Previous neuroimaging studies have shown that musically induced pleasure may arise from the interaction between auditory cortical networks and mesolimbic reward networks. If such interaction is critical for music-induced pleasure to emerge, then those individuals who do not experience it should show alterations in the cortical-mesolimbic response. In the current study, we addressed this question using fMRI in three groups of 15 participants, each with different sensitivity to music reward. We demonstrate that the music anhedonic participants showed selective reduction of activity for music in the nucleus accumbens (NAcc), but normal activation levels for a monetary gambling task. Furthermore, this group also exhibited decreased functional connectivity between the right auditory cortex and ventral striatum (including the NAcc). In contrast, individuals with greater than average response to music showed enhanced connectivity between these structures. Thus, our results suggest that specific musical anhedonia may be associated with a reduction in the interplay between the auditory cortex and the subcortical reward network, indicating a pivotal role of this interaction for the enjoyment of music.},
  language = {en},
  number = {46},
  urldate = {2019-06-23},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1611211113},
  url = {https://www.pnas.org/content/113/46/E7337},
  author = {{Mart{\'i}nez-Molina}, Noelia and {Mas-Herrero}, Ernest and {Rodr{\'i}guez-Fornells}, Antoni and Zatorre, Robert J. and {Marco-Pallar{\'e}s}, Josep},
  month = nov,
  year = {2016},
  keywords = {reward,music,emotion,anhedonia},
  pages = {E7337-E7345},
  file = {/Users/jonny/Papers/MartÃ­nez-MolinaN/2016/MartÃ­nez-Molina_2016_Neural correlates of specific musical anhedonia.pdf;/Users/jonny/Zotero/storage/B9JDZATG/E7337.html},
  pmid = {27799544}
}

@incollection{eversMusicalHallucinations2010,
  title = {Musical {{Hallucinations}}},
  isbn = {978-1-84816-268-6},
  urldate = {2019-06-23},
  booktitle = {Neurology of {{Music}}},
  publisher = {{IMPERIAL COLLEGE PRESS}},
  url = {https://www.worldscientific.com/doi/abs/10.1142/9781848162693_0012},
  author = {Evers, Stefan},
  month = jul,
  year = {2010},
  pages = {187-201},
  file = {/Users/jonny/Zotero/storage/WNVH7WRW/9781848162693_0012.html},
  doi = {10.1142/9781848162693_0012}
}

@article{stewartMusicBrainDisorders2006,
  title = {Music and the Brain: Disorders of Musical Listening},
  volume = {129},
  issn = {0006-8950},
  shorttitle = {Music and the Brain},
  abstract = {Abstract.  The study of the brain bases for normal musical listening has advanced greatly in the last 30 years. The evidence from basic and clinical neuroscienc},
  language = {en},
  number = {10},
  urldate = {2019-06-23},
  journal = {Brain},
  doi = {10.1093/brain/awl171},
  url = {https://academic.oup.com/brain/article/129/10/2533/289289},
  author = {Stewart, Lauren and {von Kriegstein}, Katharina and Warren, Jason D. and Griffiths, Timothy D.},
  month = oct,
  year = {2006},
  pages = {2533-2553},
  file = {/Users/jonny/Papers/StewartL/2006/Stewart_2006_Music and the brain.pdf;/Users/jonny/Zotero/storage/YSEQ8YYH/289289.html}
}

@article{schulz-mirbachRelationshipSwimBladder2012,
  title = {Relationship between {{Swim Bladder Morphology}} and {{Hearing Abilities}}\textendash{{A Case Study}} on {{Asian}} and {{African Cichlids}}},
  volume = {7},
  issn = {1932-6203},
  abstract = {Background Several teleost species have evolved anterior extensions of the swim bladder which come close to or directly contact the inner ears. A few comparative studies have shown that these morphological specializations may enhance hearing abilities. This study investigates the diversity of swim bladder morphology in four Asian and African cichlid species and analyzes how this diversity affects their hearing sensitivity. Methodology/Principal Findings We studied swim bladder morphology by dissections and by making 3D reconstructions from high-resolution microCT scans. The auditory sensitivity was determined in terms of sound pressure levels (SPL) and particle acceleration levels (PAL) using the auditory evoked potential (AEP) recording technique. The swim bladders in Hemichromis guttatus and Steatocranus tinanti lacked anterior extensions and the swim bladder was considerably small in the latter species. In contrast, Paratilapia polleni and especially Etroplus maculatus possessed anterior extensions bringing the swim bladder close to the inner ears. All species were able to detect frequencies up to 3 kHz (SPL) except S. tinanti which only responded to frequencies up to 0.7 kHz. P. polleni and E. maculatus showed significantly higher auditory sensitivities at 0.5 and 1 kHz than the two species lacking anterior swim bladder extensions. The highest auditory sensitivities were found in E. maculatus, which possessed the most intimate swim bladder-inner ear relationship (maximum sensitivity 66 dB re 1 {$\mathrm{\mu}$}Pa at 0.5 kHz). Conclusions Our results indicate that anterior swim bladder extensions seem to improve mean absolute auditory sensitivities by 21\textendash{}42 dB (SPLs) and 21\textendash{}36 dB (PALs) between 0.5 and 1 kHz. Besides anterior extensions, the size of the swim bladder appears to be an important factor for extending the detectable frequency range (up to 3 kHz).},
  language = {en},
  number = {8},
  urldate = {2019-06-24},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0042292},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042292},
  author = {{Schulz-Mirbach}, Tanja and Metscher, Brian and Ladich, Friedrich},
  month = aug,
  year = {2012},
  keywords = {Hearing,Fish,External otitis,Inner ear,Otolith,Semicircular canals,Sound pressure,Swimming},
  pages = {e42292},
  file = {/Users/jonny/Papers/Schulz-MirbachT/2012/Schulz-Mirbach_2012_Relationship between Swim Bladder Morphology and Hearing AbilitiesâA Case Study.pdf;/Users/jonny/Zotero/storage/9RBN24EM/article.html}
}

@article{smithEfficientAuditoryCoding2006,
  title = {Efficient Auditory Coding},
  volume = {439},
  copyright = {2006 Nature Publishing Group},
  issn = {1476-4687},
  abstract = {The auditory neural code must serve a wide range of auditory tasks that require great sensitivity in time and frequency and be effective over the diverse array of sounds present in natural acoustic environments. It has been suggested1,2,3,4,5 that sensory systems might have evolved highly efficient coding strategies to maximize the information conveyed to the brain while minimizing the required energy and neural resources. Here we show that, for natural sounds, the complete acoustic waveform can be represented efficiently with a nonlinear model based on a population spike code. In this model, idealized spikes encode the precise temporal positions and magnitudes of underlying acoustic features. We find that when the features are optimized for coding either natural sounds or speech, they show striking similarities to time-domain cochlear filter estimates, have a frequency-bandwidth dependence similar to that of auditory nerve fibres, and yield significantly greater coding efficiency than conventional signal representations. These results indicate that the auditory code might approach an information theoretic optimum and that the acoustic structure of speech might be adapted to the coding capacity of the mammalian auditory system.},
  language = {En},
  number = {7079},
  urldate = {2019-06-24},
  journal = {Nature},
  doi = {10.1038/nature04485},
  url = {https://www.nature.com/articles/nature04485},
  author = {Smith, Evan C. and Lewicki, Michael S.},
  month = feb,
  year = {2006},
  pages = {978},
  file = {/Users/jonny/Papers/SmithE/2006/Smith_2006_Efficient auditory coding.pdf;/Users/jonny/Zotero/storage/2AWEUDNG/nature04485.html}
}

@article{bartlettOrganizationPhysiologyAuditory2013c,
  title = {The Organization and Physiology of the Auditory Thalamus and Its Role in Processing Acoustic Features Important for Speech Perception},
  volume = {126},
  issn = {0093-934X},
  abstract = {The auditory thalamus, or medial geniculate body (MGB), is the primary sensory input to auditory cortex. Therefore, it plays a critical role in the complex auditory processing necessary for robust speech perception. This review will describe the functional organization of the thalamus as it relates to processing acoustic features important for speech perception, focusing on thalamic nuclei that relate to auditory representations of language sounds. The MGB can be divided into three main subdivisions, the ventral, dorsal, and medial subdivisions, each with different connectivity, auditory response properties, neuronal properties, and synaptic properties. Together, the MGB subdivisions actively and dynamically shape complex auditory processing and form ongoing communication loops with auditory cortex and subcortical structures.},
  number = {1},
  urldate = {2019-06-24},
  journal = {Brain and Language},
  doi = {10.1016/j.bandl.2013.03.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0093934X13000722},
  author = {Bartlett, Edward L.},
  month = jul,
  year = {2013},
  keywords = {Inferior colliculus,Corticothalamic,Thalamocortical,Vocalizations,Calbindin,Amplitude modulation,Marmoset,Monotonic,Thalamic reticular nucleus},
  pages = {29-48},
  file = {/Users/jonny/Papers/BartlettE/2013/Bartlett_2013_The organization and physiology of the auditory thalamus and its role in4.pdf;/Users/jonny/Zotero/storage/6RAC5AMD/S0093934X13000722.html}
}

@misc{IssuesRepresentationTime,
  title = {Issues on the Representation of Time and Structure in Music: {{Contemporary Music Review}}: {{Vol}} 9, {{No}} 1-2},
  urldate = {2019-06-27},
  url = {https://www.tandfonline.com/doi/abs/10.1080/07494469300640461},
  file = {/Users/jonny/Zotero/storage/FBHSIDL8/07494469300640461.html}
}

@article{toddCognitiveTheoryExpression1989,
  title = {Towards a Cognitive Theory of Expression: {{The}} Performance and Perception of Rubato},
  volume = {4},
  issn = {0749-4467},
  shorttitle = {Towards a Cognitive Theory of Expression},
  abstract = {Expression is examined from the viewpoint of communication theory and it is argued that a proper understanding of expression involves an integrated description of both performance and perception. A framework is developed in which to couch a general theory of expression. As an example, a number of algorithms, implemented in Lisp are described which model the performance and perception of rubato. The model is based on two factors: 1) the use of ``phrase final lengthening'' to signal a group boundary and 2) the ability of the listener to track a variable tempo. The study shows that rubato is a rich source of information for the listener and that any realistic music parser must take this into account. On the other hand any performance model must take into account the constraints of perception.},
  number = {1},
  urldate = {2019-06-27},
  journal = {Contemporary Music Review},
  doi = {10.1080/07494468900640451},
  url = {https://doi.org/10.1080/07494468900640451},
  author = {Todd, Neil},
  month = jan,
  year = {1989},
  keywords = {Cognition,music perception,computational modeling,music performance,musical expression,rubato},
  pages = {405-416},
  file = {/Users/jonny/Zotero/storage/JXXDIATX/07494468900640451.html}
}

@article{iyerEmbodiedMindSituated2002,
  title = {Embodied {{Mind}}, {{Situated Cognition}}, and {{Expressive Microtiming}} in {{African}}-{{American Music}}},
  volume = {19},
  copyright = {\textcopyright\textcopyright{} Regents of the University of California},
  issn = {0730-7829, 1533-8312},
  abstract = {Skip to Next Section
The dual theories of embodied mind and situated cognition, in which physical/temporal embodiment and physical/social/cultural environment contribute crucially to the structure of mind, are brought to bear on issues in music perception. It is argued that cognitive universals grounded in human bodily experience are tempered by the cultural specificity that constructs the role of the body in musical performance. Special focus is given to microrhythmic techniques in specific forms of African-American music, using audio examples created by the author or sampled from well-known jazz recordings.},
  language = {en},
  number = {3},
  urldate = {2019-06-27},
  journal = {Music Perception: An Interdisciplinary Journal},
  doi = {10.1525/mp.2002.19.3.387},
  url = {https://mp.ucpress.edu/content/19/3/387},
  author = {Iyer, Vijay},
  month = mar,
  year = {2002},
  pages = {387-414},
  file = {/Users/jonny/Papers/IyerV/2002/Iyer_2002_Embodied Mind, Situated Cognition, and Expressive Microtiming in.pdf;/Users/jonny/Zotero/storage/H2WTB8XR/387.html}
}

@article{pressingBlackAtlanticRhythm2002,
  title = {Black {{Atlantic Rhythm}}: {{Its Computational}} and {{Transcultural Foundations}}},
  volume = {19},
  copyright = {\textcopyright\textcopyright{} Regents of the University of California},
  issn = {0730-7829, 1533-8312},
  shorttitle = {Black {{Atlantic Rhythm}}},
  abstract = {Skip to Next Section
The "Black Atlantic" rhythmic diaspora, be it realized in jazz, blues, gospel, reggae, rock, candombl{\'e}{\'e}, cumbia, hip-hop or whatever, seems to have widespread capacity to facilitate dance, engagement, social interaction, expression and catharsis. This article examines the reasons for this. Black Atlantic rhythm is founded on the idea of groove or feel, which forms a kinetic framework for reliable prediction of events and time pattern communication, its power cemented by repetition and engendered movement. Overlaid on this are characteristic devices that include syncopation, overlay,displacement, off-beat phrasing, polyrhythm/polymeter, hocketing, heterophony, swing, speech-based rhythms, and call-and-response. Using an evolutionary argument, I point out here that nearly all of these have at their heart the establishment of perceptual multiplicity or rivalry, affecting expectation, which acts as either a message or a message enhancement technique (via increased engagement and focusing of attention), or both. The causal path for the remaining devices is based on adopting structures shared with speech, notably prosody, conversational interaction, and narrative. Several examples illustrate how, particularly in jazz and jazz-related forms, extensions and relatively complex creative adaptations of traditional African and African diasporic rhythmic techniques are a natural consequence of a culture of questioning and reflection that encompasses maintenance of historical reference and accommodation to innovation.},
  language = {en},
  number = {3},
  urldate = {2019-06-27},
  journal = {Music Perception: An Interdisciplinary Journal},
  doi = {10.1525/mp.2002.19.3.285},
  url = {https://mp.ucpress.edu/content/19/3/285},
  author = {Pressing, Jeff},
  month = mar,
  year = {2002},
  pages = {285-310},
  file = {/Users/jonny/Papers/PressingJ/2002/Pressing_2002_Black Atlantic Rhythm.pdf;/Users/jonny/Zotero/storage/QFBY759G/285.html}
}

@article{daviesEffectMicrotimingDeviations2013,
  title = {The {{Effect}} of {{Microtiming Deviations}} on the {{Perception}} of {{Groove}} in {{Short Rhythms}}},
  volume = {30},
  copyright = {\textcopyright{} 2013 by The Regents of the University of California},
  issn = {0730-7829, 1533-8312},
  abstract = {Skip to Next Section
Groove is a sensation of movement or wanting to move when we listen to certain types of music; it is central to the appreciation of many styles such as Jazz, Funk, Latin, and many more. To better understand the mechanisms that lead to the sensation of groove, we explore the relationship between groove and systematic microtiming deviations. Manifested as small, intentional deviations in timing, systematic microtiming is widely considered within the music community to be a critical component of music performances that groove. To investigate the effect of microtiming on the perception of groove we synthesized typical rhythm patterns for Jazz, Funk, and Samba with idiomatic microtiming deviation patterns for each style. The magnitude of the deviations was parametrically varied from nil to about double the natural level. In two experiments, untrained listeners and experts listened to all combinations of same and different music and microtiming style and magnitude combinations, and rated liking, groove, naturalness, and speed. Contrary to a common and frequently expressed belief in the literature, systematic microtiming led to decreased groove ratings, as well as liking and naturalness, with the exception of the simple short-long shuffle Jazz pattern. A comparison of the ratings between the two listener groups revealed this effect to be stronger for the expert listener group than for the untrained listeners, suggesting that musical expertise plays an important role in the perception and appreciation of microtiming in rhythmic patterns.},
  language = {en},
  number = {5},
  urldate = {2019-06-27},
  journal = {Music Perception: An Interdisciplinary Journal},
  doi = {10.1525/mp.2013.30.5.497},
  url = {https://mp.ucpress.edu/content/30/5/497},
  author = {Davies, Matthew and Madison, Guy and Silva, Pedro and Gouyon, Fabien},
  month = jun,
  year = {2013},
  keywords = {rhythm,movement,groove,listening experiment,microtiming},
  pages = {497-510},
  file = {/Users/jonny/Papers/DaviesM/2013/Davies_2013_The Effect of Microtiming Deviations on the Perception of Groove in Short.pdf;/Users/jonny/Zotero/storage/LNVDMKKE/497.html}
}

@article{trainorPrimalRoleVestibular2009,
  series = {Special {{Issue}} on "{{The Rhythmic Brain}}"},
  title = {The Primal Role of the Vestibular System in Determining Musical Rhythm},
  volume = {45},
  issn = {0010-9452},
  abstract = {Previous studies have indicated that physical movement on either every second or on every third beat of an unaccented auditory rhythm pattern can disambiguate whether it is perceived in duple time as a march or in triple time as a waltz. Here we demonstrate that this disambiguation can also be accomplished by direct galvanic stimulation of the vestibular system. The galvanically induced sensation, without any actual movement, that the head moved from side to side on either every second or on every third beat of the ambiguous auditory rhythm pattern strongly biased whether adults perceived it as being in duple or in triple time. These results imply that the vestibular system plays a primal role in the perception of musical rhythm.},
  number = {1},
  urldate = {2019-06-27},
  journal = {Cortex},
  doi = {10.1016/j.cortex.2007.10.014},
  url = {http://www.sciencedirect.com/science/article/pii/S0010945208002463},
  author = {Trainor, Laurel J. and Gao, Xiaoqing and Lei, Jing-jiang and Lehtovaara, Karen and Harris, Laurence R.},
  month = jan,
  year = {2009},
  keywords = {Galvanic stimulation,Musical rhythm,Vestibular stimulus,Vestibulo-auditory interaction},
  pages = {35-43},
  file = {/Users/jonny/Papers/TrainorL/2009/Trainor_2009_The primal role of the vestibular system in determining musical rhythm.pdf;/Users/jonny/Zotero/storage/VWCHY4DY/S0010945208002463.html}
}

@article{claytonFreeRhythmEthnomusicology1996,
  title = {Free {{Rhythm}}: {{Ethnomusicology}} and the {{Study}} of {{Music}} without {{Metre}}},
  volume = {59},
  issn = {0041-977X},
  shorttitle = {Free {{Rhythm}}},
  number = {2},
  urldate = {2019-06-27},
  journal = {Bulletin of the School of Oriental and African Studies, University of London},
  url = {https://www.jstor.org/stable/619715},
  author = {Clayton, Martin R. L.},
  year = {1996},
  pages = {323-332}
}

@article{belloFivePerspectivesMusical2015,
  title = {Five {{Perspectives}} on {{Musical Rhythm}}},
  volume = {44},
  issn = {0929-8215},
  number = {1},
  urldate = {2019-06-27},
  journal = {Journal of New Music Research},
  doi = {10.1080/09298215.2014.996572},
  url = {https://doi.org/10.1080/09298215.2014.996572},
  author = {Bello, Juan P. and Rowe, Robert and Guedes, Carlos and Toussaint, Godfried},
  month = jan,
  year = {2015},
  pages = {1-2},
  file = {/Users/jonny/Papers/BelloJ/2015/Bello_2015_Five Perspectives on Musical Rhythm.pdf;/Users/jonny/Zotero/storage/DKFBL7A8/09298215.2014.html}
}

@article{patelEvolutionaryNeuroscienceMusical2014,
  title = {The Evolutionary Neuroscience of Musical Beat Perception: The {{Action Simulation}} for {{Auditory Prediction}} ({{ASAP}}) Hypothesis},
  volume = {8},
  issn = {1662-5137},
  shorttitle = {The Evolutionary Neuroscience of Musical Beat Perception},
  abstract = {Every human culture has some form of music with a beat: a perceived periodic pulse that structures the perception of musical rhythm and which serves as a framework for synchronized movement to music. What are the neural mechanisms of musical beat perception, and how did they evolve? One view, which dates back to Darwin and implicitly informs some current models of beat perception, is that the relevant neural mechanisms are relatively general and are widespread among animal species. On the basis of recent neural and cross-species data on musical beat processing, this paper argues for a different view. Here we argue that beat perception is a complex brain function involving temporally-precise communication between auditory regions and motor planning regions of the cortex (even in the absence of overt movement). More specifically, we propose that simulation of periodic movement in motor planning regions provides a neural signal that helps the auditory system predict the timing of upcoming beats. This ``action simulation for auditory prediction'' (ASAP) hypothesis leads to testable predictions. We further suggest that ASAP relies on dorsal auditory pathway connections between auditory regions and motor planning regions via the parietal cortex, and suggest that these connections may be stronger in humans than in non-human primates due to the evolution of vocal learning in our lineage. This suggestion motivates cross-species research to determine which species are capable of human-like beat perception, i.e., beat perception that involves accurate temporal prediction of beat times across a fairly broad range of tempi.},
  urldate = {2019-06-27},
  journal = {Frontiers in Systems Neuroscience},
  doi = {10.3389/fnsys.2014.00057},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4026735/},
  author = {Patel, Aniruddh D. and Iversen, John R.},
  month = may,
  year = {2014},
  file = {/Users/jonny/Papers/PatelA/2014/Patel_2014_The evolutionary neuroscience of musical beat perception.pdf},
  pmid = {24860439},
  pmcid = {PMC4026735}
}

@article{hsuSelfsimilarityNoiseCalled1991,
  title = {Self-Similarity of the "1/f Noise" Called Music.},
  volume = {88},
  issn = {0027-8424, 1091-6490},
  abstract = {Author Information
K J Hs{\"u} and A Hs{\"u}Eidgen{\"o}ssische Technische Hochschule, Z{\"u}rich, Switzerland.},
  language = {en},
  number = {8},
  urldate = {2019-06-27},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.88.8.3507},
  url = {https://www.pnas.org/content/88/8/3507},
  author = {Hs{\"u}, K. J. and Hs{\"u}, A.},
  month = apr,
  year = {1991},
  pages = {3507-3509},
  file = {/Users/jonny/Papers/HsÃ¼K/1991/HsÃ¼_1991_Self-similarity of the 1-f noise called music.pdf;/Users/jonny/Zotero/storage/LBHK5JZL/tab-article-info.html},
  pmid = {11607178}
}

@article{brownUniversalsWorldMusics2013,
  title = {Universals in the World's Musics},
  volume = {41},
  issn = {0305-7356},
  abstract = {Many decades of skepticism have prevented the field of musicology from embracing the importance of musical universals. When universals have been discussed, it has generally been in the form of meta-critiques about the concept of universals, rather than in positive proposals about actual universals. We present here a typology of four categories of musical universals and a listing of 70 putative universals in musics cross-culturally. These universals span a wide variety of features, including pitch, rhythm, melodic structure, form, vocal style, expressive devices, instruments, performance contexts, contents, and behaviors.},
  language = {en},
  number = {2},
  urldate = {2019-06-27},
  journal = {Psychology of Music},
  doi = {10.1177/0305735611425896},
  url = {https://doi.org/10.1177/0305735611425896},
  author = {Brown, Steven and Jordania, Joseph},
  month = mar,
  year = {2013},
  pages = {229-248},
  file = {/Users/jonny/Papers/BrownS/2013/Brown_2013_Universals in the worldâs musics.pdf}
}

@article{middlebrooksSoundLocalizationHuman1991,
  title = {Sound {{Localization}} by {{Human Listeners}}},
  volume = {42},
  issn = {0066-4308},
  number = {1},
  urldate = {2019-06-28},
  journal = {Annual Review of Psychology},
  doi = {10.1146/annurev.ps.42.020191.001031},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.ps.42.020191.001031},
  author = {Middlebrooks, John C. and Green, David M.},
  month = jan,
  year = {1991},
  pages = {135-159},
  file = {/Users/jonny/Papers/MiddlebrooksJ/1991/Middlebrooks_1991_Sound Localization by Human Listeners.pdf;/Users/jonny/Zotero/storage/9S9KYK2C/annurev.ps.42.020191.html}
}

@article{anthwalDevelopmentMammalianOuter2016,
  title = {The Development of the Mammalian Outer and Middle~Ear},
  volume = {228},
  issn = {0021-8782},
  abstract = {The mammalian ear is a complex structure divided into three main parts: the outer; middle; and inner ear. These parts are formed from all three germ layers and neural crest cells, which have to integrate successfully in order to form a fully functioning organ of hearing. Any defect in development of the outer and middle ear leads to conductive hearing loss, while defects in the inner ear can lead to sensorineural hearing loss. This review focuses on the development of the parts of the ear involved with sound transduction into the inner ear, and the parts largely ignored in the world of hearing research: the outer and middle ear. The published data on the embryonic origin, signalling, genetic control, development and timing of the mammalian middle and outer ear are reviewed here along with new data showing the Eustachian tube cartilage is of dual embryonic origin. The embryonic origin of some of these structures has only recently been uncovered (Science, 339, 2013, 1453; Development, 140, 2013, 4386), while the molecular mechanisms controlling the growth, structure and integration of many outer and middle ear components are hardly known. The genetic analysis of outer and middle ear development is rather limited, with a small number of genes often affecting either more than one part of the ear or having only very small effects on development. This review therefore highlights the necessity for further research into the development of outer and middle ear structures, which will be important for the understanding and treatment of conductive hearing loss.},
  number = {2},
  urldate = {2019-06-28},
  journal = {Journal of Anatomy},
  doi = {10.1111/joa.12344},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4718165/},
  author = {Anthwal, Neal and Thompson, Hannah},
  month = feb,
  year = {2016},
  pages = {217-232},
  file = {/Users/jonny/Papers/AnthwalN/2016/Anthwal_2016_The development of the mammalian outer and middle ear.pdf},
  pmid = {26227955},
  pmcid = {PMC4718165}
}

@article{websterEarStructureFunction1966,
  title = {Ear {{Structure}} and {{Function}} in {{Modern Mammals}}},
  volume = {6},
  issn = {1540-7063},
  abstract = {Abstract.  The acoustic portions of the mammalian ear display greater morphological diversity in peripheral than in central portions. In many mammals the pinna},
  language = {en},
  number = {3},
  urldate = {2019-06-28},
  journal = {Integrative and Comparative Biology},
  doi = {10.1093/icb/6.3.451},
  url = {https://academic.oup.com/icb/article/6/3/451/127315},
  author = {Webster, Douglas B.},
  month = aug,
  year = {1966},
  pages = {451-466},
  file = {/Users/jonny/Papers/WebsterD/1966/Webster_1966_Ear Structure and Function in Modern Mammals.pdf;/Users/jonny/Zotero/storage/IAQWEBLY/127315.html}
}

@article{kettenStructureFunctionWhale1997,
  title = {Structure and {{Function}} in {{Whale Ears}}},
  volume = {8},
  issn = {0952-4622},
  abstract = {Ultrasonic echolocation abilities are well documented in several dolphin species, but hearing characteristics are unknown for most whales. Vocalization data suggest whale hearing spans infra- to ultrasonic ranges. This paper presents an overview of whale ear anatomy and analyzes 1) how whale ears are adapted for underwater hearing and 2) how inner ear differences relate to different hearing capacities among whales. Whales have adaptations for rapid, deep diving and long submersion; e.g., broad- bore Eustachian tubes, no pinnae, and no air-filled external canals, that impact sound reception. In odontocetes, two soft tissue channels conduct sound to the ear. In mysticetes, bone and soft tissue conduction are likely. The middle ear is air-filled but has an extensible mucosa. Cochlear structures are hypertrophied and vestibular components are reduced. Auditory ganglion cell densities are double land mammal averages (2000\textendash{}4000/mm). Basilar membrane lengths range 20\textendash{}70 mm; gradients are larger than in terrestrial mammals. Odontocetes have 20\textendash{}60\% bony membrane support and basal ratios {$>$}0.6, consistent with hearing {$>$}150 kHz. Mysticetes have apical ratios {$<$}0.002 and no bony lateral support, implying acute infrasonic hearing. Cochlear hypertrophy may be adaptive for high background noise. Vestibular loss is consistent with cervical fusion. Exceptionally high auditory fiber counts suggest both mysticetes and odontocetes have ears ``wired'' for more complex signal processing mechanisms than most land mammals.},
  number = {1-2},
  urldate = {2019-06-28},
  journal = {Bioacoustics},
  doi = {10.1080/09524622.1997.9753356},
  url = {https://doi.org/10.1080/09524622.1997.9753356},
  author = {KETTEN, DARLENE R.},
  month = jan,
  year = {1997},
  keywords = {auditory nerve,cochlea,auditory system,basilar membrane,cetacean ear,inner ear,mysticete,odontocete},
  pages = {103-135},
  file = {/Users/jonny/Zotero/storage/DAG2ZVA5/09524622.1997.html}
}

@book{cheveignePitchPerception2010,
  title = {Pitch Perception},
  urldate = {2019-07-01},
  publisher = {{Oxford University Press}},
  url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199233557.001.0001/oxfordhb-9780199233557-e-04},
  author = {de Cheveign{\'e}, Alain},
  month = jan,
  year = {2010},
  doi = {10.1093/oxfordhb/9780199233557.013.0004}
}

@article{larsenPitchRepresentationsAuditory2008,
  title = {Pitch {{Representations}} in the {{Auditory Nerve}}: {{Two Concurrent Complex Tones}}},
  volume = {100},
  issn = {0022-3077},
  shorttitle = {Pitch {{Representations}} in the {{Auditory Nerve}}},
  abstract = {Pitch differences between concurrent sounds are important cues used in auditory scene analysis and also play a major role in music perception. To investigate the neural codes underlying these perceptual abilities, we recorded from single fibers in the cat auditory nerve in response to two concurrent harmonic complex tones with missing fundamentals and equal-amplitude harmonics. We investigated the efficacy of rate-place and interspike-interval codes to represent both pitches of the two tones, which had fundamental frequency (F0) ratios of 15/14 or 11/9. We relied on the principle of scaling invariance in cochlear mechanics to infer the spatiotemporal response patterns to a given stimulus from a series of measurements made in a single fiber as a function of F0. Templates created by a peripheral auditory model were used to estimate the F0s of double complex tones from the inferred distribution of firing rate along the tonotopic axis. This rate-place representation was accurate for F0s {$\greaterequivlnt$}900 Hz. Surprisingly, rate-based F0 estimates were accurate even when the two-tone mixture contained no resolved harmonics, so long as some harmonics were resolved prior to mixing. We also extended methods used previously for single complex tones to estimate the F0s of concurrent complex tones from interspike-interval distributions pooled over the tonotopic axis. The interval-based representation was accurate for F0s {$\lessequivlnt$}900 Hz, where the two-tone mixture contained no resolved harmonics. Together, the rate-place and interval-based representations allow accurate pitch perception for concurrent sounds over the entire range of human voice and cat vocalizations.},
  number = {3},
  urldate = {2019-07-01},
  journal = {Journal of Neurophysiology},
  doi = {10.1152/jn.01361.2007},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2544468/},
  author = {Larsen, Erik and Cedolin, Leonardo and Delgutte, Bertrand},
  month = sep,
  year = {2008},
  pages = {1301-1319},
  pmid = {18632887},
  pmcid = {PMC2544468}
}

@article{terhardtPitchConsonanceHarmony1974,
  title = {Pitch, Consonance, and Harmony},
  volume = {55},
  issn = {0001-4966},
  number = {5},
  urldate = {2019-07-01},
  journal = {The Journal of the Acoustical Society of America},
  doi = {10.1121/1.1914648},
  url = {https://asa.scitation.org/doi/10.1121/1.1914648},
  author = {Terhardt, Ernst},
  month = may,
  year = {1974},
  pages = {1061-1069},
  file = {/Users/jonny/Zotero/storage/H28A4XD5/1.html}
}

@article{oxenhamRevisitingPlaceTemporal2013,
  title = {Revisiting Place and Temporal Theories of Pitch},
  volume = {34},
  issn = {1346-3969},
  abstract = {The nature of pitch and its neural coding have been studied for over a century. A popular debate has revolved around the question of whether pitch is coded via ``place'' cues in the cochlea, or via timing cues in the auditory nerve. In the most recent incarnation of this debate, the role of temporal fine structure has been emphasized in conveying important pitch and speech information, particularly because the lack of temporal fine structure coding in cochlear implants might explain some of the difficulties faced by cochlear implant users in perceiving music and pitch contours in speech. In addition, some studies have postulated that hearing-impaired listeners may have a specific deficit related to processing temporal fine structure. This article reviews some of the recent literature surrounding the debate, and argues that much of the recent evidence suggesting the importance of temporal fine structure processing can also be accounted for using spectral (place) or temporal-envelope cues.},
  number = {6},
  urldate = {2019-07-01},
  journal = {Acoustical science and technology / edited by the Acoustical Society of Japan},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4215732/},
  author = {Oxenham, Andrew J.},
  year = {2013},
  pages = {388-396},
  file = {/Users/jonny/Papers/OxenhamA/2013/Oxenham_2013_Revisiting place and temporal theories of pitch.pdf},
  pmid = {25364292},
  pmcid = {PMC4215732}
}

@article{frisinaSubcorticalNeuralCoding2001,
  title = {Subcortical Neural Coding Mechanisms for Auditory Temporal Processing},
  volume = {158},
  issn = {0378-5955},
  abstract = {Biologically relevant sounds such as speech, animal vocalizations and music have distinguishing temporal features that are utilized for effective auditory perception. Common temporal features include sound envelope fluctuations, often modeled in the laboratory by amplitude modulation (AM), and starts and stops in ongoing sounds, which are frequently approximated by hearing researchers as gaps between two sounds or are investigated in forward masking experiments. The auditory system has evolved many neural processing mechanisms for encoding important temporal features of sound. Due to rapid progress made in the field of auditory neuroscience in the past three decades, it is not possible to review all progress in this field in a single article. The goal of the present report is to focus on single-unit mechanisms in the mammalian brainstem auditory system for encoding AM and gaps as illustrative examples of how the system encodes key temporal features of sound. This report, following a systems analysis approach, starts with findings in the auditory nerve and proceeds centrally through the cochlear nucleus, superior olivary complex and inferior colliculus. Some general principles can be seen when reviewing this entire field. For example, as one ascends the central auditory system, a neural encoding shift occurs. An emphasis on synchronous responses for temporal coding exists in the auditory periphery, and more reliance on rate coding occurs as one moves centrally. In addition, for AM, modulation transfer functions become more bandpass as the sound level of the signal is raised, but become more lowpass in shape as background noise is added. In many cases, AM coding can actually increase in the presence of background noise. For gap processing or forward masking, coding for gaps changes from a decrease in spike firing rate for neurons of the peripheral auditory system that have sustained response patterns, to an increase in firing rate for more central neurons with transient responses. Lastly, for gaps and forward masking, as one ascends the auditory system, some suppression effects become quite long (echo suppression), and in some stimulus configurations enhancement to a second sound can take place.},
  number = {1},
  urldate = {2019-07-02},
  journal = {Hearing Research},
  doi = {10.1016/S0378-5955(01)00296-9},
  url = {http://www.sciencedirect.com/science/article/pii/S0378595501002969},
  author = {Frisina, Robert D.},
  month = aug,
  year = {2001},
  keywords = {Hearing,Aging,Amplitude modulation,Ear,Forward masking,Deafness},
  pages = {1-27},
  file = {/Users/jonny/Papers/FrisinaR/2001/Frisina_2001_Subcortical neural coding mechanisms for auditory temporal processing.pdf;/Users/jonny/Zotero/storage/HQX7KAVI/S0378595501002969.html}
}

@article{musacchiaMusiciansHaveEnhanced2007,
  title = {Musicians Have Enhanced Subcortical Auditory and Audiovisual Processing of Speech and Music},
  volume = {104},
  copyright = {\textcopyright{} 2007 by The National Academy of Sciences of the USA},
  issn = {0027-8424, 1091-6490},
  abstract = {Musical training is known to modify cortical organization. Here, we show that such modifications extend to subcortical sensory structures and generalize to processing of speech. Musicians had earlier and larger brainstem responses than nonmusician controls to both speech and music stimuli presented in auditory and audiovisual conditions, evident as early as 10 ms after acoustic onset. Phase-locking to stimulus periodicity, which likely underlies perception of pitch, was enhanced in musicians and strongly correlated with length of musical practice. In addition, viewing videos of speech (lip-reading) and music (instrument being played) enhanced temporal and frequency encoding in the auditory brainstem, particularly in musicians. These findings demonstrate practice-related changes in the early sensory encoding of auditory and audiovisual information.},
  language = {en},
  number = {40},
  urldate = {2019-07-02},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0701498104},
  url = {https://www.pnas.org/content/104/40/15894},
  author = {Musacchia, Gabriella and Sams, Mikko and Skoe, Erika and Kraus, Nina},
  month = oct,
  year = {2007},
  keywords = {plasticity,visual,brainstem,multisensory language},
  pages = {15894-15898},
  file = {/Users/jonny/Papers/MusacchiaG/2007/Musacchia_2007_Musicians have enhanced subcortical auditory and audiovisual processing of.pdf;/Users/jonny/Zotero/storage/4TKZYBJF/15894.html},
  pmid = {17898180}
}

@article{leeSelectiveSubcorticalEnhancement2009,
  title = {Selective {{Subcortical Enhancement}} of {{Musical Intervals}} in {{Musicians}}},
  volume = {29},
  copyright = {Copyright \textcopyright{} 2009 Society for Neuroscience 0270-6474/09/295832-09\$15.00/0},
  issn = {0270-6474, 1529-2401},
  abstract = {By measuring the auditory brainstem response to two musical intervals, the major sixth (E3 and G2) and the minor seventh (E3 and F\#2), we found that musicians have a more specialized sensory system for processing behaviorally relevant aspects of sound. Musicians had heightened responses to the harmonics of the upper tone (E), as well as certain combination tones (sum tones) generated by nonlinear processing in the auditory system. In music, the upper note is typically carried by the upper voice, and the enhancement of the upper tone likely reflects musicians' extensive experience attending to the upper voice. Neural phase locking to the temporal periodicity of the amplitude-modulated envelope, which underlies the perception of musical harmony, was also more precise in musicians than nonmusicians. Neural enhancements were strongly correlated with years of musical training, and our findings, therefore, underscore the role that long-term experience with music plays in shaping auditory sensory encoding.},
  language = {en},
  number = {18},
  urldate = {2019-07-02},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.6133-08.2009},
  url = {http://www.jneurosci.org/content/29/18/5832},
  author = {Lee, Kyung Myun and Skoe, Erika and Kraus, Nina and Ashley, Richard},
  month = may,
  year = {2009},
  pages = {5832-5840},
  file = {/Users/jonny/Papers/LeeK/2009/Lee_2009_Selective Subcortical Enhancement of Musical Intervals in Musicians.pdf;/Users/jonny/Zotero/storage/37IPZID2/5832.html},
  pmid = {19420250}
}

@incollection{picklesChapterAuditoryPathways2015,
  series = {The {{Human Auditory System}}},
  title = {Chapter 1 - {{Auditory}} Pathways: Anatomy and Physiology},
  volume = {129},
  shorttitle = {Chapter 1 - {{Auditory}} Pathways},
  abstract = {This chapter outlines the anatomy and physiology of the auditory pathways. After a brief analysis of the external, middle ears, and cochlea, the responses of auditory nerve fibers are described. The central nervous system is analyzed in more detail. A scheme is provided to help understand the complex and multiple auditory pathways running through the brainstem. The multiple pathways are based on the need to preserve accurate timing while extracting complex spectral patterns in the auditory input. The auditory nerve fibers branch to give two pathways, a ventral sound-localizing stream, and a dorsal mainly pattern recognition stream, which innervate the different divisions of the cochlear nucleus. The outputs of the two streams, with their two types of analysis, are progressively combined in the inferior colliculus and onwards, to produce the representation of what can be called the ``auditory objects'' in the external world. The progressive extraction of critical features in the auditory stimulus in the different levels of the central auditory system, from cochlear nucleus to auditory cortex, is described. In addition, the auditory centrifugal system, running from cortex in multiple stages to the organ of Corti of the cochlea, is described.},
  urldate = {2019-07-02},
  booktitle = {Handbook of {{Clinical Neurology}}},
  publisher = {{Elsevier}},
  url = {http://www.sciencedirect.com/science/article/pii/B9780444626301000019},
  author = {Pickles, James O.},
  editor = {Aminoff, Michael J. and Boller, Fran{\c c}ois and Swaab, Dick F.},
  month = jan,
  year = {2015},
  keywords = {physiology,auditory cortex,Hearing,inferior colliculus,review,cochlea,cochlear nucleus,medial geniculate,anatomy,superior olive},
  pages = {3-25},
  file = {/Users/jonny/Papers/PicklesJ/2015/Pickles_2015_Chapter 1 - Auditory pathways.pdf;/Users/jonny/Zotero/storage/KS927QD3/B9780444626301000019.html},
  doi = {10.1016/B978-0-444-62630-1.00001-9}
}

@article{ciumanEfferentSystemOlivocochlear2010,
  title = {The {{Efferent System}} or {{Olivocochlear Function Bundle}} \textendash{} {{Fine Regulator}} and {{Protector}} of {{Hearing Perception}}},
  volume = {6},
  issn = {1550-9702},
  abstract = {The efferent system of the ear possesses several distinct functions, in particular noise protection, mediation of selective attention and improvement of signal to noise ratio. It also supports adaptation and frequency selectivity by modification of the micromechanical properties of outer hair cells. There are many differences in anatomy and physiology between the medial and lateral olivocochlear system suggesting that they are functionally separate systems. The efferent system is affected by inner ear stressors, e.g. noise, ototoxic drugs, and might play a key role in tinnitus generation and maintenance. The anatomy, physiology and its realtionships to inner ear pathologies are discussed in this review article.},
  number = {4},
  urldate = {2019-07-02},
  journal = {International Journal of Biomedical Science : IJBS},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615293/},
  author = {Ciuman, Raphael Richard},
  month = dec,
  year = {2010},
  pages = {276-288},
  file = {/Users/jonny/Papers/CiumanR/2010/Ciuman_2010_The Efferent System or Olivocochlear Function Bundle â Fine Regulator and.pdf},
  pmid = {23675203},
  pmcid = {PMC3615293}
}

@article{kawaseAntimaskingEffectsOlivocochlear1993,
  title = {Antimasking Effects of the Olivocochlear Reflex. {{II}}. {{Enhancement}} of Auditory-Nerve Response to Masked Tones},
  volume = {70},
  issn = {0022-3077},
  abstract = {1. The antimasking effects of olivocochlear (OC) efferent feedback were studied in anesthetized or decerebrate cats by comparing responses of single auditory-nerve fibers (ANFs) to tone bursts in continuous masking noise seen with and without addition of a moderate-level contralateral noise known to activate the OC reflex. Responses were measured as a function of tone-burst intensity, tone-burst frequency, and masker intensity and were analyzed so as to allow quantitative estimates of the detectability of the tone bursts against the noise background. 2. Addition of the contralateral OC elicitor both increased the maximum discharge rates to the masked tone bursts and decreased the rates to the ipsilateral masker. The rate increases to the tone bursts could be explained on the basis of a decrease in adaptation caused by decreasing the steady response to the masker. The result is a steepening of the rate-versus-level function for masked tone bursts and a concomitant increase in the estimated discriminability of small increments of tone-burst intensity. 3. For tone bursts at the fiber's characteristic frequency (CF), the OC effects on detection threshold for the masked tone bursts depended on masker level, with small increases in threshold for low masker levels and somewhat larger decreases in threshold for higher masker levels. For tone bursts below CF, OC effects, when present, always decreased the detection threshold. 4. The largest antimasking effects were seen for fibers with CFs between 6 and 12 kHz and for masker levels within 20 dB of the fiber's threshold to the masker. These trends appeared to hold for fibers of all spontaneous rates (SRs). 5. Enhancement of the response to unmasked tone bursts and concomitant decrease in the "spontaneous rate" was elicited by OC activation in fibers if threshold sensitivity approached -10 dB SPL. This "enhancement-in-quiet" appears to arise when an animal-generated noise produces a continuous response (in the absence of purposely applied sound) that is suppressed by OC activity. This finding raises questions as to the range of "true" spontaneous rates in the cat. 6. The results highlight two important distinctions between the effects of OC feedback in quiet versus those in noise. In quiet, the effects are predominately suppressive and are restricted to stimuli at frequencies near a fiber's CF and at intensities within its dynamic range. In continuous background noise, the OC reflex can enhance the responses to transient stimuli. Such effects are seen throughout the fiber's response area.(ABSTRACT TRUNCATED AT 400 WORDS)},
  language = {eng},
  number = {6},
  journal = {Journal of Neurophysiology},
  doi = {10.1152/jn.1993.70.6.2533},
  author = {Kawase, T. and Delgutte, B. and Liberman, M. C.},
  month = dec,
  year = {1993},
  keywords = {Animals,Membrane Potentials,Cats,Attention,Female,Pitch Discrimination,Auditory Threshold,Loudness Perception,Male,Olivary Nucleus,Vestibulocochlear Nerve,Perceptual Masking,Efferent Pathways,Functional Laterality,Cochlear Nerve,Reflex; Acoustic},
  pages = {2533-2549},
  pmid = {8120597}
}

@article{winslowSingletoneIntensityDiscrimination1988,
  title = {Single-Tone Intensity Discrimination Based on Auditory-Nerve Rate Responses in Backgrounds of Quiet, Noise, and with Stimulation of the Crossed Olivocochlear Bundle},
  volume = {35},
  issn = {0378-5955},
  abstract = {We use simple statistical models of the firing patterns of high, medium, and low spontaneous rate auditory-nerve fibers to study mechanisms which determine the overall dynamic range of the auditory periphery. The models relate experimentally measured rate response properties of fibers with best frequency (BF) near 8.0 kHz to their ability to encode changes in BF tone level by changes in discharge rate in backgrounds of quiet and noise, with and without electrical stimulation of the crossed olivocochlear bundle (COCB). Application of the models to the BF tone rate responses of auditory-nerve fibers in backgrounds of quiet shows that optimum processing of the rate responses of fibers with BF near 8.0 kHz yields performance in the intensity discrimination task meeting or exceeding that of human subjects over an 80 dB range of levels. By defining a statistical measure of dynamic range, we confirm the results of Costalupes et al. (1984) demonstrating that masking noise shifts the dynamic range of auditory-nerve fibers to higher stimulus levels, thus preventing rate saturation. However, model analysis shows that masking noise also produces large reductions of dynamic range as well as large increases in the minimum intensity difference that can be encoded by the rate responses of single and ensembles of fibers. Electrical stimulation of the COCB can restore auditory-nerve fiber dynamic range and sensitivity to changes in BF tone level in noise backgrounds, in some cases to roughly that observed in backgrounds of quiet.},
  number = {2},
  urldate = {2019-07-02},
  journal = {Hearing Research},
  doi = {10.1016/0378-5955(88)90116-5},
  url = {http://www.sciencedirect.com/science/article/pii/0378595588901165},
  author = {Winslow, Raimond L and Sachs, Murray B},
  month = sep,
  year = {1988},
  keywords = {Crossed olivocochlear bundle,Dynamic range,Intensity discrimination},
  pages = {165-189},
  file = {/Users/jonny/Papers/WinslowR/1988/Winslow_1988_Single-tone intensity discrimination based on auditory-nerve rate responses in.pdf;/Users/jonny/Zotero/storage/DMBECRGY/0378595588901165.html}
}

@article{parkInterauralLevelDifference2004,
  title = {Interaural Level Difference Processing in the Lateral Superior Olive and the Inferior Colliculus},
  volume = {92},
  issn = {0022-3077},
  abstract = {Interaural level differences (ILDs) provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILDs are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. However, in many cases ILD sensitivity is also influenced by overall intensity, which challenges the idea of unambiguous ILD coding. We investigated whether ambiguity is reduced from one synaptic level to another for 2 centers in the so-called ILD processing pathway. We recorded from single cells in the free-tailed bat lateral superior olive (LSO), the first station where ILDs are coded, and the central nucleus of the inferior colliculus (ICC), which receives a strong projection from the LSO, as well as convergent projections from many other auditory centers. We assessed effects of overall intensity by comparing ILD functions generated with different fixed intensities to the excitatory ear. LSO cells were characterized by functions that shifted in a systematic manner with increasing intensity to the excitatory ear. In contrast, significantly more ICC cells had functions that were stable across overall sound intensity, indicating that hierarchical transformations increase stability. Furthermore, a population analysis based on proportion of active cells indicated that stability in the ICC was greatly enhanced when overall population activity was considered.},
  language = {eng},
  number = {1},
  journal = {Journal of Neurophysiology},
  doi = {10.1152/jn.00961.2003},
  author = {Park, Thomas J. and Klug, Achim and Holinstat, Michael and Grothe, Benedikt},
  month = jul,
  year = {2004},
  keywords = {Animals,Action Potentials,Acoustic Stimulation,Auditory Perception,Olivary Nucleus,Chiroptera,Echolocation,Inferior Colliculi},
  pages = {289-301},
  pmid = {15056693}
}

@article{parkInterauralLevelDifference2004a,
  title = {Interaural {{Level Difference Processing}} in the {{Lateral Superior Olive}} and the {{Inferior Colliculus}}},
  volume = {92},
  issn = {0022-3077},
  abstract = {Interaural level differences (ILDs) provide salient cues for localizing high-frequency sounds in space, and populations of neurons that are sensitive to ILDs are found at almost every synaptic level from brain stem to cortex. These cells are predominantly excited by stimulation of one ear and predominantly inhibited by stimulation of the other ear, such that the magnitude of their response is determined in large part by the intensities at the 2 ears. However, in many cases ILD sensitivity is also influenced by overall intensity, which challenges the idea of unambiguous ILD coding. We investigated whether ambiguity is reduced from one synaptic level to another for 2 centers in the so-called ILD processing pathway. We recorded from single cells in the free-tailed bat lateral superior olive (LSO), the first station where ILDs are coded, and the central nucleus of the inferior colliculus (ICC), which receives a strong projection from the LSO, as well as convergent projections from many other auditory centers. We assessed effects of overall intensity by comparing ILD functions generated with different fixed intensities to the excitatory ear. LSO cells were characterized by functions that shifted in a systematic manner with increasing intensity to the excitatory ear. In contrast, significantly more ICC cells had functions that were stable across overall sound intensity, indicating that hierarchical transformations increase stability. Furthermore, a population analysis based on proportion of active cells indicated that stability in the ICC was greatly enhanced when overall population activity was considered.},
  number = {1},
  urldate = {2019-07-02},
  journal = {Journal of Neurophysiology},
  doi = {10.1152/jn.00961.2003},
  url = {https://www.physiology.org/doi/full/10.1152/jn.00961.2003},
  author = {Park, Thomas J. and Klug, Achim and Holinstat, Michael and Grothe, Benedikt},
  month = jul,
  year = {2004},
  pages = {289-301},
  file = {/Users/jonny/Papers/ParkT/2004/Park_2004_Interaural Level Difference Processing in the Lateral Superior Olive and the.pdf;/Users/jonny/Zotero/storage/D8E8G8WF/jn.00961.html}
}

@article{dyckImpactBassDrum2013,
  title = {The {{Impact}} of the {{Bass Drum}} on {{Human Dance Movement}}},
  volume = {30},
  copyright = {\textcopyright{} 2013 by The Regents of the University of California},
  issn = {0730-7829, 1533-8312},
  abstract = {Skip to Next Section
The present study aims to gain better insight into the connection between music and dance by examining the dynamic effects of the bass drum on a dancing audience in a club-like environment. One hundred adult participants moved freely in groups of five to a musical sequence that comprised six songs. Each song consisted of one section that was repeated three times, each time with a different sound pressure level of the bass drum. Hip and head movements were recorded using motion capture and motion sensing. The study demonstrates that people modify their bodily behavior according to the dynamic level of the bass drum when moving to contemporary dance music in a social context. Participants moved more actively and displayed a higher degree of tempo entrainment as the sound pressure level of the bass drum increased. These results indicate that the prominence of the bass drum in contemporary dance music serves not merely as a stylistic element; indeed, it has a strong influence on dancing itself.},
  language = {en},
  number = {4},
  urldate = {2019-07-02},
  journal = {Music Perception: An Interdisciplinary Journal},
  doi = {10.1525/mp.2013.30.4.349},
  url = {https://mp.ucpress.edu/content/30/4/349},
  author = {Dyck, Edith Van and Moelants, Dirk and Demey, Michiel and Deweppe, Alexander and Coussement, Pieter and Leman, Marc},
  month = apr,
  year = {2013},
  keywords = {entrainment,activity count,bass drum,dance,joint action},
  pages = {349-359},
  file = {/Users/jonny/Papers/DyckE/2013/Dyck_2013_The Impact of the Bass Drum on Human Dance Movement.pdf;/Users/jonny/Zotero/storage/K3AFXJ9H/349.html}
}

@article{phillips-silverFeelingBeatMovement2005,
  title = {Feeling the {{Beat}}: {{Movement Influences Infant Rhythm Perception}}},
  volume = {308},
  copyright = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  shorttitle = {Feeling the {{Beat}}},
  abstract = {We hear the melody in music, but we feel the beat. We demonstrate that the perception of musical rhythm is a multisensory experience in infancy. In particular, movement of the body, by bouncing on every second versus every third beat of an ambiguous auditory rhythm pattern, influences whether that auditory rhythm pattern is encoded in duple form (a march) or in triple form (a waltz). Visual information is not necessary for the effect, indicating that it likely reflects a strong, early-developing interaction between auditory and vestibular information in the human nervous system.
Infants bounced in a waltz (three-beat) or a march (two-beat) rhythm prefer the corresponding pattern in an otherwise ambiguous series of sounds.
Infants bounced in a waltz (three-beat) or a march (two-beat) rhythm prefer the corresponding pattern in an otherwise ambiguous series of sounds.},
  language = {en},
  number = {5727},
  urldate = {2019-07-02},
  journal = {Science},
  doi = {10.1126/science.1110922},
  url = {https://science.sciencemag.org/content/308/5727/1430},
  author = {{Phillips-Silver}, Jessica and Trainor, Laurel J.},
  month = jun,
  year = {2005},
  pages = {1430-1430},
  file = {/Users/jonny/Papers/Phillips-SilverJ/2005/Phillips-Silver_2005_Feeling the Beat.pdf;/Users/jonny/Zotero/storage/NLBKIGF2/1430.html},
  pmid = {15933193}
}

@article{phillips-silverVestibularInfluenceAuditory2008,
  title = {Vestibular Influence on Auditory Metrical Interpretation},
  volume = {67},
  issn = {0278-2626},
  abstract = {When we move to music we feel the beat, and this feeling can shape the sound we hear. Previous studies have shown that when people listen to a metrically ambiguous rhythm pattern, moving the body on a certain beat\textemdash{}adults, by actively bouncing themselves in synchrony with the experimenter, and babies, by being bounced passively in the experimenter's arms\textemdash{}can bias their auditory metrical representation so that they interpret the pattern in a corresponding metrical form [Phillips-Silver, J., \& Trainor, L. J. (2005). Feeling the beat: Movement influences infant rhythm perception. Science, 308, 1430; Phillips-Silver, J., \& Trainor, L. J. (2007). Hearing what the body feels: Auditory encoding of rhythmic movement. Cognition, 105, 533\textendash{}546]. The present studies show that in adults, as well as in infants, metrical encoding of rhythm can be biased by passive motion. Furthermore, because movement of the head alone affected auditory encoding whereas movement of the legs alone did not, we propose that vestibular input may play a key role in the effect of movement on auditory rhythm processing. We discuss possible cortical and subcortical sites for the integration of auditory and vestibular inputs that may underlie the interaction between movement and auditory metrical rhythm perception.},
  number = {1},
  urldate = {2019-07-02},
  journal = {Brain and Cognition},
  doi = {10.1016/j.bandc.2007.11.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262607001832},
  author = {{Phillips-Silver}, Jessica and Trainor, Laurel J.},
  month = jun,
  year = {2008},
  keywords = {Music,Movement,Rhythm,Auditory system,Metrical structure,Vestibular system},
  pages = {94-102},
  file = {/Users/jonny/Papers/Phillips-SilverJ/2008/Phillips-Silver_2008_Vestibular influence on auditory metrical interpretation.pdf;/Users/jonny/Zotero/storage/CRQJV8BL/S0278262607001832.html}
}

@article{norman-haignereDivergenceFunctionalOrganization2019a,
  title = {Divergence in the Functional Organization of Human and Macaque Auditory Cortex Revealed by {{fMRI}} Responses to Harmonic Tones},
  volume = {22},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  issn = {1546-1726},
  abstract = {Norman-Haignere et al. report that humans but not macaque monkeys possess cortical regions with a strong preference for harmonic tones compared to noise. This species difference may be driven by the demands of speech and music perception in humans.},
  language = {En},
  number = {7},
  urldate = {2019-07-02},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-019-0410-7},
  url = {https://www.nature.com/articles/s41593-019-0410-7},
  author = {{Norman-Haignere}, Sam V. and Kanwisher, Nancy and McDermott, Josh H. and Conway, Bevil R.},
  month = jul,
  year = {2019},
  pages = {1057},
  file = {/Users/jonny/Papers/Norman-HaignereS/2019/Norman-Haignere_2019_Divergence in the functional organization of human and macaque auditory cortex.pdf;/Users/jonny/Zotero/storage/RDTBDGCX/s41593-019-0410-7.html}
}

@article{martinez-molinaNeuralCorrelatesSpecific2016a,
  title = {Neural Correlates of Specific Musical Anhedonia},
  volume = {113},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  issn = {0027-8424, 1091-6490},
  abstract = {Although music is ubiquitous in human societies, there are some people for whom music holds no reward value despite normal perceptual ability and preserved reward-related responses in other domains. The study of these individuals with specific musical anhedonia may be crucial to understand better the neural correlates underlying musical reward. Previous neuroimaging studies have shown that musically induced pleasure may arise from the interaction between auditory cortical networks and mesolimbic reward networks. If such interaction is critical for music-induced pleasure to emerge, then those individuals who do not experience it should show alterations in the cortical-mesolimbic response. In the current study, we addressed this question using fMRI in three groups of 15 participants, each with different sensitivity to music reward. We demonstrate that the music anhedonic participants showed selective reduction of activity for music in the nucleus accumbens (NAcc), but normal activation levels for a monetary gambling task. Furthermore, this group also exhibited decreased functional connectivity between the right auditory cortex and ventral striatum (including the NAcc). In contrast, individuals with greater than average response to music showed enhanced connectivity between these structures. Thus, our results suggest that specific musical anhedonia may be associated with a reduction in the interplay between the auditory cortex and the subcortical reward network, indicating a pivotal role of this interaction for the enjoyment of music.},
  language = {en},
  number = {46},
  urldate = {2019-07-02},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1611211113},
  url = {https://www.pnas.org/content/113/46/E7337},
  author = {{Mart{\'i}nez-Molina}, Noelia and {Mas-Herrero}, Ernest and {Rodr{\'i}guez-Fornells}, Antoni and Zatorre, Robert J. and {Marco-Pallar{\'e}s}, Josep},
  month = nov,
  year = {2016},
  keywords = {reward,music,emotion,anhedonia},
  pages = {E7337-E7345},
  file = {/Users/jonny/Papers/MartÃ­nez-MolinaN/2016/MartÃ­nez-Molina_2016_Neural correlates of specific musical anhedonia2.pdf;/Users/jonny/Zotero/storage/DARJM9BA/E7337.html},
  pmid = {27799544}
}

@article{koelschInvestigatingNeuralEncoding2018,
  title = {Investigating the {{Neural Encoding}} of {{Emotion}} with {{Music}}},
  volume = {98},
  issn = {0896-6273},
  abstract = {Does our understanding of the human brain remain incomplete without a proper understanding of how the brain processes music? Here, the author makes a passionate plea for the use of music in the investigation of human emotion and its brain correlates, arguing that music can change activity in all brain structures associated with emotions, which has important implications on how we understand human emotions and their disorders and how we can make better use of beneficial effects of music in therapy.},
  number = {6},
  urldate = {2019-07-02},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2018.04.029},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627318303374},
  author = {Koelsch, Stefan},
  month = jun,
  year = {2018},
  keywords = {amygdala,hippocampus,nucleus accumbens,brain,music,emotion,attachment-related emotions,reward system},
  pages = {1075-1079},
  file = {/Users/jonny/Papers/KoelschS/2018/Koelsch_2018_Investigating the Neural Encoding of Emotion with Music.pdf;/Users/jonny/Zotero/storage/JTGL6TZJ/S0896627318303374.html}
}

@article{DistributedNeuralSignatures2017,
  title = {Distributed Neural Signatures of Natural Audiovisual Speech and Music in the Human Auditory Cortex},
  volume = {157},
  issn = {1053-8119},
  abstract = {During a conversation or when listening to music, auditory and visual information are combined automatically into audiovisual objects. However, it is \ldots{}},
  language = {en},
  urldate = {2019-07-02},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2016.12.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811916307121},
  month = aug,
  year = {2017},
  pages = {108-117},
  file = {/Users/jonny/Zotero/storage/GAJ8MAC5/S1053811916307121.html}
}

@article{kingRecentAdvancesUnderstanding2018,
  title = {Recent Advances in Understanding the Auditory Cortex},
  volume = {7},
  issn = {2046-1402},
  abstract = {Our ability to make sense of the auditory world results from neural processing that begins in the ear, goes through multiple subcortical areas, and continues in the cortex. The specific contribution of the auditory cortex to this chain of processing is far from understood. Although many of the properties of neurons in the auditory cortex resemble those of subcortical neurons, they show somewhat more complex selectivity for sound features, which is likely to be important for the analysis of natural sounds, such as speech, in real-life listening conditions. Furthermore, recent work has shown that auditory cortical processing is highly context-dependent, integrates auditory inputs with other sensory and motor signals, depends on experience, and is shaped by cognitive demands, such as attention. Thus, in addition to being the locus for more complex sound selectivity, the auditory cortex is increasingly understood to be an integral part of the network of brain regions responsible for prediction, auditory perceptual decision-making, and learning. In this review, we focus on three key areas that are contributing to this understanding: the sound features that are preferentially represented by cortical neurons, the spatial organization of those preferences, and the cognitive roles of the auditory cortex.},
  urldate = {2019-07-02},
  journal = {F1000Research},
  doi = {10.12688/f1000research.15580.1},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6173113/},
  author = {King, Andrew J. and Teki, Sundeep and Willmore, Ben D.B.},
  month = sep,
  year = {2018},
  file = {/Users/jonny/Papers/KingA/2018/King_2018_Recent advances in understanding the auditory cortex.pdf},
  pmid = {30345008},
  pmcid = {PMC6173113}
}

@article{bartonOrthogonalAcousticDimensions2012,
  title = {Orthogonal Acoustic Dimensions Define Auditory Field Maps in Human Cortex},
  volume = {109},
  issn = {1091-6490},
  abstract = {The functional organization of human auditory cortex has not yet been characterized beyond a rudimentary level of detail. Here, we use functional MRI to measure the microstructure of orthogonal tonotopic and periodotopic gradients forming complete auditory field maps (AFMs) in human core and belt auditory cortex. These AFMs show clear homologies to subfields of auditory cortex identified in nonhuman primates and in human cytoarchitectural studies. In addition, we present measurements of the macrostructural organization of these AFMs into "clover leaf" clusters, consistent with the macrostructural organization seen across human visual cortex. As auditory cortex is at the interface between peripheral hearing and central processes, improved understanding of the organization of this system could open the door to a better understanding of the transformation from auditory spectrotemporal signals to higher-order information such as speech categories.},
  language = {eng},
  number = {50},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  doi = {10.1073/pnas.1213381109},
  author = {Barton, Brian and Venezia, Jonathan H. and Saberi, Kourosh and Hickok, Gregory and Brewer, Alyssa A.},
  month = dec,
  year = {2012},
  keywords = {Animals,Acoustic Stimulation,Brain Mapping,Auditory Cortex,Humans,Psychoacoustics,Female,Magnetic Resonance Imaging,Male,Adult,Haplorhini},
  pages = {20738-20743},
  file = {/Users/jonny/Papers/BartonB/2012/Barton_2012_Orthogonal acoustic dimensions define auditory field maps in human cortex.pdf},
  pmid = {23188798},
  pmcid = {PMC3528571}
}

@article{baumannTopographyFrequencyTime2015,
  title = {The Topography of Frequency and Time Representation in Primate Auditory Cortices},
  volume = {4},
  issn = {2050-084X},
  abstract = {Natural sounds can be characterised by their spectral content and temporal modulation, but how the brain is organized to analyse these two critical sound dimensions remains uncertain. Using functional magnetic resonance imaging, we demonstrate a topographical representation of amplitude modulation rate in the auditory cortex of awake macaques. The representation of this temporal dimension is organized in approximately concentric bands of equal rates across the superior temporal plane in both hemispheres, progressing from high rates in the posterior core to low rates in the anterior core and lateral belt cortex. In A1 the resulting gradient of modulation rate runs approximately perpendicular to the axis of the tonotopic gradient, suggesting an orthogonal organisation of spectral and temporal sound dimensions. In auditory belt areas this relationship is more complex. The data suggest a continuous representation of modulation rate across several physiological areas, in contradistinction to a separate representation of frequency within each area.},
  language = {eng},
  journal = {eLife},
  doi = {10.7554/eLife.03256},
  author = {Baumann, Simon and Joly, Olivier and Rees, Adrian and Petkov, Christopher I. and Sun, Li and Thiele, Alexander and Griffiths, Timothy D.},
  month = jan,
  year = {2015},
  keywords = {Animals,Time Factors,Acoustic Stimulation,Auditory Cortex,auditory cortex,topography,macaque,Auditory Perception,fMRI,neuroscience,Male,tonotopy,Macaca,amplitude modulation},
  file = {/Users/jonny/Papers/BaumannS/2015/Baumann_2015_The topography of frequency and time representation in primate auditory cortices.pdf},
  pmid = {25590651},
  pmcid = {PMC4398946}
}

@article{jasminUnderstandingRostralCaudal2019a,
  title = {Understanding Rostral\textendash{}Caudal Auditory Cortex Contributions to Auditory Perception},
  volume = {20},
  copyright = {2019 Springer Nature Limited},
  issn = {1471-0048},
  abstract = {How is the processing of auditory information by the cortex organized? Scott and colleagues describe differences in the connectivity and properties of the rostral and caudal auditory cortex and propose links to the functional specializations of the rostral and caudal auditory streams.},
  language = {En},
  number = {7},
  urldate = {2019-07-03},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/s41583-019-0160-2},
  url = {https://www.nature.com/articles/s41583-019-0160-2},
  author = {Jasmin, Kyle and Lima, C{\'e}sar F. and Scott, Sophie K.},
  month = jul,
  year = {2019},
  pages = {425},
  file = {/Users/jonny/Papers/JasminK/2019/Jasmin_2019_Understanding rostralâcaudal auditory cortex contributions to auditory2.pdf;/Users/jonny/Zotero/storage/C7F48KZG/s41583-019-0160-2.html}
}

@article{mckettonLargerAuditoryCortical2019,
  title = {Larger {{Auditory Cortical Area}} and {{Broader Frequency Tuning Underlie Absolute Pitch}}},
  volume = {39},
  copyright = {Copyright \textcopyright{} 2019 the authors},
  issn = {0270-6474, 1529-2401},
  abstract = {Absolute pitch (AP), the ability of some musicians to precisely identify and name musical tones in isolation, is associated with a number of gross morphological changes in the brain, but the fundamental neural mechanisms underlying this ability have not been clear. We presented a series of logarithmic frequency sweeps to age- and sex-matched groups of musicians with or without AP and controls without musical training. We used fMRI and population receptive field (pRF) modeling to measure the responses in the auditory cortex in 61 human subjects. The tuning response of each fMRI voxel was characterized as Gaussian, with independent center frequency and bandwidth parameters. We identified three distinct tonotopic maps, corresponding to primary (A1), rostral (R), and rostral-temporal (RT) regions of auditory cortex. We initially hypothesized that AP abilities might manifest in sharper tuning in the auditory cortex. However, we observed that AP subjects had larger cortical area, with the increased area primarily devoted to broader frequency tuning. We observed anatomically that A1, R and RT were significantly larger in AP musicians than in non-AP musicians or control subjects, which did not differ significantly from each other. The increased cortical area in AP in areas A1 and R were primarily low frequency and broadly tuned, whereas the distribution of responses in area RT did not differ significantly. We conclude that AP abilities are associated with increased early auditory cortical area devoted to broad-frequency tuning and likely exploit increased ensemble encoding.
SIGNIFICANCE STATEMENT Absolute pitch (AP), the ability of some musicians to precisely identify and name musical tones in isolation, is associated with a number of gross morphological changes in the brain, but the fundamental neural mechanisms have not been clear. Our study shows that AP musicians have significantly larger volume in early auditory cortex than non-AP musicians and non-musician controls and that this increased volume is primarily devoted to broad-frequency tuning. We conclude that AP musicians are likely able to exploit increased ensemble representations to encode and identify frequency.},
  language = {en},
  number = {15},
  urldate = {2019-07-03},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.1532-18.2019},
  url = {https://www.jneurosci.org/content/39/15/2930},
  author = {McKetton, Larissa and DeSimone, Kevin and Schneider, Keith A.},
  month = apr,
  year = {2019},
  keywords = {auditory cortex,tonotopy,music,absolute pitch,Heschl's gyrus,tuning sharpness},
  pages = {2930-2937},
  file = {/Users/jonny/Papers/McKettonL/2019/McKetton_2019_Larger Auditory Cortical Area and Broader Frequency Tuning Underlie Absolute.pdf;/Users/jonny/Zotero/storage/D4Y7W77R/2930.html},
  pmid = {30745420}
}

@article{deutschAbsolutePitchAmerican2006,
  title = {Absolute Pitch among {{American}} and {{Chinese}} Conservatory Students: Prevalence Differences, and Evidence for a Speech-Related Critical Period},
  volume = {119},
  issn = {0001-4966},
  shorttitle = {Absolute Pitch among {{American}} and {{Chinese}} Conservatory Students},
  abstract = {Absolute pitch is extremely rare in the U.S. and Europe; this rarity has so far been unexplained. This paper reports a substantial difference in the prevalence of absolute pitch in two normal populations, in a large-scale study employing an on-site test, without self-selection from within the target populations. Music conservatory students in the U.S. and China were tested. The Chinese subjects spoke the tone language Mandarin, in which pitch is involved in conveying the meaning of words. The American subjects were nontone language speakers. The earlier the age of onset of musical training, the greater the prevalence of absolute pitch; however, its prevalence was far greater among the Chinese than the U.S. students for each level of age of onset of musical training. The findings suggest that the potential for acquiring absolute pitch may be universal, and may be realized by enabling infants to associate pitches with verbal labels during the critical period for acquisition of features of their native language.},
  language = {eng},
  number = {2},
  journal = {The Journal of the Acoustical Society of America},
  doi = {10.1121/1.2151799},
  author = {Deutsch, Diana and Henthorn, Trevor and Marvin, Elizabeth and Xu, HongShuai},
  month = feb,
  year = {2006},
  keywords = {Humans,Female,Pitch Perception,Male,Music,Language,Adult,Adolescent,Speech Acoustics,China,Ethnic Groups,Prevalence,Students,United States},
  pages = {719-722},
  pmid = {16521731}
}

@article{aljanakiDevelopingBenchmarkEmotional2017,
  title = {Developing a Benchmark for Emotional Analysis of Music},
  volume = {12},
  issn = {1932-6203},
  abstract = {Music emotion recognition (MER) field rapidly expanded in the last decade. Many new methods and new audio features are developed to improve the performance of MER algorithms. However, it is very difficult to compare the performance of the new methods because of the data representation diversity and scarcity of publicly available data. In this paper, we address these problems by creating a data set and a benchmark for MER. The data set that we release, a MediaEval Database for Emotional Analysis in Music (DEAM), is the largest available data set of dynamic annotations (valence and arousal annotations for 1,802 songs and song excerpts licensed under Creative Commons with 2Hz time resolution). Using DEAM, we organized the `Emotion in Music' task at MediaEval Multimedia Evaluation Campaign from 2013 to 2015. The benchmark attracted, in total, 21 active teams to participate in the challenge. We analyze the results of the benchmark: the winning algorithms and feature-sets. We also describe the design of the benchmark, the evaluation procedures and the data cleaning and transformations that we suggest. The results from the benchmark suggest that the recurrent neural network based approaches combined with large feature-sets work best for dynamic MER.},
  language = {en},
  number = {3},
  urldate = {2019-07-04},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0173392},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0173392},
  author = {Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
  month = mar,
  year = {2017},
  keywords = {Algorithms,Emotions,Acoustics,Neural networks,Linear regression analysis,Bioacoustics,Music perception,Machine learning algorithms},
  pages = {e0173392},
  file = {/Users/jonny/Papers/AljanakiA/2017/Aljanaki_2017_Developing a benchmark for emotional analysis of music.pdf;/Users/jonny/Zotero/storage/EQF78CWP/article.html}
}

@misc{MusicOurEars,
  title = {Music in {{Our Ears}}: {{The Biological Bases}} of {{Musical Timbre Perception}}},
  urldate = {2019-07-06},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002759}
}

@misc{MusicOurEarsa,
  title = {Music in {{Our Ears}}: {{The Biological Bases}} of {{Musical Timbre Perception}}},
  urldate = {2019-07-06},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002759},
  file = {/Users/jonny/Zotero/storage/PKYBT6MD/Music in Our Ears The Biological Bases of Musical.PDF}
}

@article{patilMusicOurEars2012a,
  title = {Music in {{Our Ears}}: {{The Biological Bases}} of {{Musical Timbre Perception}}},
  volume = {8},
  issn = {1553-7358},
  shorttitle = {Music in {{Our Ears}}},
  abstract = {Timbre is the attribute of sound that allows humans and other animals to distinguish among different sound sources. Studies based on psychophysical judgments of musical timbre, ecological analyses of sound's physical characteristics as well as machine learning approaches have all suggested that timbre is a multifaceted attribute that invokes both spectral and temporal sound features. Here, we explored the neural underpinnings of musical timbre. We used a neuro-computational framework based on spectro-temporal receptive fields, recorded from over a thousand neurons in the mammalian primary auditory cortex as well as from simulated cortical neurons, augmented with a nonlinear classifier. The model was able to perform robust instrument classification irrespective of pitch and playing style, with an accuracy of 98.7\%. Using the same front end, the model was also able to reproduce perceptual distance judgments between timbres as perceived by human listeners. The study demonstrates that joint spectro-temporal features, such as those observed in the mammalian primary auditory cortex, are critical to provide the rich-enough representation necessary to account for perceptual judgments of timbre by human listeners, as well as recognition of musical instruments.},
  language = {en},
  number = {11},
  urldate = {2019-07-06},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1002759},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002759},
  author = {Patil, Kailash and Pressnitzer, Daniel and Shamma, Shihab and Elhilali, Mounya},
  month = nov,
  year = {2012},
  keywords = {Auditory cortex,Perception,Neurophysiology,Acoustics,Bioacoustics,Music perception,Sensory perception,Support vector machines},
  pages = {e1002759},
  file = {/Users/jonny/Papers/PatilK/2012/Patil_2012_Music in Our Ears2.pdf;/Users/jonny/Zotero/storage/UDMPTTQ8/article.html}
}

@article{chowningCenterComputerResearch2000,
  title = {The {{Center}} for {{Computer Research}} in {{Music}} and {{Acoustics}} ({{CCRMA}}) {{Stanford University}}},
  abstract = {The early years of digital sound synthesis were filled with promise following Max Mathews' publication in 1963 of his pioneering work at Bell Telephone Laboratories [1]. The digital control of loudspeakers allowed for the production of any conceivable sound given the correct sequence of numbers (samples). Producing the correct sequence of numbers, however, turned out to be a formidable task. Acoustics and psychoacoustics, the first a well-developed field of knowledge and the second less so, did not provide information at the level of detail required to simulate even the simplest sound of an acoustic instrument.},
  language = {en},
  author = {Chowning, John M},
  year = {2000},
  pages = {6},
  file = {/Users/jonny/Zotero/storage/KNU6IKKN/Chowning - 2000 - The Center for Computer Research in Music and Acou.pdf}
}

@article{fletcherEvolutionMusicalInstruments2012,
  title = {The Evolution of Musical Instruments},
  volume = {132},
  issn = {0001-4966},
  number = {3},
  urldate = {2019-07-07},
  journal = {The Journal of the Acoustical Society of America},
  doi = {10.1121/1.4755634},
  url = {https://asa.scitation.org/doi/10.1121/1.4755634},
  author = {Fletcher, Neville H.},
  month = sep,
  year = {2012},
  pages = {2071-2071},
  file = {/Users/jonny/Zotero/storage/DQLNZNRA/1.html}
}

@article{reisenweaverDevelopmentFluteSolo2011,
  title = {The {{Development}} of the {{Flute}} as a {{Solo Instrument}} from the {{Medieval}} to the {{Baroque Era}}},
  volume = {2},
  issn = {2167-3799},
  number = {1},
  journal = {Musical Offerings},
  doi = {10.15385/jmo.2011.2.1.2},
  url = {https://digitalcommons.cedarville.edu/musicalofferings/vol2/iss1/2},
  author = {Reisenweaver, Anna},
  month = may,
  year = {2011},
  file = {/Users/jonny/Papers/ReisenweaverA/2011/Reisenweaver_2011_The Development of the Flute as a Solo Instrument from the Medieval to the.pdf;/Users/jonny/Zotero/storage/FHT8T29E/2.html}
}

@article{brewsterExperimentallyDerivedGuidelines,
  title = {Experimentally {{Derived Guidelines}} for the {{Creation}} of {{Earcons}}},
  abstract = {There is a lack of guidelines for designers to use when creating sounds for their interfaces. This paper proposes a set of general guidelines for the creation of earcons based upon six experiments we have performed. Using them a designer will be able to create effective earcons.},
  language = {en},
  author = {Brewster, Stephen A and Wright, Peter C and Edwards, Alistair D N},
  pages = {3},
  file = {/Users/jonny/Zotero/storage/F7GCCIRE/Brewster et al. - Experimentally Derived Guidelines for the Creation.pdf}
}

@article{walkerSpearconsSpeechbasedEarcons2006,
  title = {Spearcons: Speech-Based Earcons Improve Navigation Performance in Auditory Menus},
  shorttitle = {Spearcons},
  abstract = {With shrinking displays and increasing technology use by visually impaired users, it is important to improve usability with non-GUI interfaces such as menus. Using non-speech sounds called earcons or auditory icons has been proposed to enhance menu navigation. We compared search time and accuracy of menu navigation using four types of auditory representations: speech only; hierarchical earcons; auditory icons; and a new type called spearcons. Spearcons are created by speeding up a spoken phrase until it is not recognized as speech. Using a within-subjects design, participants searched a 5 x 5 menu for target items using each type of audio cue. Spearcons and speech-only both led to faster and more accurate menu navigation than auditory icons and hierarchical earcons. There was a significant practice effect for search time, within each type of auditory cue. These results suggest that spearcons are more effective than previous auditory cues in menu-based interfaces, and may lead to better performance and accuracy, as well as more flexible menu structures.},
  language = {en},
  urldate = {2019-07-09},
  url = {https://smartech.gatech.edu/handle/1853/50642},
  author = {Walker, Bruce N. and Nance, Amanda and Lindsay, Jeffrey},
  month = jun,
  year = {2006},
  file = {/Users/jonny/Papers/WalkerB/2006/Walker_2006_Spearcons.pdf;/Users/jonny/Zotero/storage/FWW4D452/50642.html}
}

@article{mcgookinUnderstandingConcurrentEarcons2004,
  title = {Understanding {{Concurrent Earcons}}: {{Applying Auditory Scene Analysis Principles}} to {{Concurrent Earcon Recognition}}},
  volume = {1},
  issn = {1544-3558},
  shorttitle = {Understanding {{Concurrent Earcons}}},
  abstract = {Two investigations into the identification of concurrently presented, structured sounds, called earcons were carried out. One of the experiments investigated how varying the number of concurrently presented earcons affected their identification. It was found that varying the number had a significant effect on the proportion of earcons identified. Reducing the number of concurrently presented earcons lead to a general increase in the proportion of presented earcons successfully identified. The second experiment investigated how modifying the earcons and their presentation, using techniques influenced by auditory scene analysis, affected earcon identification. It was found that both modifying the earcons such that each was presented with a unique timbre, and altering their presentation such that there was a 300 ms onset-to-onset time delay between each earcon were found to significantly increase identification. Guidelines were drawn from this work to assist future interface designers when incorporating concurrently presented earcons.},
  number = {2},
  urldate = {2019-07-09},
  journal = {ACM Trans. Appl. Percept.},
  doi = {10.1145/1024083.1024087},
  url = {http://doi.acm.org/10.1145/1024083.1024087},
  author = {McGookin, David K. and Brewster, Stephen A.},
  month = oct,
  year = {2004},
  keywords = {auditory scene analysis,auditory display,Earcons,sonification},
  pages = {130--155},
  file = {/Users/jonny/Papers/McGookinD/2004/McGookin_2004_Understanding Concurrent Earcons.pdf}
}

@article{dinglerLearnabiltiySoundCues2008,
  title = {Learnabiltiy of {{Sound Cues}} for {{Environmental Features}}: {{Auditory Icons}}, {{Earcons}}, {{Spearcons}}, and {{Speech}}},
  shorttitle = {Learnabiltiy of {{Sound Cues}} for {{Environmental Features}}},
  abstract = {Awareness of features in our environment is essential for many daily activities. While often awareness of such features comes from vision, this modality is sometimes unavailable or undesirable. In these instances, auditory cues can be an excellent method of representing environmental features. The study reported here investigated the learnability of well known (auditory icons, earcons, and speech) and more novel (spearcons, earcon-icon hybrids, and sized hybrids) sonification techniques for representing common environmental features. Spearcons, which are speech stimuli that have been greatly sped up, were found to be as learnable as speech, while earcons unsurprisingly were much more difficult to learn. Practical implications are discussed.},
  language = {en\_US},
  urldate = {2019-07-09},
  url = {https://smartech.gatech.edu/handle/1853/49940},
  author = {Dingler, Tilman and Lindsay, Jeffrey and Walker, Bruce N.},
  month = jun,
  year = {2008},
  file = {/Users/jonny/Papers/DinglerT/2008/Dingler_2008_Learnabiltiy of Sound Cues for Environmental Features.pdf;/Users/jonny/Zotero/storage/TLRQ9XNY/49940.html}
}

@article{zatorreMusicalMelodySpeech2012,
  title = {Musical {{Melody}} and {{Speech Intonation}}: {{Singing}} a {{Different Tune}}},
  volume = {10},
  issn = {1545-7885},
  shorttitle = {Musical {{Melody}} and {{Speech Intonation}}},
  abstract = {Music and speech are often cited as characteristically human forms of communication. Both share the features of hierarchical structure, complex sound systems, and sensorimotor sequencing demands, and both are used to convey and influence emotions, among other functions [1]. Both music and speech also prominently use acoustical frequency modulations, perceived as variations in pitch, as part of their communicative repertoire. Given these similarities, and the fact that pitch perception and production involve the same peripheral transduction system (cochlea) and the same production mechanism (vocal tract), it might be natural to assume that pitch processing in speech and music would also depend on the same underlying cognitive and neural mechanisms. In this essay we argue that the processing of pitch information differs significantly for speech and music; specifically, we suggest that there are two pitch-related processing systems, one for more coarse-grained, approximate analysis and one for more fine-grained accurate representation, and that the latter is unique to music. More broadly, this dissociation offers clues about the interface between sensory and motor systems, and highlights the idea that multiple processing streams are a ubiquitous feature of neuro-cognitive architectures.},
  language = {en},
  number = {7},
  urldate = {2019-07-10},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.1001372},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001372},
  author = {Zatorre, Robert J. and Baum, Shari R.},
  month = jul,
  year = {2012},
  keywords = {Speech,Cognition,Pitch perception,Brainstem,Music cognition,Bioacoustics,Music perception,Speech signal processing},
  pages = {e1001372},
  file = {/Users/jonny/Papers/ZatorreR/2012/Zatorre_2012_Musical Melody and Speech Intonation.pdf;/Users/jonny/Zotero/storage/EM56F7KX/article.html}
}

@article{bothaMusilanguageHmmmmmEvolutionary2009,
  title = {On Musilanguage/``{{Hmmmmm}}'' as an Evolutionary Precursor to Language},
  volume = {29},
  issn = {0271-5309},
  abstract = {It has been contended that modern language and music are similar in ways from which inferences can be drawn about their origin and evolution. Specifically, it has been inferred that language and music had a common precursor \textendash{} referred to by Steven Brown as ``musilanguage'' and by Steven Mithen as ``Hmmmmm''. The present article examines in some depth Brown's musilanguage model and Mithen's ``Hmmmmm'' theory as these apply to the origin and evolution of language. It does so from the perspectives of the various ways in which (i) the putative similarities between language and music are construed, (ii) some differences between language and music are accounted for, (iii) the nature of the shared precursor of language and music is portrayed, (iv) the evolution of language out of that precursor is accounted for, and (v) some core inferences are drawn. The article shows that, considered from these perspectives, the musilanguage model and ``Hmmmmm'' theory cannot be accepted in their respective current articulations.},
  number = {1},
  urldate = {2019-07-10},
  journal = {Language \& Communication},
  doi = {10.1016/j.langcom.2008.01.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0271530908000025},
  author = {Botha, Rudolf},
  month = jan,
  year = {2009},
  keywords = {Language evolution,âHmmmmmâ,Evolution of music,Musilanguage,Precursor of language,Precursor of music,Window on language evolution},
  pages = {61-76},
  file = {/Users/jonny/Papers/BothaR/2009/Botha_2009_On musilanguage-âHmmmmmâ as an evolutionary precursor to language.pdf;/Users/jonny/Zotero/storage/MRVKBNMB/S0271530908000025.html}
}

@article{ardilaHowLocalizedAre2016,
  title = {How {{Localized}} Are {{Language Brain Areas}}? {{A Review}} of {{Brodmann Areas Involvement}} in {{Oral Language}}},
  volume = {31},
  issn = {0887-6177},
  shorttitle = {How {{Localized}} Are {{Language Brain Areas}}?},
  abstract = {Abstract.  The interest in understanding how language is ``localized'' in the brain has existed for centuries. Departing from seven meta-analytic studies of funct},
  language = {en},
  number = {1},
  urldate = {2019-07-10},
  journal = {Archives of Clinical Neuropsychology},
  doi = {10.1093/arclin/acv081},
  url = {https://academic.oup.com/acn/article/31/1/112/2194573},
  author = {Ardila, Alfredo and Bernal, Byron and Rosselli, Monica},
  month = feb,
  year = {2016},
  pages = {112-122},
  file = {/Users/jonny/Papers/ArdilaA/2016/Ardila_2016_How Localized are Language Brain Areas.pdf;/Users/jonny/Zotero/storage/9LX8M6AT/2194573.html}
}

@article{peelleHemisphericLateralizationSpeech2012,
  title = {The Hemispheric Lateralization of Speech Processing Depends on What ``Speech'' Is: A Hierarchical Perspective},
  volume = {6},
  issn = {1662-5161},
  shorttitle = {The Hemispheric Lateralization of Speech Processing Depends on What ``Speech'' Is},
  abstract = {The hemispheric lateralization of speech processing depends on what ``speech'' is: a hierarchical perspective},
  language = {English},
  urldate = {2019-07-10},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2012.00309},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2012.00309/full},
  author = {Peelle, Jonathan E.},
  year = {2012},
  keywords = {Speech,Speech Perception,Language,sentence processing,lateralization},
  file = {/Users/jonny/Papers/PeelleJ/2012/Peelle_2012_The hemispheric lateralization of speech processing depends on what âspeechâ is.pdf}
}

@article{binderCurrentControversiesWernicke2017,
  title = {Current {{Controversies}} on {{Wernicke}}'s {{Area}} and Its {{Role}} in {{Language}}},
  volume = {17},
  issn = {1534-6293},
  abstract = {Purpose of ReviewThe aim of the study is to assess historical anatomical and functional definitions of Wernicke's area in light of modern lesion and neuroimaging data.Recent Findings``Wernicke's area'' has become an anatomical label usually applied to the left posterior superior temporal gyrus and adjacent supramarginal gyrus. Recent evidence shows that this region is not critical for speech perception or for word comprehension. Rather, it supports retrieval of phonological forms (mental representations of phoneme sequences), which are used for speech output and short-term memory tasks. Focal damage to this region produces phonemic paraphasia without impairing word comprehension, i.e., conduction aphasia. Neuroimaging studies in recent decades provide evidence for a widely distributed temporal, parietal, and frontal network supporting language comprehension, which does not include the anatomically defined Wernicke area.SummaryThe term Wernicke's area, if used at all, should not be used to refer to a zone critical for speech comprehension.},
  language = {en},
  number = {8},
  urldate = {2019-07-10},
  journal = {Current Neurology and Neuroscience Reports},
  doi = {10.1007/s11910-017-0764-8},
  url = {https://doi.org/10.1007/s11910-017-0764-8},
  author = {Binder, Jeffrey R.},
  month = jun,
  year = {2017},
  keywords = {Semantic memory,Phonology,Middle temporal gyrus,Superior temporal gyrus,Wernicke aphasia},
  pages = {58},
  file = {/Users/jonny/Papers/BinderJ/2017/Binder_2017_Current Controversies on Wernickeâs Area and its Role in Language.pdf}
}

@article{dewittPhonemeWordRecognition2012,
  title = {Phoneme and Word Recognition in the Auditory Ventral Stream},
  volume = {109},
  issn = {0027-8424, 1091-6490},
  abstract = {Spoken word recognition requires complex, invariant representations. Using a meta-analytic approach incorporating more than 100 functional imaging experiments, we show that preference for complex sounds emerges in the human auditory ventral stream in a hierarchical fashion, consistent with nonhuman primate electrophysiology. Examining speech sounds, we show that activation associated with the processing of short-timescale patterns (i.e., phonemes) is consistently localized to left mid-superior temporal gyrus (STG), whereas activation associated with the integration of phonemes into temporally complex patterns (i.e., words) is consistently localized to left anterior STG. Further, we show left mid- to anterior STG is reliably implicated in the invariant representation of phonetic forms and that this area also responds preferentially to phonetic sounds, above artificial control sounds or environmental sounds. Together, this shows increasing encoding specificity and invariance along the auditory ventral stream for temporally complex speech sounds.},
  language = {en},
  number = {8},
  urldate = {2019-07-10},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1113427109},
  url = {https://www.pnas.org/content/109/8/E505},
  author = {DeWitt, Iain and Rauschecker, Josef P.},
  month = feb,
  year = {2012},
  keywords = {auditory cortex,language,functional MRI,meta-analysis,object recognition},
  pages = {E505-E514},
  file = {/Users/jonny/Papers/DeWittI/2012/DeWitt_2012_Phoneme and word recognition in the auditory ventral stream.pdf;/Users/jonny/Zotero/storage/KZQYSSG6/E505.html},
  pmid = {22308358}
}

@article{blumsteindanielt.SoundArousalMusic2012,
  title = {The Sound of Arousal in Music Is Context-Dependent},
  volume = {8},
  abstract = {Humans, and many non-human animals, produce and respond to harsh, unpredictable, nonlinear sounds when alarmed, possibly because these are produced when acoustic production systems (vocal cords and syrinxes) are overblown in stressful, dangerous situations. Humans can simulate nonlinearities in music and soundtracks through the use of technological manipulations. Recent work found that film soundtracks from different genres differentially contain such sounds. We designed two experiments to determine specifically how simulated nonlinearities in soundtracks influence perceptions of arousal and valence. Subjects were presented with emotionally neutral musical exemplars that had neither noise nor abrupt frequency transitions, or versions of these musical exemplars that had noise or abrupt frequency upshifts or downshifts experimentally added. In a second experiment, these acoustic exemplars were paired with benign videos. Judgements of both arousal and valence were altered by the addition of these simulated nonlinearities in the first, music-only, experiment. In the second, multi-modal, experiment, valence (but not arousal) decreased with the addition of noise or frequency downshifts. Thus, the presence of a video image suppressed the ability of simulated nonlinearities to modify arousal. This is the first study examining how nonlinear simulations in music affect emotional judgements. These results demonstrate that the perception of potentially fearful or arousing sounds is influenced by the perceptual context and that the addition of a visual modality can antagonistically suppress the response to an acoustic stimulus.},
  number = {5},
  urldate = {2019-07-11},
  journal = {Biology Letters},
  doi = {10.1098/rsbl.2012.0374},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsbl.2012.0374},
  author = {{Blumstein Daniel T.} and {Bryant Gregory A.} and {Kaye Peter}},
  month = oct,
  year = {2012},
  pages = {744-747},
  file = {/Users/jonny/Papers/Blumstein Daniel T./2012/Blumstein Daniel T._2012_The sound of arousal in music is context-dependent.pdf;/Users/jonny/Zotero/storage/THWFEVK8/rsbl.2012.html}
}

@article{blumsteinFilmSoundtracksContain2010a,
  title = {Do Film Soundtracks Contain Nonlinear Analogues to Influence Emotion?},
  volume = {6},
  issn = {1744-957X},
  abstract = {A variety of vertebrates produce nonlinear vocalizations when they are under duress. By their very nature, vocalizations containing nonlinearities may sound harsh and are somewhat unpredictable; observations that are consistent with them being particularly evocative to those hearing them. We tested the hypothesis that humans capitalize on this seemingly widespread vertebrate response by creating nonlinear analogues in film soundtracks to evoke particular emotions. We used lists of highly regarded films to generate a set of highly ranked action/adventure, dramatic, horror and war films. We then scored the presence of a variety of nonlinear analogues in these film soundtracks. Dramatic films suppressed noise of all types, contained more abrupt frequency transitions and musical sidebands, and fewer noisy screams than expected. Horror films suppressed abrupt frequency transitions and musical sidebands, but had more non-musical sidebands, and noisy screams than expected. Adventure films had more male screams than expected. Together, our results suggest that film-makers manipulate sounds to create nonlinear analogues in order to manipulate our emotional responses.},
  language = {eng},
  number = {6},
  journal = {Biology Letters},
  doi = {10.1098/rsbl.2010.0333},
  author = {Blumstein, Daniel T. and Davitian, Richard and Kaye, Peter D.},
  month = dec,
  year = {2010},
  keywords = {Animals,Humans,Nonlinear Dynamics,Male,Music,Sound,Emotions,Acoustics,Motion Pictures},
  pages = {751-754},
  file = {/Users/jonny/Papers/BlumsteinD/2010/Blumstein_2010_Do film soundtracks contain nonlinear analogues to influence emotion2.pdf},
  pmid = {20504815},
  pmcid = {PMC3001365}
}

@article{bryantAnimalSignalsEmotion2013,
  title = {Animal Signals and Emotion in Music: Coordinating Affect across Groups},
  volume = {4},
  issn = {1664-1078},
  shorttitle = {Animal Signals and Emotion in Music},
  abstract = {Researchers studying the emotional impact of music have not traditionally been concerned with the principled relationship between form and function in evolved animal signals. The acoustic structure of musical forms is related in important ways to emotion perception, and thus research on nonhuman animal vocalizations is relevant for understanding emotion in music. Musical behavior occurs in cultural contexts that include many other coordinated activities which mark group identity, and can allow people to communicate within and between social alliances. The emotional impact of music might be best understood as a proximate mechanism serving an ultimately social function. Here I describe recent work that reveals intimate connections between properties of certain animal signals and evocative aspects of human music, including 1) examinations of the role of nonlinearities (e.g., broadband noise) in nonhuman animal vocalizations, and the analogous production and perception of these features in human music, and 2) an analysis of group musical performances and possible relationships to nonhuman animal chorusing and emotional contagion effects. Communicative features in music are likely due primarily to evolutionary byproducts of phylogenetically older, but still intact communication systems. But in some cases, such as the coordinated rhythmic sounds produced by groups of musicians, our appreciation and emotional engagement might be due to the operation of an adaptive social signaling system. Future empirical work should examine human musical behavior through the comparative lens of behavioral ecology and an adaptationist cognitive science. By this view, particular coordinated sound combinations generated by musicians exploit evolved perceptual response biases\textemdash{}many shared across species\textemdash{}and proliferate through cultural evolutionary processes.},
  language = {English},
  urldate = {2019-07-11},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2013.00990},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00990/full?utm_source=newsletter\&utm_medium=web\&utm_campaign=Psychology-w6-2014\#h3},
  author = {Bryant, Gregory A.},
  year = {2013},
  keywords = {Arousal,nonlinearities,coalition signaling,emotion in music,music distortion},
  file = {/Users/jonny/Papers/BryantG/2013/Bryant_2013_Animal signals and emotion in music.pdf}
}

@article{wallmarkEmbodiedListeningTimbre2018,
  title = {Embodied {{Listening}} and {{Timbre}}: {{Perceptual}}, {{Acoustical}}, and {{Neural Correlates}}},
  volume = {35},
  copyright = {\textcopyright{} 2018 by The Regents of the University of California},
  issn = {0730-7829, 1533-8312},
  shorttitle = {Embodied {{Listening}} and {{Timbre}}},
  abstract = {Skip to Next Section
Timbre plays an essential role in transmitting musical affect, and in recent years, our understanding of emotional expression in music has been enriched by contributions from the burgeoning field of embodied music cognition. However, little attention has been paid to timbre as a possible mediator between musical embodiment and affect. In three experiments, we investigated the embodied dimensions of timbre perception by focusing on timbral qualities considered ``noisy'' and aversive. In Experiment 1, participants rated brief isolated natural timbres scaled into ordinal levels of ``noisiness.'' Experiment 2 employed the same design with a focus on polyphonic timbre, using brief (400 ms) excerpts from 6 popular music genres as stimuli. In Experiment 3, functional magnetic resonance imaging was used to explore neural activations associated with perception of stimuli from Experiment 1. Converging results from behavioral, acoustical, and fMRI data suggest a motor component to timbre processing, particularly timbral qualities considered ``noisy,'' indicating a possible enactive mechanism in timbre processing. Activity in somatomotor areas, insula, and the limbic system increased the more participants disliked a timbre, and connectivity between the premotor cortex and insula relay decreased. Implications for recent theories of embodied music cognition, affect, and timbre semantics are discussed in conclusion.},
  language = {en},
  number = {3},
  urldate = {2019-07-11},
  journal = {Music Perception: An Interdisciplinary Journal},
  doi = {10.1525/mp.2018.35.3.332},
  url = {https://mp.ucpress.edu/content/35/3/332},
  author = {Wallmark, Zachary and Iacoboni, Marco and Deblieck, Choi and Kendall, Roger A.},
  month = feb,
  year = {2018},
  keywords = {fMRI,noise,embodied cognition,musical affect,timbre perception},
  pages = {332-363},
  file = {/Users/jonny/Papers/WallmarkZ/2018/Wallmark_2018_Embodied Listening and Timbre.pdf;/Users/jonny/Zotero/storage/Y3ALPVPJ/332.html}
}

@incollection{wehrsAffectFilmMusic2017,
  address = {{Cham}},
  title = {Affect and {{Film Music}}: {{A Brief History}}},
  isbn = {978-3-319-63303-9},
  shorttitle = {Affect and {{Film Music}}},
  abstract = {Tracing the history of film music from The Birth of a Nation (1915) to The Dark Knight (2012), Wehrs shows how film music acts on both affects, defined by Carl Plantinga as ``felt bodily states,'' and emotions. Wehrs analyzes the way Max Steiner's leitmotifs in Adventures of Don Juan (1948) cue audience response to the character, while Jerry Goldsmith's more sophisticated score for First Knight (1995) links motifs to character and narrative development. Although current film music like Hans Zimmer's for The Dark Knight has intense affective power, it cannot enhance character or plot development, Wehrs argues, because it is minimalist and tied to the sound design. Wehrs shows that Zimmer's music contributes to emotionally flattened, dehumanized qualities of The Dark Knight.},
  language = {en},
  urldate = {2019-07-11},
  booktitle = {The {{Palgrave Handbook}} of {{Affect Studies}} and {{Textual Criticism}}},
  publisher = {{Springer International Publishing}},
  url = {https://doi.org/10.1007/978-3-319-63303-9_28},
  author = {Wehrs, William},
  editor = {Wehrs, Donald R. and Blake, Thomas},
  year = {2017},
  pages = {735-752},
  doi = {10.1007/978-3-319-63303-9_28}
}

@misc{PDFSoundDesign,
  title = {(2) ({{PDF}}) {{The}} Sound Design of Cinematic Voices},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  language = {en},
  urldate = {2019-07-12},
  journal = {ResearchGate},
  url = {https://www.researchgate.net/publication/256498072_The_sound_design_of_cinematic_voices},
  file = {/Users/jonny/Zotero/storage/GTYW3MW2/256498072_The_sound_design_of_cinematic_voices.html}
}

@article{paulettoSoundDesignCinematic2012a,
  title = {The Sound Design of Cinematic Voices},
  volume = {2},
  issn = {2042-8855},
  abstract = {In films, the voice is the principal audio channel carrying both a character's expressivity and storytelling information. It is also a unique sound as it carries language and explicit meaning. The ...},
  number = {2},
  urldate = {2019-07-12},
  journal = {The New Soundtrack},
  doi = {10.3366/sound.2012.0034},
  url = {https://www.euppublishing.com/doi/abs/10.3366/sound.2012.0034},
  author = {Pauletto, Sandra},
  month = aug,
  year = {2012},
  keywords = {recording,sound design,cinematic voices,editing,mixing,post-production,production},
  pages = {127-142},
  file = {/Users/jonny/Papers/PaulettoS/2012/Pauletto_2012_The sound design of cinematic voices2.pdf;/Users/jonny/Zotero/storage/U9EVRZT3/sound.2012.html}
}

@misc{PDFDevelopmentHuman,
  title = {(2) ({{PDF}}) {{Development}} of the {{Human Cochlea}}},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  language = {en},
  urldate = {2019-07-15},
  journal = {ResearchGate},
  url = {https://www.researchgate.net/publication/21259352_Development_of_the_Human_Cochlea},
  file = {/Users/jonny/Zotero/storage/T9D7TJ9D/21259352_Development_of_the_Human_Cochlea.html}
}

@article{locherNeurosensoryDevelopmentCell2013,
  title = {Neurosensory Development and Cell Fate Determination in the Human Cochlea},
  volume = {8},
  issn = {1749-8104},
  abstract = {BACKGROUND: Hearing depends on correct functioning of the cochlear hair cells, and their innervation by spiral ganglion neurons. Most of the insight into the embryological and molecular development of this sensory system has been derived from animal studies. In contrast, little is known about the molecular expression patterns and dynamics of signaling molecules during normal fetal development of the human cochlea. In this study, we investigated the onset of hair cell differentiation and innervation in the human fetal cochlea at various stages of development.
RESULTS: At 10 weeks of gestation, we observed a prosensory domain expressing SOX2 and SOX9/SOX10 within the cochlear duct epithelium. In this domain, hair cell differentiation was consistently present from 12 weeks, coinciding with downregulation of SOX9/SOX10, to be followed several weeks later by downregulation of SOX2. Outgrowing neurites from spiral ganglion neurons were found penetrating into the cochlear duct epithelium prior to hair cell differentiation, and directly targeted the hair cells as they developed. Ubiquitous Peripherin expression by spiral ganglion neurons gradually diminished and became restricted to the type II spiral ganglion neurons by 18 weeks. At 20 weeks, when the onset of human hearing is thought to take place, the expression profiles in hair cells and spiral ganglion neurons matched the expression patterns of the adult mammalian cochleae.
CONCLUSIONS: Our study provides new insights into the fetal development of the human cochlea, contributing to our understanding of deafness and to the development of new therapeutic strategies to restore hearing.},
  language = {eng},
  journal = {Neural Development},
  doi = {10.1186/1749-8104-8-20},
  author = {Locher, Heiko and Frijns, Johan H. M. and {van Iperen}, Liesbeth and {de Groot}, John C. M. J. and Huisman, Margriet A. and {Chuva de Sousa Lopes}, Susana M.},
  month = oct,
  year = {2013},
  keywords = {Humans,Female,Cochlea,Cell Differentiation,Pregnancy,Cochlear Duct,Fetus,Hair Cells; Auditory,Proliferating Cell Nuclear Antigen,SOX9 Transcription Factor,SOXB1 Transcription Factors,SOXE Transcription Factors,Spiral Ganglion},
  pages = {20},
  file = {/Users/jonny/Papers/LocherH/2013/Locher_2013_Neurosensory development and cell fate determination in the human cochlea.pdf},
  pmid = {24131517},
  pmcid = {PMC3854452}
}

@article{rask-andersenHumanCochleaAnatomical2012,
  title = {Human {{Cochlea}}: {{Anatomical Characteristics}} and Their {{Relevance}} for {{Cochlear Implantation}}},
  volume = {295},
  copyright = {Copyright \textcopyright{} 2012 Wiley Periodicals, Inc.},
  issn = {1932-8494},
  shorttitle = {Human {{Cochlea}}},
  abstract = {This is a review of the anatomical characteristics of human cochlea and the importance of variations in this anatomy to the process of cochlear implantation (CI). Studies of the human cochlea are essential to better comprehend the physiology and pathology of man's hearing. The human cochlea is difficult to explore due to its vulnerability and bordering capsule. Inner ear tissue undergoes quick autolytic changes making investigations of autopsy material difficult, even though excellent results have been presented over time. Important issues today are novel inner ear therapies including CI and new approaches for inner ear pharmacological treatments. Inner ear surgery is now a reality, and technical advancements in the design of electrode arrays and surgical approaches allow preservation of remaining structure/function in most cases. Surgeons should aim to conserve cochlear structures for future potential stem cell and gene therapies. Renewal interest of round window approaches necessitates further acquaintance of this complex anatomy and its variations. Rough cochleostomy drilling at the intricate ``hook'' region can generate intracochlear bone-dust-inducing fibrosis and new bone formation, which could negatively influence auditory nerve responses at a later time point. Here, we present macro- and microanatomic investigations of the human cochlea viewing the extensive anatomic variations that influence electrode insertion. In addition, electron microscopic (TEM and SEM) and immunohistochemical results, based on specimens removed at surgeries for life-threatening petroclival meningioma and some well-preserved postmortal tissues, are displayed. These give us new information about structure as well as protein and molecular expression in man. Our aim was not to formulate a complete description of the complex human anatomy but to focus on aspects clinically relevant for electric stimulation, predominantly, the sensory targets, and how surgical atraumaticity best could be reached. Anat Rec, 2012. \textcopyright{} 2012 Wiley Periodicals, Inc.},
  language = {en},
  number = {11},
  urldate = {2019-07-15},
  journal = {The Anatomical Record},
  doi = {10.1002/ar.22599},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ar.22599},
  author = {Rask-Andersen, Helge and Liu, Wei and Erixon, Elsa and Kinnefors, Anders and Pfaller, Kristian and Schrott-Fischer, Annelies and Glueckert, Rudolf},
  year = {2012},
  keywords = {anatomical variations,cochlear implantation,human cochlea,trauma,ultrastructure},
  pages = {1791-1811},
  file = {/Users/jonny/Papers/RaskâAndersenH/2012/RaskâAndersen_2012_Human Cochlea.pdf;/Users/jonny/Zotero/storage/MV3G52XT/ar.html}
}

@article{kuhlmanUltrasonicImagingNormal1988,
  title = {Ultrasonic Imaging of Normal Fetal Response to External Vibratory Acoustic Stimulation},
  volume = {158},
  issn = {0002-9378, 1097-6868},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}p{$>$}Fetal facial reactions and response decrement patterns to external noise stimulation were studied to characterize normal fetal neurobehavior in the third trimester. Response decrement, or habituation, is thought to reflect higher central nervous system function. Two hundred women with uncomplicated pregnancies, who were subsequently delivered of healthy infants at term, were studied between 26 and 41 weeks' gestation. After ultrasound views of the fetal face were obtained, a vibratory acoustic stimulus was applied repetitively to the maternal abdomen near the fetal head. Response decrement was defined as cessation of all components of the facial reaction, except eye blinking, over two sequential stimuli. Three response patterns were scored: (1) no startle, (2) startle without response decrement, and (3) response decrement by 12 stimuli. Whereas only 53\% of fetuses between 26 and 27 weeks displayed startle reaction, all fetuses displayed startle responses by 28 weeks. As gestational age advances, an increasing number of fetuses exhibit response decrement, from no decrement at 26 to 27 weeks to 100\% decrement at 40 to 41 weeks. A maturation of neurobehavioral response patterns takes place in normal third-trimester pregnancies. Response decrement testing may be a useful tool for in utero neurologic evaluation.{$<$}/p{$>$}},
  language = {English},
  number = {1},
  urldate = {2019-07-15},
  journal = {American Journal of Obstetrics \& Gynecology},
  doi = {10.5555/uri:pii:0002937888907739},
  url = {https://www.ajog.org/article/0002-9378(88)90773-9/abstract},
  author = {Kuhlman, Kathleen A. and Burns, Kayreen A. and Depp, Richard and Sabbagha, Rudy E.},
  month = jan,
  year = {1988},
  pages = {47-51},
  file = {/Users/jonny/Zotero/storage/KC3TU34P/pdf.html}
}

@article{kisilevskyFetalSensitivityProperties2009,
  title = {Fetal Sensitivity to Properties of Maternal Speech and Language},
  volume = {32},
  issn = {0163-6383},
  abstract = {Fetal speech and language abilities were examined in 104 low-risk fetuses at 33\textendash{}41 weeks gestational age using a familiarization/novelty paradigm. Fetuses were familiarized with a tape recording of either their mother or a female stranger reading the same passage and subsequently presented with a novel speaker or language: Studies (1) \& (2) the alternate voice, (3) the father's voice, and (4) a female stranger speaking in native English or a foreign language (Mandarin); heart rate was recorded continuously. Data analyses revealed a novelty response to the mother's voice and a novel foreign language. An offset response was observed following termination of the father's and a female stranger's voice. These findings provide evidence of fetal attention, memory, and learning of voices and language, indicating that newborn speech/language abilities have their origins before birth. They suggest that neural networks sensitive to properties of the mother's voice and native-language speech are being formed.},
  number = {1},
  urldate = {2019-07-15},
  journal = {Infant Behavior and Development},
  doi = {10.1016/j.infbeh.2008.10.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0163638308000866},
  author = {Kisilevsky, B. S. and Hains, S. M. J. and Brown, C. A. and Lee, C. T. and Cowperthwaite, B. and Stutzman, S. S. and Swansburg, M. L. and Lee, K. and Xie, X. and Huang, H. and Ye, H. -H. and Zhang, K. and Wang, Z.},
  month = jan,
  year = {2009},
  keywords = {Speech,Learning,Language,Audition,Voice,Fetus},
  pages = {59-71},
  file = {/Users/jonny/Zotero/storage/PTJ523GQ/S0163638308000866.html}
}

@article{wirthEffectsStandardizedAcoustic2016,
  title = {Effects of Standardized Acoustic Stimulation in Premature Infants: A Randomized Controlled Trial},
  volume = {36},
  copyright = {2016 Nature Publishing Group},
  issn = {1476-5543},
  shorttitle = {Effects of Standardized Acoustic Stimulation in Premature Infants},
  abstract = {Objective:The objective of this study was to investigate the effects of recorded lullabies and taped maternal voice in premature infants.Study design:Sixty-two preterm infants in a stable condition with 30{$<$}37 weeks of gestation and {$<$}10 days of postnatal age were randomly assigned to hear (A) recorded lullabies or (B) taped maternal voice for 30 min each evening during 14 consecutive days or (C) receive no standardized acoustic stimulation (control group). Heart rate and respiratory rate were recorded daily before, during and after the intervention (A and B) or a comparable period with no intervention (C), whereas activity was measured on days 1, 7 and 14 of the intervention using accelerometers.Results:Both interventions led to a significant decrease in heart rate and respiratory rate during and after the stimulation when compared with the control group. The changes were more pronounced in infants with higher gestational ages (P=0.001). Lower activity was measured during the intervention when compared with the control group (P{$<$}0.01).Conclusions:Standardized acoustic stimulation with recorded lullabies and taped maternal voice led to a decrease in heart rate and respiratory rate, and was associated with lower activity. Whether this indicates a reduced stress reaction needs to be investigated in further studies.},
  language = {en},
  number = {6},
  urldate = {2019-07-15},
  journal = {Journal of Perinatology},
  doi = {10.1038/jp.2016.1},
  url = {https://www.nature.com/articles/jp20161},
  author = {Wirth, L. and Dorn, F. and Wege, M. and Zemlin, M. and Lemmer, B. and Gorbey, S. and Timmesfeld, N. and Maier, R. F.},
  month = jun,
  year = {2016},
  pages = {486-492},
  file = {/Users/jonny/Zotero/storage/STUQB7XR/jp20161.html}
}

@article{ullalLinkingPrenatalExperience2013,
  title = {Linking Prenatal Experience to the Emerging Musical Mind},
  volume = {7},
  issn = {1662-5137},
  abstract = {The musical brain is built over time through experience with a multitude of sounds in the auditory environment. However, learning the melodies, timbres, and rhythms unique to the music and language of one's culture begins already within the mother's womb during the third trimester of human development. We review evidence that the intrauterine auditory environment plays a key role in shaping later auditory development and musical preferences. We describe evidence that externally and internally generated sounds influence the developing fetus, and argue that such prenatal auditory experience may set the trajectory for the development of the musical mind.},
  language = {English},
  urldate = {2019-07-15},
  journal = {Frontiers in Systems Neuroscience},
  doi = {10.3389/fnsys.2013.00048},
  url = {https://www.frontiersin.org/articles/10.3389/fnsys.2013.00048/full},
  author = {Ullal, Sangeeta and {Vanden Bosch der Nederlanden}, Christina M. and Tichko, Parker and Lahav, Amir and Hannon, Erin E.},
  year = {2013},
  keywords = {Auditory Perception,Music,Language,Rhythm,Infancy,maternal heartbeat,NICu,pre-term development,tempo},
  file = {/Users/jonny/Papers/UllalS/2013/Ullal_2013_Linking prenatal experience to the emerging musical mind.pdf}
}

@article{skoeMusicalTrainingHeightens2013,
  title = {Musical Training Heightens Auditory Brainstem Function during Sensitive Periods in Development},
  volume = {4},
  issn = {1664-1078},
  abstract = {Experience has a profound influence on how sound is processed in the brain. Yet little is known about how enriched experiences interact with developmental processes to shape neural processing of sound. We examine this question as part of a large cross-sectional study of auditory brainstem development involving more than 700 participants, 213 of whom were classified as musicians. We hypothesized that experience-dependent processes piggyback on developmental processes, resulting in a waxing-and-waning effect of experience that tracks with the undulating developmental baseline. This hypothesis led to the prediction that experience-dependent plasticity would be amplified during periods when developmental changes are underway (i.e., early and later in life) and that the peak in experience-dependent plasticity would coincide with the developmental apex for each subcomponent of the auditory brainstem response. Consistent with our predictions, we reveal that musicians have heightened response features at distinctive times in the life span that coincide with periods of developmental change and climax. The effect of musicianship is also quite specific: we find that only select components of auditory brainstem activity are affected, with musicians having heightened function for onset latency, high frequency phase-locking, and response consistency, and with little effect observed for other measures, including lower frequency phase-locking and non-stimulus-related activity. By showing that musicianship imparts a neural signature that is especially evident during childhood and old age, our findings reinforce the idea that the nervous system's response to sound is ``chiseled'' by how a person interacts with his specific auditory environment, with the effect of the environment wielding its greatest influence during certain privileged windows of development.},
  language = {English},
  urldate = {2019-07-15},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2013.00622},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00622/full},
  author = {Skoe, Erika and Kraus, Nina},
  year = {2013},
  keywords = {development,experience-dependent plasticity,auditory brainstem response,musical training,sensitive periods},
  file = {/Users/jonny/Papers/SkoeE/2013/Skoe_2013_Musical training heightens auditory brainstem function during sensitive periods.pdf}
}

@article{webbMotherVoiceHeartbeat2015,
  title = {Mother's Voice and Heartbeat Sounds Elicit Auditory Plasticity in the Human Brain before Full Gestation},
  volume = {112},
  issn = {0027-8424, 1091-6490},
  abstract = {Brain development is largely shaped by early sensory experience. However, it is currently unknown whether, how early, and to what extent the newborn's brain is shaped by exposure to maternal sounds when the brain is most sensitive to early life programming. The present study examined this question in 40 infants born extremely prematurely (between 25- and 32-wk gestation) in the first month of life. Newborns were randomized to receive auditory enrichment in the form of audio recordings of maternal sounds (including their mother's voice and heartbeat) or routine exposure to hospital environmental noise. The groups were otherwise medically and demographically comparable. Cranial ultrasonography measurements were obtained at 30 {$\pm$} 3 d of life. Results show that newborns exposed to maternal sounds had a significantly larger auditory cortex (AC) bilaterally compared with control newborns receiving standard care. The magnitude of the right and left AC thickness was significantly correlated with gestational age but not with the duration of sound exposure. Measurements of head circumference and the widths of the frontal horn (FH) and the corpus callosum (CC) were not significantly different between the two groups. This study provides evidence for experience-dependent plasticity in the primary AC before the brain has reached full-term maturation. Our results demonstrate that despite the immaturity of the auditory pathways, the AC is more adaptive to maternal sounds than environmental noise. Further studies are needed to better understand the neural processes underlying this early brain plasticity and its functional implications for future hearing and language development.},
  language = {en},
  number = {10},
  urldate = {2019-07-15},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1414924112},
  url = {https://www.pnas.org/content/112/10/3152},
  author = {Webb, Alexandra R. and Heller, Howard T. and Benson, Carol B. and Lahav, Amir},
  month = mar,
  year = {2015},
  keywords = {auditory,brain,heartbeat,motherâs voice,preterm newborns},
  pages = {3152-3157},
  file = {/Users/jonny/Papers/WebbA/2015/Webb_2015_Motherâs voice and heartbeat sounds elicit auditory plasticity in the human.pdf;/Users/jonny/Zotero/storage/REBIIFHM/3152.html},
  pmid = {25713382}
}

@article{krausUnravelingBiologyAuditory2015,
  title = {Unraveling the {{Biology}} of {{Auditory Learning}}: {{A Cognitive}}\textendash{{Sensorimotor}}\textendash{{Reward Framework}}},
  volume = {19},
  issn = {1364-6613},
  shorttitle = {Unraveling the {{Biology}} of {{Auditory Learning}}},
  abstract = {The auditory system is stunning in its capacity for change: a single neuron can modulate its tuning in minutes. Here we articulate a conceptual framework to understand the biology of auditory learning where an animal must engage cognitive, sensorimotor, and reward systems to spark neural remodeling. Central to our framework is a consideration of the auditory system as an integrated whole that interacts with other circuits to guide and refine life in sound. Despite our emphasis on the auditory system, these principles may apply across the nervous system. Understanding neuroplastic changes in both normal and impaired sensory systems guides strategies to improve everyday communication.},
  number = {11},
  urldate = {2019-07-15},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2015.08.017},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661315002089},
  author = {Kraus, Nina and {White-Schwoch}, Travis},
  month = nov,
  year = {2015},
  keywords = {auditory learning,language development,language impairment,neuroplasticity},
  pages = {642-654},
  file = {/Users/jonny/Papers/KrausN/2015/Kraus_2015_Unraveling the Biology of Auditory Learning.pdf;/Users/jonny/Zotero/storage/UW5UYSRW/S1364661315002089.html}
}

@article{tierneyMusicTrainingAlters2015,
  title = {Music Training Alters the Course of Adolescent Auditory Development},
  volume = {112},
  issn = {0027-8424, 1091-6490},
  abstract = {Fundamental changes in brain structure and function during adolescence are well-characterized, but the extent to which experience modulates adolescent neurodevelopment is not. Musical experience provides an ideal case for examining this question because the influence of music training begun early in life is well-known. We investigated the effects of in-school music training, previously shown to enhance auditory skills, versus another in-school training program that did not focus on development of auditory skills (active control). We tested adolescents on neural responses to sound and language skills before they entered high school (pretraining) and again 3 y later. Here, we show that in-school music training begun in high school prolongs the stability of subcortical sound processing and accelerates maturation of cortical auditory responses. Although phonological processing improved in both the music training and active control groups, the enhancement was greater in adolescents who underwent music training. Thus, music training initiated as late as adolescence can enhance neural processing of sound and confer benefits for language skills. These results establish the potential for experience-driven brain plasticity during adolescence and demonstrate that in-school programs can engender these changes.},
  language = {en},
  number = {32},
  urldate = {2019-07-15},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1505114112},
  url = {https://www.pnas.org/content/112/32/10062},
  author = {Tierney, Adam T. and Krizman, Jennifer and Kraus, Nina},
  month = aug,
  year = {2015},
  keywords = {auditory,music,training},
  pages = {10062-10067},
  file = {/Users/jonny/Papers/TierneyA/2015/Tierney_2015_Music training alters the course of adolescent auditory development.pdf;/Users/jonny/Zotero/storage/Y69TJM2R/10062.html},
  pmid = {26195739}
}

@article{mcmahonAuditoryBrainDevelopment2012,
  title = {Auditory Brain Development in Premature Infants: The Importance of Early Experience: {{McMahon}} et Al.},
  volume = {1252},
  issn = {00778923},
  shorttitle = {Auditory Brain Development in Premature Infants},
  language = {en},
  number = {1},
  urldate = {2019-07-15},
  journal = {Annals of the New York Academy of Sciences},
  doi = {10.1111/j.1749-6632.2012.06445.x},
  url = {http://doi.wiley.com/10.1111/j.1749-6632.2012.06445.x},
  author = {McMahon, Erin and Wintermark, Pia and Lahav, Amir},
  month = apr,
  year = {2012},
  pages = {17-24},
  file = {/Users/jonny/Zotero/storage/8WHPFCWE/McMahon et al. - 2012 - Auditory brain development in premature infants t.pdf}
}

@article{hafnerCriticalAgesBrainstem1993,
  title = {Critical Ages in Brainstem Development Revealed by Neonatal 3-Channel {{Lissajous}}' Trajectory of Auditory Brainstem Evoked Potentials},
  volume = {66},
  issn = {0378-5955},
  abstract = {Auditory brainstem evoked potentials (ABEPs) were recorded from 91 newborns from 7 age groups between 26 to 43 weeks of gestation. In addition to the widely used vertex-mastoid derivation, potentials were recorded from three orthogonal electrode configurations, and represented in 3 dimensional voltage-space as three-channel Lissajous' trajectories (3CLTs). ABEPs were evoked by alternating polarity, monaural 75 dBnHL clicks presented at rates of 10/s, 55/s and 80/s. Potentials were also recorded to 45 dBnHL and 15 dBnHL clicks presented at 10/s. 3CLT point by point (apex latencies, amplitudes and orientation) as well as planar segment (planar segment position and duration) descriptors, along with peak latencies of the vertex-mastoid peaks, were followed for effects of age, stimulus intensity and rate. ABEPs began to appear consistently at 29 weeks of gestation to high stimulus intensities, with a rapid decrease of ABEP thresholds up to 34 weeks. At 35 weeks, thresholds stabilized approximately at adult values. The results indicate a significant effect of stimulus rate and intensity as well as of gestational age group on apex latencies. The findings also showed changes in apex orientations associated with stimulus rate and intensity interacting with gestational age. 3CLT descriptors enhanced the understanding of these results in relation to developmental and maturational aspects of the auditory system. The results may be explained by maturational change in relative contributions of constituents of the complex ABEP generators.},
  number = {2},
  urldate = {2019-07-15},
  journal = {Hearing Research},
  doi = {10.1016/0378-5955(93)90137-P},
  url = {http://www.sciencedirect.com/science/article/pii/037859559390137P},
  author = {Hafner, H. and Pratt, H. and Blazer, S. and Sujov, P.},
  month = apr,
  year = {1993},
  keywords = {Development,Auditory brainstem,Evoked potentials,Newborns,Three-channel Lissajous' trajectories},
  pages = {157-168},
  file = {/Users/jonny/Papers/HafnerH/1993/Hafner_1993_Critical ages in brainstem development revealed by neonatal 3-channel.pdf;/Users/jonny/Zotero/storage/AMDFKLGN/037859559390137P.html}
}

@article{kisilevskyFetalSensitivityProperties2009a,
  title = {Fetal Sensitivity to Properties of Maternal Speech and Language},
  volume = {32},
  issn = {1934-8800},
  abstract = {Fetal speech and language abilities were examined in 104 low-risk fetuses at 33-41 weeks gestational age using a familiarization/novelty paradigm. Fetuses were familiarized with a tape recording of either their mother or a female stranger reading the same passage and subsequently presented with a novel speaker or language: Studies (1) \& (2) the alternate voice, (3) the father's voice, and (4) a female stranger speaking in native English or a foreign language (Mandarin); heart rate was recorded continuously. Data analyses revealed a novelty response to the mother's voice and a novel foreign language. An offset response was observed following termination of the father's and a female stranger's voice. These findings provide evidence of fetal attention, memory, and learning of voices and language, indicating that newborn speech/language abilities have their origins before birth. They suggest that neural networks sensitive to properties of the mother's voice and native-language speech are being formed.},
  language = {eng},
  number = {1},
  journal = {Infant Behavior \& Development},
  doi = {10.1016/j.infbeh.2008.10.002},
  author = {Kisilevsky, B. S. and Hains, S. M. J. and Brown, C. A. and Lee, C. T. and Cowperthwaite, B. and Stutzman, S. S. and Swansburg, M. L. and Lee, K. and Xie, X. and Huang, H. and Ye, H.-H. and Zhang, K. and Wang, Z.},
  month = jan,
  year = {2009},
  keywords = {Time Factors,Acoustic Stimulation,Humans,Female,Reaction Time,Male,Speech,Speech Perception,Discrimination (Psychology),Language,Recognition (Psychology),Voice,Analysis of Variance,Heart Rate,Pregnancy,Fetal Development,Gestational Age,Mother-Child Relations},
  pages = {59-71},
  pmid = {19058856}
}

@misc{chornaNeuroprocessingMechanismsMusic2019,
  type = {Research Article},
  title = {Neuroprocessing {{Mechanisms}} of {{Music}} during {{Fetal}} and {{Neonatal Development}}: {{A Role}} in {{Neuroplasticity}} and {{Neurodevelopment}}},
  shorttitle = {Neuroprocessing {{Mechanisms}} of {{Music}} during {{Fetal}} and {{Neonatal Development}}},
  abstract = {The primary aim of this viewpoint article is to examine recent literature on fetal and neonatal processing of music. In particular, we examine the behavioral, neurophysiological, and neuroimaging literature describing fetal and neonatal music perception and processing to the first days of term equivalent life. Secondly, in light of the recent systematic reviews published on this topic, we discuss the impact of music interventions on the potential neuroplasticity pathways through which the early exposure to music, live or recorded, may impact the fetal, preterm, and full-term infant brain. We conclude with recommendations for music stimuli selection and its role within the framework of early socioemotional development and environmental enrichment.},
  language = {en},
  urldate = {2019-07-15},
  journal = {Neural Plasticity},
  url = {https://www.hindawi.com/journals/np/2019/3972918/},
  author = {Chorna, O. and Filippa, M. and De Almeida, J. Sa and Lordier, L. and Monaci, M. G. and H{\"u}ppi, P. and Grandjean, D. and Guzzetta, A.},
  year = {2019},
  file = {/Users/jonny/Papers/ChornaO/2019/Chorna_2019_Neuroprocessing Mechanisms of Music during Fetal and Neonatal Development.pdf;/Users/jonny/Zotero/storage/JRDYUW8D/3972918.html},
  doi = {10.1155/2019/3972918}
}

@article{teieComparativeAnalysisUniversal2016,
  title = {A {{Comparative Analysis}} of the {{Universal Elements}} of {{Music}} and the {{Fetal Environment}}},
  volume = {7},
  issn = {1664-1078},
  abstract = {Although the idea that pulse in music may be related to human pulse is ancient and has recently been promoted by researchers (Parncutt, 2006; Snowdon \& Teie, 2010), there has been no ordered delineation of the characteristics of music that are based on the sounds of the womb. I describe features of music that are based on sounds that are present in the womb: tempo of pulse (pulse is understood as the regular, underlying beat that defines the meter), amplitude contour of pulse, meter, musical notes, melodic frequency range, continuity, syllabic contour, melodic rhythm, melodic accents, phrase length, and phrase contour. There are a number of features of prenatal development that allow for the formation of long-term memories of the sounds of the womb in the areas of the brain that are responsible for emotions. Taken together, these features and the similarities between the sounds of the womb and the elemental building blocks of music allow for a postulation that the fetal acoustic environment may provide the bases for the fundamental musical elements that are found in the music of all cultures. This hypothesis is supported by a one-to-one matching of the universal features of music with the sounds of the womb: 1) all of the regularly heard sounds that are present in the fetal environment are represented in the music of every culture, and 2) all of the features of music that are present in the music of all cultures can be traced to the fetal environment.},
  language = {English},
  urldate = {2019-07-15},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2016.01158},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01158/full},
  author = {Teie, David},
  year = {2016},
  keywords = {Music,Rhythm,Pulse,origin of music,womb},
  file = {/Users/jonny/Papers/TeieD/2016/Teie_2016_A Comparative Analysis of the Universal Elements of Music and the Fetal.pdf}
}

@article{richardsSoundLevelsHuman1992,
  title = {Sound Levels in the Human Uterus.},
  volume = {80},
  issn = {0029-7844},
  abstract = {Abstract: OBJECTIVE: We sought to determine the degree to which noises and voices are attenuated or enhanced as they pass into the uterus. METHODS: In...},
  language = {eng},
  number = {2},
  urldate = {2019-07-15},
  journal = {Obstetrics and gynecology},
  url = {http://europepmc.org/abstract/med/1635729},
  author = {Richards, D. S. and Frentzen, B. and Gerhardt, K. J. and McCann, M. E. and Abrams, R. M.},
  month = aug,
  year = {1992},
  pages = {186-190},
  file = {/Users/jonny/Zotero/storage/WBEGHKPB/1635729.html},
  pmid = {1635729}
}

@misc{QuerleuFetalHearing,
  title = {Querleu: {{Fetal}} Hearing - {{Google Scholar}}},
  urldate = {2019-07-15},
  url = {https://scholar.google.com/scholar_lookup?title=Fetal+hearing\%2E\&journal=Eur\%2E+J\%2E+Obstet\%2E+Gynecol\%2E+Reprod\%2E+Biol\%2E\&author=Querleu+D.\&author=Renard+X.\&author=Versyp+F.\&author=Paris-Delrue+L.\&author=and+Cr\%C3\%A8pin+G.\&publication_year=1988\&volume=28\&pages=191-212},
  file = {/Users/jonny/Zotero/storage/SFUJ4MPV/scholar_lookup.html}
}

@article{gerhardtFetalExposuresSound2000,
  title = {Fetal {{Exposures}} to {{Sound}} and {{Vibroacoustic Stimulation}}},
  volume = {20},
  copyright = {2000 Nature America, Inc.},
  issn = {1476-5543},
  abstract = {Sounds in the environment of a pregnant woman penetrate the tissues and fluids surrounding the fetal head and stimulate the inner ear through a bone conduction route. The sounds available to the fetus are dominated by low-frequency energy, whereas energy above 0.5 kHz is attenuated by 40 to 50 dB. The fetus easily detects vowels, whereas consonants, which are higher in frequency and less intense than vowels, are largely unavailable. Rhythmic patterns of music are probably detected, but overtones are missing. A newborn human shows preference for his/her mother's voice and to musical pieces to which he/she was previously exposed, indicating a capacity to learn while in utero. Intense, sustained noises or impulses produce changes in the hearing of the fetus and damage inner and outer hair cells within the cochlea. The damage occurs in the region of the inner ear that is stimulated by low-frequency sound energy.},
  language = {En},
  number = {1},
  urldate = {2019-07-15},
  journal = {Journal of Perinatology},
  doi = {10.1038/sj.jp.7200446},
  url = {https://www.nature.com/articles/7200446},
  author = {Gerhardt, Kenneth J. and Abrams, Robert M.},
  month = dec,
  year = {2000},
  pages = {S21},
  file = {/Users/jonny/Papers/GerhardtK/2000/Gerhardt_2000_Fetal Exposures to Sound and Vibroacoustic Stimulation.pdf;/Users/jonny/Zotero/storage/BC4H4R4E/7200446.html}
}

@article{pargaDescriptionExternallyRecorded2018,
  title = {A Description of Externally Recorded Womb Sounds in Human Subjects during Gestation},
  volume = {13},
  issn = {1932-6203},
  abstract = {Objective Reducing environmental noise benefits premature infants in neonatal intensive care units (NICU), but excessive reduction may lead to sensory deprivation, compromising development. Instead of minimal noise levels, environments that mimic intrauterine soundscapes may facilitate infant development by providing a sound environment reflecting fetal life. This soundscape may support autonomic and emotional development in preterm infants. We aimed to assess the efficacy and feasibility of external non-invasive recordings in pregnant women, endeavoring to capture intra-abdominal or womb sounds during pregnancy with electronic stethoscopes and build a womb sound library to assess sound trends with gestational development. We also compared these sounds to popular commercial womb sounds marketed to new parents. Study design Intra-abdominal sounds from 50 mothers in their second and third trimester (13 to 40 weeks) of pregnancy were recorded for 6 minutes in a quiet clinic room with 4 electronic stethoscopes, placed in the right upper and lower quadrants, and left upper and lower quadrants of the abdomen. These recording were partitioned into 2-minute intervals in three different positions: standing, sitting and lying supine. Maternal and gestational age, Body Mass Index (BMI) and time since last meal were collected during recordings. Recordings were analyzed using long-term average spectral and waveform analysis, and compared to sounds from non-pregnant abdomens and commercially-marketed womb sounds selected for their availability, popularity, and claims they mimic the intrauterine environment. Results Maternal sounds shared certain common characteristics, but varied with gestational age. With fetal development, the maternal abdomen filtered high (500\textendash{}5,000 Hz) and mid-frequency (100\textendash{}500 Hz) energy bands, but no change appeared in contributions from low-frequency signals (10\textendash{}100 Hz) with gestational age. Variation appeared between mothers, suggesting a resonant chamber role for intra-abdominal space. Compared to commercially-marketed sounds, womb signals were dominated by bowel sounds, were of lower frequency, and showed more variation in intensity. Conclusions High-fidelity intra-abdominal or womb sounds during pregnancy can be recorded non-invasively. Recordings vary with gestational age, and show a predominance of low frequency noise and bowel sounds which are distinct from popular commercial products. Such recordings may be utilized to determine whether sounds influence preterm infant development in the NICU.},
  language = {en},
  number = {5},
  urldate = {2019-07-15},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0197045},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197045},
  author = {Parga, Joanna J. and Daland, Robert and Kesavan, Kalpashri and Macey, Paul M. and Zeltzer, Lonnie and Harper, Ronald M.},
  month = may,
  year = {2018},
  keywords = {Uterus,Pregnancy,Sound pressure,Abdomen,Infants,Neonatal care,Neonates,Resonance frequency},
  pages = {e0197045},
  file = {/Users/jonny/Papers/PargaJ/2018/Parga_2018_A description of externally recorded womb sounds in human subjects during.pdf;/Users/jonny/Zotero/storage/V7ZYV8XA/article.html}
}

@article{richardsSoundLevelsHuman1992a,
  title = {Sound {{Levels}} in the {{Human Uterus}}},
  volume = {80},
  issn = {0029-7844},
  abstract = {Objective: We sought to determine the degree to which noises and voices are attenuated or enhanced as they pass into the uterus.
        Methods: In eight parturients, a hydrophone in the uterus was used to measure sound pressure levels for externally generated one-third-octave band noises, male and female voices, and the subject's voice.
        Results: Low-frequency sounds (0.125 kHz) generated outside the mother were enhanced by an average of 3.7 dB. There was a gradual increase in attenuation for increasing frequencies, with a maximum attenuation of 10.0 dB at 4.0 kHz. Sound attenuation was slightly less if the insonation was from in front of the woman rather than behind. Intrauterine sound levels of the mother's voice were enhanced by an average of 5.2 dB, whereas external male and female voices were attenuated by 2.1 and 3.2 dB, respectively. The effect of frequency on attenuation, the differences between front and back insonation, and the differences between speakers in attenuation were all statistically significant.
        Conclusions: The intrauterine environment is rich with externally generated sounds. This may imply fetal risk from maternal noise exposure and may aid in understanding fetal imprinting from prenatal exposure to voices.
        
        \textcopyright{} 1992 The American College of Obstetricians and Gynecologists},
  language = {en-US},
  number = {2},
  urldate = {2019-07-15},
  journal = {Obstetrics \& Gynecology},
  url = {https://journals.lww.com/greenjournal/abstract/1992/08000/sound_levels_in_the_human_uterus.6.aspx\#pdf-link},
  author = {Richards, Douglas S. and Frentzen, Barbara and Gerhardt, Kenneth J. and McCANN, Mary E. and Abrams, Robert M.},
  month = aug,
  year = {1992},
  pages = {186},
  file = {/Users/jonny/Zotero/storage/UBBZAJJE/sound_levels_in_the_human_uterus.6.html}
}

@article{snowdonCatsPreferSpeciesappropriate2015,
  title = {Cats Prefer Species-Appropriate Music},
  volume = {166},
  issn = {0168-1591},
  abstract = {Many studies have attempted to use music to influence the behavior of nonhuman animals; however, these studies have often led to conflicting outcomes. We have developed a theoretical framework that hypothesizes that in order for music to be effective with other species, it must be in the frequency range and with similar tempos to those used in natural communication by each species. We have used this framework to compose music that is species-appropriate for a few animal species. In this paper, we created species-appropriate music for domestic cats and tested this music in comparison with music with similar affective content composed for humans. We presented two examples of cat music in counterbalanced order with two examples of human music, and we evaluated the behavior and response latencies of cats to each piece. Cats showed a significant preference for and interest in species-appropriate music compared with human music (median (interquartile range (IQR)) 1.5 (0.5\textendash{}2.0) acts for cat music, 0.25 (0.0\textendash{}0.5) acts for human music, P{$<$}0.002) and responded with significantly shorter latencies (median (IQR) 110.0 (54\textendash{}138.75)s for cat music, 171.75 (151\textendash{}180)s for human music (P{$<$}0.001). Younger and older cats were more responsive to cat music than middle-aged acts (cubic trend, r2=0.477, P{$<$}0.001). The results suggest novel and more appropriate ways for using music as auditory enrichment for nonhuman animals.},
  urldate = {2019-07-15},
  journal = {Applied Animal Behaviour Science},
  doi = {10.1016/j.applanim.2015.02.012},
  url = {http://www.sciencedirect.com/science/article/pii/S016815911500060X},
  author = {Snowdon, Charles T. and Teie, David and Savage, Megan},
  month = may,
  year = {2015},
  keywords = {Cats,Music,Emotions,Auditory enrichment,Species-appropriate},
  pages = {106-111},
  file = {/Users/jonny/Papers/SnowdonC/2015/Snowdon_2015_Cats prefer species-appropriate music.pdf;/Users/jonny/Zotero/storage/Q2936UJQ/S016815911500060X.html}
}

@article{partanenPrenatalMusicExposure2013,
  title = {Prenatal {{Music Exposure Induces Long}}-{{Term Neural Effects}}},
  volume = {8},
  issn = {1932-6203},
  abstract = {We investigated the neural correlates induced by prenatal exposure to melodies using brains' event-related potentials (ERPs). During the last trimester of pregnancy, the mothers in the learning group played the `Twinkle twinkle little star' -melody 5 times per week. After birth and again at the age of 4 months, we played the infants a modified melody in which some of the notes were changed while ERPs to unchanged and changed notes were recorded. The ERPs were also recorded from a control group, who received no prenatal stimulation. Both at birth and at the age of 4 months, infants in the learning group had stronger ERPs to the unchanged notes than the control group. Furthermore, the ERP amplitudes to the changed and unchanged notes at birth were correlated with the amount of prenatal exposure. Our results show that extensive prenatal exposure to a melody induces neural representations that last for several months.},
  number = {10},
  urldate = {2019-07-15},
  journal = {PLoS ONE},
  doi = {10.1371/journal.pone.0078946},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3813619/},
  author = {Partanen, Eino and Kujala, Teija and Tervaniemi, Mari and Huotilainen, Minna},
  month = oct,
  year = {2013},
  file = {/Users/jonny/Papers/PartanenE/2013/Partanen_2013_Prenatal Music Exposure Induces Long-Term Neural Effects.pdf},
  pmid = {24205353},
  pmcid = {PMC3813619}
}

@article{cheourMismatchNegativityMMN2000,
  title = {Mismatch Negativity ({{MMN}}) as a Tool for Investigating Auditory Discrimination and Sensory Memory in Infants and Children},
  volume = {111},
  issn = {1388-2457},
  abstract = {For decades behavioral methods, such as the head-turning or sucking paradigms, have been the primary methods to investigate auditory discrimination, learning and the function of sensory memory in infancy and early childhood. During recent years, however, a new method for investigating these issues in children has emerged. This method makes use of the mismatch negativity (MMN), the brain's automatic change-detection response, which has been used intensively in both basic and clinical studies in adults for twenty years. This review demonstrates that, unlike many other components of event-related potentials, the MMN is developmentally quite stable and can be obtained even from pre-term infants. Further, MMN amplitude is only slightly smaller in infants than is usually reported in school-age children and it does not seem to differ much from that obtained in adults. MMN latency has been reported to be slightly longer in infants than in adults but reaches adult values by the early school-age years. Child MMN does not seem to be analogous to adult MMN, however. For example, contrary to the results of adult studies, a prominent MMN can be obtained from in all waking- and sleep states in infants. Moreover, MMN scalp distribution seems to be broader and more central in children than in adults.},
  number = {1},
  urldate = {2019-07-15},
  journal = {Clinical Neurophysiology},
  doi = {10.1016/S1388-2457(99)00191-1},
  url = {http://www.sciencedirect.com/science/article/pii/S1388245799001911},
  author = {Cheour, Marie and H.T. Lepp{\"a}nen, Paavo and Kraus, Nina},
  month = jan,
  year = {2000},
  keywords = {Auditory,Event-related potentials,Mismatch negativity,Children,Memory trace},
  pages = {4-16},
  file = {/Users/jonny/Papers/CheourM/2000/Cheour_2000_Mismatch negativity (MMN) as a tool for investigating auditory discrimination.pdf;/Users/jonny/Zotero/storage/9T2LEX99/S1388245799001911.html}
}

@article{winklerNewbornInfantsDetect2009,
  title = {Newborn Infants Detect the Beat in Music},
  volume = {106},
  copyright = {\textcopyright{} 2009 by The National Academy of Sciences of the USA.  Freely available online through the PNAS open access option.},
  issn = {0027-8424, 1091-6490},
  abstract = {To shed light on how humans can learn to understand music, we need to discover what the perceptual capabilities with which infants are born. Beat induction, the detection of a regular pulse in an auditory signal, is considered a fundamental human trait that, arguably, played a decisive role in the origin of music. Theorists are divided on the issue whether this ability is innate or learned. We show that newborn infants develop expectation for the onset of rhythmic cycles (the downbeat), even when it is not marked by stress or other distinguishing spectral features. Omitting the downbeat elicits brain activity associated with violating sensory expectations. Thus, our results strongly support the view that beat perception is innate.},
  language = {en},
  number = {7},
  urldate = {2019-07-15},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0809035106},
  url = {https://www.pnas.org/content/106/7/2468},
  author = {Winkler, Istv{\'a}n and H{\'a}den, G{\'a}bor P. and Ladinig, Olivia and Sziller, Istv{\'a}n and Honing, Henkjan},
  month = feb,
  year = {2009},
  keywords = {rhythm,event-related brain potentials (ERP),neonates},
  pages = {2468-2471},
  file = {/Users/jonny/Papers/WinklerI/2009/Winkler_2009_Newborn infants detect the beat in music.pdf;/Users/jonny/Zotero/storage/8NPME43D/2468.html},
  pmid = {19171894}
}

@article{winklerNewbornInfantsDetect2009a,
  title = {Newborn Infants Detect the Beat in Music},
  volume = {106},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {7},
  urldate = {2019-07-15},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0809035106},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0809035106},
  author = {Winkler, I. and Haden, G. P. and Ladinig, O. and Sziller, I. and Honing, H.},
  month = feb,
  year = {2009},
  pages = {2468-2471},
  file = {/Users/jonny/Zotero/storage/GVRPZN7A/Winkler et al. - 2009 - Newborn infants detect the beat in music.pdf}
}

@article{trehubInfantsPerceptionGood1990,
  title = {Infants' Perception of Good and Bad Melodies},
  volume = {9},
  issn = {2162-1535(Electronic),0275-3987(Print)},
  abstract = {Exposed 30 healthy infants (aged 7\textendash{}10 mo) to repetitions of 1 of 3 melodies in transposition, the 3 melodies exhibiting different degrees of conformance to Western music structure. One was a good Western melody, consisting of notes from the diatonic scale; another was a bad Western melody with notes drawn from the chromatic scale but not from any single diatonic scale; and the third was a bad non-Western melody with notes not drawn from the chromatic scale. Ss discriminated a semitone change in the context of the good Western melody but not the bad Western or the bad non-Western melody. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {1},
  journal = {Psychomusicology: A Journal of Research in Music Cognition},
  doi = {10.1037/h0094162},
  author = {Trehub, Sandra E. and Thorpe, Leigh A. and Trainor, Laurel J.},
  year = {1990},
  keywords = {Perception,Music},
  pages = {5-19},
  file = {/Users/jonny/Zotero/storage/K6Q69R69/1991-00880-001.html}
}

@misc{BeginningBriefHistory,
  title = {In the Beginning: {{A}} Brief History of Infant Music Perception - {{Sandra E}}. {{Trehub}}, 2010},
  urldate = {2019-07-15},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10298649100140S206},
  file = {/Users/jonny/Zotero/storage/TQSKRG9S/10298649100140S206.html}
}

@article{trainorDevelopmentEvaluativeResponses1998,
  title = {The Development of Evaluative Responses to Music:: {{Infants}} Prefer to Listen to Consonance over Dissonance},
  volume = {21},
  issn = {0163-6383},
  shorttitle = {The Development of Evaluative Responses to Music},
  abstract = {In Experiment 1, 6-month-old infants looked longer in order to listen to a set of consonant intervals than to a set of dissonant intervals. In Experiment 2, infants preferred to listen to the original version of a Mozart minuet than to a version altered to contain many dissonant intervals. Thus, although infants do not yet have the musical-system-specific knowledge of scale structure that is involved in adults' emotional reactions to music, infants are similar to adults in their evaluative reactions to consonance and dissonance.},
  number = {1},
  urldate = {2019-07-15},
  journal = {Infant Behavior and Development},
  doi = {10.1016/S0163-6383(98)90055-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0163638398900558},
  author = {Trainor, Laurel J. and Heinmiller, Becky M.},
  month = jan,
  year = {1998},
  keywords = {auditory perception,consonance,dissonance,infant,music preference},
  pages = {77-88},
  file = {/Users/jonny/Papers/TrainorL/1998/Trainor_1998_The development of evaluative responses to music.pdf;/Users/jonny/Zotero/storage/HNYKY23X/S0163638398900558.html}
}

@article{dallabellaDevelopmentalStudyAffective2001,
  title = {A Developmental Study of the Affective Value of Tempo and Mode in Music},
  volume = {80},
  issn = {0010-0277},
  abstract = {Do children use the same properties as adults in determining whether music sounds happy or sad? We addressed this question with a set of 32 excerpts (16 happy and 16 sad) taken from pre-existing music. The tempo (i.e. the number of beats per minute) and the mode (i.e. the specific subset of pitches used to write a given musical excerpt) of these excerpts were modified independently and jointly in order to measure their effects on happy-sad judgments. Adults and children from 3 to 8 years old were required to judge whether the excerpts were happy or sad. The results show that as adults, 6--8-year-old children are affected by mode and tempo manipulations. In contrast, 5-year-olds' responses are only affected by a change of tempo. The youngest children (3--4-year-olds) failed to distinguish the happy from the sad tone of the music above chance. The results indicate that tempo is mastered earlier than mode to infer the emotional tone conveyed by music.},
  language = {eng},
  number = {3},
  journal = {Cognition},
  author = {Dalla Bella, S. and Peretz, I. and Rousseau, L. and Gosselin, N.},
  month = jul,
  year = {2001},
  keywords = {Humans,Female,Auditory Perception,Male,Music,Affect,Adult,Child,Child Development,Child; Preschool},
  pages = {B1-10},
  pmid = {11274986}
}

@article{crowderPerceptionMajorMinor1991a,
  title = {Perception of the Major/Minor Distinction: {{V}}. {{Preferences}} among Infants},
  volume = {29},
  issn = {0090-5054},
  shorttitle = {Perception of the Major/Minor Distinction},
  abstract = {Six-month-old infants expressed musical preferences by choosing one of two directional targets to look at, one of which produced a major chord and the other a minor chord. Over repeated testing in this way, no reliable preference for either harmony was expressed. However, a comparable choice between consonant and dissonant chords produced reliable preference for consonant harmonies.},
  language = {en},
  number = {3},
  urldate = {2019-07-15},
  journal = {Bulletin of the Psychonomic Society},
  doi = {10.3758/BF03342673},
  url = {https://doi.org/10.3758/BF03342673},
  author = {Crowder, Robert G. and Reznick, J. Steven and Rosenkrantz, Stacey L.},
  month = mar,
  year = {1991},
  keywords = {Music Perception,Major Chord,Minor Chord,Musical Preference,Reliable Preference},
  pages = {187-188},
  file = {/Users/jonny/Papers/CrowderR/1991/Crowder_1991_Perception of the major-minor distinction2.pdf}
}

@article{heFindingPitchMissing2009a,
  title = {Finding the {{Pitch}} of the {{Missing Fundamental}} in {{Infants}}},
  volume = {29},
  copyright = {Copyright \textcopyright{} 2009 Society for Neuroscience 0270-6474/09/297718-05\$15.00/0},
  issn = {0270-6474, 1529-2401},
  abstract = {Pitch perception is critical for the perception of speech and music, for object identification, and for auditory scene analysis, whereby representations are derived for each sounding object in the environment from the complex sound wave that reaches the ears. The perceived pitch of a complex sound corresponds to its fundamental frequency. However, removal of energy at the fundamental does not alter the pitch because adults use the harmonics to derive the pitch (Bendor and Wang, 2005; Trainor, 2008). Although sound frequency is represented subcortically, the integration of harmonics into a representation of pitch does not occur until auditory cortex (Bendor and Wang, 2005). Given that auditory cortex is immature in young infants, we examined the development of cortical representations for pitch by measuring electrophysiological (EEG) responses to pitch changes that required processing the pitch of the missing fundamental. Adults and infants 4 months and older showed a mismatch negativity response to these pitch changes, but 3-month-old infants did not. Thus, cortical representations of the pitch of the missing fundamental emerge between 3 and 4 months of age, indicating that there is a profound change in auditory perception for pitch in early infancy.},
  language = {en},
  number = {24},
  urldate = {2019-07-15},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.0157-09.2009},
  url = {https://www.jneurosci.org/content/29/24/7718},
  author = {He, Chao and Trainor, Laurel J.},
  month = jun,
  year = {2009},
  pages = {7718-8822},
  file = {/Users/jonny/Papers/HeC/2009/He_2009_Finding the Pitch of the Missing Fundamental in Infants2.pdf;/Users/jonny/Zotero/storage/KVIGMASD/7718.html},
  pmid = {19535583}
}

@misc{CriticalPeriodsSpeech,
  title = {Critical {{Periods}} in {{Speech Perception}}: {{New Directions}} | {{Annual Review}} of {{Psychology}}},
  urldate = {2019-07-15},
  url = {https://www.annualreviews.org/doi/full/10.1146/annurev-psych-010814-015104},
  file = {/Users/jonny/Zotero/storage/YV49UVPF/annurev-psych-010814-015104.html}
}

@article{goldMusicalRewardPrediction2019,
  title = {Musical Reward Prediction Errors Engage the Nucleus Accumbens and Motivate Learning},
  volume = {116},
  copyright = {\textcopyright{} 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  abstract = {Enjoying music reliably ranks among life's greatest pleasures. Like many hedonic experiences, it engages several reward-related brain areas, with activity in the nucleus accumbens (NAc) most consistently reflecting the listener's subjective response. Converging evidence suggests that this activity arises from musical ``reward prediction errors'' (RPEs) that signal the difference between expected and perceived musical events, but this hypothesis has not been directly tested. In the present fMRI experiment, we assessed whether music could elicit formally modeled RPEs in the NAc by applying a well-established decision-making protocol designed and validated for studying RPEs. In the scanner, participants chose between arbitrary cues that probabilistically led to dissonant or consonant music, and learned to make choices associated with the consonance, which they preferred. We modeled regressors of trial-by-trial RPEs, finding that NAc activity tracked musically elicited RPEs, to an extent that explained variance in the individual learning rates. These results demonstrate that music can act as a reward, driving learning and eliciting RPEs in the NAc, a hub of reward- and music enjoyment-related activity.},
  language = {en},
  number = {8},
  urldate = {2019-07-16},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1809855116},
  url = {https://www.pnas.org/content/116/8/3310},
  author = {Gold, Benjamin P. and {Mas-Herrero}, Ernest and Zeighami, Yashar and Benovoy, Mitchel and Dagher, Alain and Zatorre, Robert J.},
  month = feb,
  year = {2019},
  keywords = {fMRI,nucleus accumbens,music,abstract reward,reward prediction errors},
  pages = {3310-3315},
  file = {/Users/jonny/Papers/GoldB/2019/Gold_2019_Musical reward prediction errors engage the nucleus accumbens and motivate.pdf;/Users/jonny/Zotero/storage/5A63W2N8/3310.html},
  pmid = {30728301}
}

@article{goldMusicalRewardPrediction2019a,
  title = {Musical Reward Prediction Errors Engage the Nucleus Accumbens and Motivate Learning},
  volume = {116},
  copyright = {\textcopyright{} 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  abstract = {Enjoying music reliably ranks among life's greatest pleasures. Like many hedonic experiences, it engages several reward-related brain areas, with activity in the nucleus accumbens (NAc) most consistently reflecting the listener's subjective response. Converging evidence suggests that this activity arises from musical ``reward prediction errors'' (RPEs) that signal the difference between expected and perceived musical events, but this hypothesis has not been directly tested. In the present fMRI experiment, we assessed whether music could elicit formally modeled RPEs in the NAc by applying a well-established decision-making protocol designed and validated for studying RPEs. In the scanner, participants chose between arbitrary cues that probabilistically led to dissonant or consonant music, and learned to make choices associated with the consonance, which they preferred. We modeled regressors of trial-by-trial RPEs, finding that NAc activity tracked musically elicited RPEs, to an extent that explained variance in the individual learning rates. These results demonstrate that music can act as a reward, driving learning and eliciting RPEs in the NAc, a hub of reward- and music enjoyment-related activity.},
  language = {en},
  number = {8},
  urldate = {2019-07-16},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1809855116},
  url = {https://www.pnas.org/content/116/8/3310},
  author = {Gold, Benjamin P. and {Mas-Herrero}, Ernest and Zeighami, Yashar and Benovoy, Mitchel and Dagher, Alain and Zatorre, Robert J.},
  month = feb,
  year = {2019},
  keywords = {fMRI,nucleus accumbens,music,abstract reward,reward prediction errors},
  pages = {3310-3315},
  file = {/Users/jonny/Papers/GoldB/2019/Gold_2019_Musical reward prediction errors engage the nucleus accumbens and motivate2.pdf;/Users/jonny/Zotero/storage/TUAVBSW2/3310.html},
  pmid = {30728301}
}

@article{balkwillCrossCulturalInvestigationPerception1999,
  title = {A {{Cross}}-{{Cultural Investigation}} of the {{Perception}} of {{Emotion}} in {{Music}}: {{Psychophysical}} and {{Cultural Cues}}},
  volume = {17},
  issn = {0730-7829, 1533-8312},
  shorttitle = {A {{Cross}}-{{Cultural Investigation}} of the {{Perception}} of {{Emotion}} in {{Music}}},
  abstract = {Skip to Next Section
Studies of the link between music and emotion have primarily focused on listeners' sensitivity to emotion in the music of their own culture. This sensitivity may reflect listeners' enculturation to the conventions of their culture's tonal system. However, it may also reflect responses to psychophysical dimensions of sound that are independent of musical experience. A model of listeners' perception of emotion in music is proposed in which emotion in music is communicated through a combination of universal and cultural cues. Listeners may rely on either of these cues, or both, to arrive at an understanding of musically expressed emotion. The current study addressed the hypotheses derived from this model using a cross-cultural approach. The following questions were investigated: Can people identify the intended emotion in music from an unfamiliar tonal system? If they can, is their sensitivity to intended emotions associated with perceived changes in psychophysical dimensions of music? Thirty Western listeners rated the degree of joy, sadness, anger, and peace in 12 Hindustani raga excerpts (field recordings obtained in North India). In accordance with the raga-rasa system, each excerpt was intended to convey one of the four moods or "rasas" that corresponded to the four emotions rated by listeners. Listeners also provided ratings of four psychophysical variables: tempo, rhythmic complexity, melodic complexity, and pitch range. Listeners were sensitive to the intended emotion in ragas when that emotion was joy, sadness, or anger. Judgments of emotion were significantly related to judgments of psychophysical dimensions, and, in some cases, to instrument timbre. The findings suggest that listeners are sensitive to musically expressed emotion in an unfamiliar tonal system, and that this sensitivity is facilitated by psychophysical cues.},
  language = {en},
  number = {1},
  urldate = {2019-07-16},
  journal = {Music Perception: An Interdisciplinary Journal},
  doi = {10.2307/40285811},
  url = {https://mp.ucpress.edu/content/17/1/43},
  author = {Balkwill, Laura-Lee and Thompson, William Forde},
  month = oct,
  year = {1999},
  pages = {43-64},
  file = {/Users/jonny/Zotero/storage/G9VXNKV8/43.html}
}

@article{juslinCommunicationEmotionsVocal2003,
  title = {Communication of Emotions in Vocal Expression and Music Performance: Different Channels, Same Code?},
  volume = {129},
  issn = {0033-2909},
  shorttitle = {Communication of Emotions in Vocal Expression and Music Performance},
  abstract = {Many authors have speculated about a close relationship between vocal expression of emotions and musical expression of emotions. but evidence bearing on this relationship has unfortunately been lacking. This review of 104 studies of vocal expression and 41 studies of music performance reveals similarities between the 2 channels concerning (a) the accuracy with which discrete emotions were communicated to listeners and (b) the emotion-specific patterns of acoustic cues used to communicate each emotion. The patterns are generally consistent with K. R. Scherer's (1986) theoretical predictions. The results can explain why music is perceived as expressive of emotion, and they are consistent with an evolutionary perspective on vocal expression of emotions. Discussion focuses on theoretical accounts and directions for future research.},
  language = {eng},
  number = {5},
  journal = {Psychological Bulletin},
  doi = {10.1037/0033-2909.129.5.770},
  author = {Juslin, Patrik N. and Laukka, Petri},
  month = sep,
  year = {2003},
  keywords = {Humans,Music,Cues,Affect,Voice,Communication,Psychological Theory,Nonverbal Communication},
  pages = {770-814},
  pmid = {12956543}
}

@article{juslinEmotionalResponsesMusic2008,
  title = {Emotional Responses to Music: The Need to Consider Underlying Mechanisms},
  volume = {31},
  issn = {1469-1825},
  shorttitle = {Emotional Responses to Music},
  abstract = {Research indicates that people value music primarily because of the emotions it evokes. Yet, the notion of musical emotions remains controversial, and researchers have so far been unable to offer a satisfactory account of such emotions. We argue that the study of musical emotions has suffered from a neglect of underlying mechanisms. Specifically, researchers have studied musical emotions without regard to how they were evoked, or have assumed that the emotions must be based on the "default" mechanism for emotion induction, a cognitive appraisal. Here, we present a novel theoretical framework featuring six additional mechanisms through which music listening may induce emotions: (1) brain stem reflexes, (2) evaluative conditioning, (3) emotional contagion, (4) visual imagery, (5) episodic memory, and (6) musical expectancy. We propose that these mechanisms differ regarding such characteristics as their information focus, ontogenetic development, key brain regions, cultural impact, induction speed, degree of volitional influence, modularity, and dependence on musical structure. By synthesizing theory and findings from different domains, we are able to provide the first set of hypotheses that can help researchers to distinguish among the mechanisms. We show that failure to control for the underlying mechanism may lead to inconsistent or non-interpretable findings. Thus, we argue that the new framework may guide future research and help to resolve previous disagreements in the field. We conclude that music evokes emotions through mechanisms that are not unique to music, and that the study of musical emotions could benefit the emotion field as a whole by providing novel paradigms for emotion induction.},
  language = {eng},
  number = {5},
  journal = {The Behavioral and Brain Sciences},
  doi = {10.1017/S0140525X08005293},
  author = {Juslin, Patrik N. and V{\"a}stfj{\"a}ll, Daniel},
  month = oct,
  year = {2008},
  keywords = {Nerve Net,Humans,Psychoacoustics,Brain Stem,Attention,Conditioning (Psychology),Arousal,Auditory Perception,Amygdala,Music,Brain,Hypothalamus,Reflex,Frontal Lobe,Emotions,Psychological Theory,Imagination,Mental Recall,Set (Psychology)},
  pages = {559-575; discussion 575-621},
  pmid = {18826699}
}

@article{vuoskoskiMeasuringMusicinducedEmotion2011,
  title = {Measuring Music-Induced Emotion: {{A}} Comparison of Emotion Models, Personality Biases, and Intensity of Experiences},
  volume = {15},
  issn = {1029-8649},
  shorttitle = {Measuring Music-Induced Emotion},
  abstract = {Most previous studies investigating music-induced emotions have applied emotion models developed in other fields to the domain of music. The aim of this study was to compare the applicability of music-specific and general emotion models \textendash{} namely the Geneva Emotional Music Scale (GEMS), and the discrete and dimensional emotion models \textendash{} in the assessment of music-induced emotions. A related aim was to explore the role of individual difference variables (such as personality and mood) in music-induced emotions, and to discover whether some emotion models reflect these individual differences more strongly than others. One hundred and forty-eight participants listened to 16 film music excerpts and rated the emotional responses evoked by the music excerpts. Intraclass correlations and Cronbach alphas revealed that the overall consistency of ratings was the highest in the case of the dimensional model. The dimensional model also outperformed the other two models in the discrimination of music excerpts, and principal component analysis revealed that 89.9\% of the variance in the mean ratings of all the scales (in all three models) was accounted for by two principal components that could be labelled as valence and arousal. Personality-related differences were the most pronounced in the case of the discrete emotion model. Personality, mood, and the emotion model used were also associated with the intensity of experienced emotions. Implications for future music and emotion studies are raised concerning the selection of an appropriate emotion model when measuring music-induced emotions.},
  language = {en},
  number = {2},
  urldate = {2019-07-17},
  journal = {Musicae Scientiae},
  doi = {10.1177/1029864911403367},
  url = {https://doi.org/10.1177/1029864911403367},
  author = {Vuoskoski, Jonna K. and Eerola, Tuomas},
  month = jul,
  year = {2011},
  pages = {159-173},
  file = {/Users/jonny/Papers/VuoskoskiJ/2011/Vuoskoski_2011_Measuring music-induced emotion.pdf}
}

@article{juslinExpressionPerceptionInduction2004,
  title = {Expression, {{Perception}}, and {{Induction}} of {{Musical Emotions}}: {{A Review}} and a {{Questionnaire Study}} of {{Everyday Listening}}},
  volume = {33},
  issn = {0929-8215},
  shorttitle = {Expression, {{Perception}}, and {{Induction}} of {{Musical Emotions}}},
  abstract = {In this article, we provide an up-to-date overview of theory and research concerning expression, perception, and induction of emotion in music. We also provide a critique of this research, noting that previous studies have tended to neglect the social context of music listening. The most likely reason for this neglect, we argue, is that that most research on musical emotion has, implicitly or explicitly, taken the perspective of the musician in understanding responses to music. In contrast, we argue that a promising avenue toward a better understanding of emotional responses to music involves diary and questionnaire studies of how ordinary listeners actually use music in everyday life contexts. Accordingly, we present findings from an exploratory questionnaire study featuring 141 music listeners (between 17 and 74 years of age) that offers some novel insights. The results provide preliminary estimates of the occurrence of various emotions in listening to music, as well as clues to how music is used by listeners in a number of different emotional ways in various life contexts. These results confirm that emotion is strongly related to most people's primary motives for listening to music.},
  number = {3},
  urldate = {2019-07-17},
  journal = {Journal of New Music Research},
  doi = {10.1080/0929821042000317813},
  url = {https://doi.org/10.1080/0929821042000317813},
  author = {Juslin, Patrik N. and Laukka, Petri},
  month = sep,
  year = {2004},
  pages = {217-238},
  file = {/Users/jonny/Papers/JuslinP/2004/Juslin_2004_Expression, Perception, and Induction of Musical Emotions.pdf;/Users/jonny/Zotero/storage/D5LXC686/0929821042000317813.html}
}

@article{pulverFriedrichWilhelmMarpurg1912,
  title = {Friedrich {{Wilhelm Marpurg}}},
  volume = {53},
  issn = {0027-4666},
  number = {832},
  urldate = {2019-07-17},
  journal = {The Musical Times},
  doi = {10.2307/907328},
  url = {https://www.jstor.org/stable/907328},
  author = {Pulver, Jeffrey},
  year = {1912},
  pages = {375-377},
  file = {/Users/jonny/Papers/PulverJ/1912/Pulver_1912_Friedrich Wilhelm Marpurg.pdf}
}

@article{zentnerEmotionsEvokedSound2008,
  title = {Emotions Evoked by the Sound of Music: {{Characterization}}, Classification, and Measurement.},
  volume = {8},
  issn = {1931-1516, 1528-3542},
  shorttitle = {Emotions Evoked by the Sound of Music},
  abstract = {One reason for the universal appeal of music lies in the emotional rewards that music offers to its listeners. But what makes these rewards so special? The authors addressed this question by progressively characterizing music-induced emotions in 4 interrelated studies. Studies 1 and 2 (n Ï­ 354) were conducted to compile a list of music-relevant emotion terms and to study the frequency of both felt and perceived emotions across 5 groups of listeners with distinct music preferences. Emotional responses varied greatly according to musical genre and type of response (felt vs. perceived). Study 3 (n Ï­ 801)\textemdash{}a field study carried out during a music festival\textemdash{} examined the structure of music-induced emotions via confirmatory factor analysis of emotion ratings, resulting in a 9-factorial model of music-induced emotions. Study 4 (n Ï­ 238) replicated this model and found that it accounted for music-elicited emotions better than the basic emotion and dimensional emotion models. A domain-specific device to measure musically induced emotions is introduced\textemdash{}the Geneva Emotional Music Scale.},
  language = {en},
  number = {4},
  urldate = {2019-07-17},
  journal = {Emotion},
  doi = {10.1037/1528-3542.8.4.494},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1528-3542.8.4.494},
  author = {Zentner, Marcel and Grandjean, Didier and Scherer, Klaus R.},
  year = {2008},
  pages = {494-521},
  file = {/Users/jonny/Zotero/storage/4G2BGAWN/Zentner et al. - 2008 - Emotions evoked by the sound of music Characteriz.pdf}
}

@article{vuoskoskiMeasuringMusicinducedEmotion2011a,
  title = {Measuring Music-Induced Emotion: {{A}} Comparison of Emotion Models, Personality Biases, and Intensity of Experiences},
  volume = {15},
  issn = {1029-8649},
  shorttitle = {Measuring Music-Induced Emotion},
  abstract = {Most previous studies investigating music-induced emotions have applied emotion models developed in other fields to the domain of music. The aim of this study was to compare the applicability of music-specific and general emotion models \textendash{} namely the Geneva Emotional Music Scale (GEMS), and the discrete and dimensional emotion models \textendash{} in the assessment of music-induced emotions. A related aim was to explore the role of individual difference variables (such as personality and mood) in music-induced emotions, and to discover whether some emotion models reflect these individual differences more strongly than others. One hundred and forty-eight participants listened to 16 film music excerpts and rated the emotional responses evoked by the music excerpts. Intraclass correlations and Cronbach alphas revealed that the overall consistency of ratings was the highest in the case of the dimensional model. The dimensional model also outperformed the other two models in the discrimination of music excerpts, and principal component analysis revealed that 89.9\% of the variance in the mean ratings of all the scales (in all three models) was accounted for by two principal components that could be labelled as valence and arousal. Personality-related differences were the most pronounced in the case of the discrete emotion model. Personality, mood, and the emotion model used were also associated with the intensity of experienced emotions. Implications for future music and emotion studies are raised concerning the selection of an appropriate emotion model when measuring music-induced emotions.},
  language = {en},
  number = {2},
  urldate = {2019-07-17},
  journal = {Musicae Scientiae},
  doi = {10.1177/1029864911403367},
  url = {https://doi.org/10.1177/1029864911403367},
  author = {Vuoskoski, Jonna K. and Eerola, Tuomas},
  month = jul,
  year = {2011},
  pages = {159-173},
  file = {/Users/jonny/Papers/VuoskoskiJ/2011/Vuoskoski_2011_Measuring music-induced emotion2.pdf}
}

@article{trostMappingAestheticMusical2012,
  title = {Mapping {{Aesthetic Musical Emotions}} in the {{Brain}}},
  volume = {22},
  issn = {1047-3211},
  abstract = {Abstract.  Music evokes complex emotions beyond pleasant/unpleasant or happy/sad dichotomies usually investigated in neuroscience. Here, we used functional neur},
  language = {en},
  number = {12},
  urldate = {2019-07-17},
  journal = {Cerebral Cortex},
  doi = {10.1093/cercor/bhr353},
  url = {https://academic.oup.com/cercor/article/22/12/2769/304366},
  author = {Trost, Wiebke and Ethofer, Thomas and Zentner, Marcel and Vuilleumier, Patrik},
  month = dec,
  year = {2012},
  pages = {2769-2783},
  file = {/Users/jonny/Papers/TrostW/2012/Trost_2012_Mapping Aesthetic Musical Emotions in the Brain.pdf;/Users/jonny/Zotero/storage/X7DVL8Z9/304366.html}
}

@article{egermannMusicInducesUniversal2015,
  title = {Music Induces Universal Emotion-Related Psychophysiological Responses: Comparing {{Canadian}} Listeners to {{Congolese Pygmies}}},
  volume = {5},
  issn = {1664-1078},
  shorttitle = {Music Induces Universal Emotion-Related Psychophysiological Responses},
  abstract = {Subjective and psychophysiological emotional responses to music from two different cultures were compared within these two cultures. Two identical experiments were conducted: the first in the Congolese rainforest with an isolated population of Mbenz{\'e}l{\'e} Pygmies without any exposure to Western music and culture, the second with a group of Western music listeners, with no experience with Congolese music. Forty Pygmies and 40 Canadians listened in pairs to 19 music excerpts of 29 to 99 seconds in duration in random order (8 from the Pygmy population and 11 Western instrumental excerpts). For both groups, emotion components were continuously measured: subjective feeling (using a two- dimensional valence and arousal rating interface), peripheral physiological activation, and facial expression. While Pygmy music was rated as positive and arousing by Pygmies, ratings of Western music by Westerners covered the range from arousing to calming and from positive to negative. Comparing psychophysiological responses to emotional qualities of Pygmy music across participant groups showed no similarities. However, Western stimuli, rated as high and low arousing by Canadians, created similar responses in both participant groups (with high arousal associated with increases in subjective and physiological activation). Several low-level acoustical features of the music presented (tempo, pitch, and timbre) were shown to affect subjective and physiological arousal similarly in both cultures. Results suggest that while the subjective dimension of emotional valence might be mediated by cultural learning, changes in arousal might involve a more basic, universal response to low-level acoustical characteristics of music.},
  language = {English},
  urldate = {2019-07-17},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2014.01341},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.01341/full},
  author = {Egermann, Hauke and Fernando, Nathalie and Chuen, Lorraine and McAdams, Stephen},
  year = {2015},
  keywords = {Music,Affect,Psychophysiology,emotion,Cross-cultural,Universal},
  file = {/Users/jonny/Papers/EgermannH/2015/Egermann_2015_Music induces universal emotion-related psychophysiological responses.pdf}
}

@article{aljanakiDatadrivenApproachMidlevel2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.04903},
  primaryClass = {cs, eess},
  title = {A Data-Driven Approach to Mid-Level Perceptual Musical Feature Modeling},
  abstract = {Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset\textbackslash{}footnote\{https://osf.io/5aupt/\} of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application.},
  urldate = {2019-07-17},
  journal = {arXiv:1806.04903 [cs, eess]},
  url = {http://arxiv.org/abs/1806.04903},
  author = {Aljanaki, Anna and Soleymani, Mohammad},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/jonny/Papers/AljanakiA/2018/Aljanaki_2018_A data-driven approach to mid-level perceptual musical feature modeling.pdf;/Users/jonny/Zotero/storage/ESEKWPRU/1806.html}
}

@article{stevensMusicPerceptionCognition2012,
  title = {Music {{Perception}} and {{Cognition}}: {{A Review}} of {{Recent Cross}}-{{Cultural Research}}},
  volume = {4},
  copyright = {Copyright \textcopyright{} 2012 Cognitive Science Society, Inc.},
  issn = {1756-8765},
  shorttitle = {Music {{Perception}} and {{Cognition}}},
  abstract = {Experimental investigations of cross-cultural music perception and cognition reported during the past decade are described. As globalization and Western music homogenize the world musical environment, it is imperative that diverse music and musical contexts are documented. Processes of music perception include grouping and segmentation, statistical learning and sensitivity to tonal and temporal hierarchies, and the development of tonal and temporal expectations. The interplay of auditory, visual, and motor modalities is discussed in light of synchronization and the way music moves via emotional response. Further research is needed to test deep-rooted psychological assumptions about music cognition with diverse materials and groups in dynamic contexts. Although empirical musicology provides keystones to unlock musical structures and organization, the psychological reality of those theorized structures for listeners and performers, and the broader implications for theories of music perception and cognition, awaits investigation.},
  language = {en},
  number = {4},
  urldate = {2019-07-17},
  journal = {Topics in Cognitive Science},
  doi = {10.1111/j.1756-8765.2012.01215.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2012.01215.x},
  author = {Stevens, Catherine J.},
  year = {2012},
  keywords = {Pitch,Rhythm,Empirical cognitive ethnomusicology,Entrainment,Expectations,Meter,Multimodal perception,Universals},
  pages = {653-667},
  file = {/Users/jonny/Papers/StevensC/2012/Stevens_2012_Music Perception and Cognition.pdf;/Users/jonny/Zotero/storage/PEUUAFMT/j.1756-8765.2012.01215.html}
}

@article{russellThereUniversalRecognition1994,
  title = {Is There Universal Recognition of Emotion from Facial Expression? {{A}} Review of the Cross-Cultural Studies},
  volume = {115},
  issn = {0033-2909},
  shorttitle = {Is There Universal Recognition of Emotion from Facial Expression?},
  abstract = {Emotions are universally recognized from facial expressions--or so it has been claimed. To support that claim, research has been carried out in various modern cultures and in cultures relatively isolated from Western influence. A review of the methods used in that research raises questions of its ecological, convergent, and internal validity. Forced-choice response format, within-subject design, preselected photographs of posed facial expressions, and other features of method are each problematic. When they are altered, less supportive or nonsupportive results occur. When they are combined, these method factors may help to shape the results. Facial expressions and emotion labels are probably associated, but the association may vary with culture and is loose enough to be consistent with various alternative accounts, 8 of which are discussed.},
  language = {eng},
  number = {1},
  journal = {Psychological Bulletin},
  author = {Russell, J. A.},
  month = jan,
  year = {1994},
  keywords = {Humans,Discrimination (Psychology),Visual Perception,Emotions,Facial Expression,Judgment,Cross-Cultural Comparison,Research Design},
  pages = {102-141},
  pmid = {8202574}
}

@article{laukkaUniversalCulturespecificFactors2013,
  title = {Universal and Culture-Specific Factors in the Recognition and Performance of Musical Affect Expressions},
  volume = {13},
  issn = {1931-1516},
  abstract = {We present a cross-cultural study on the performance and perception of affective expression in music. Professional bowed-string musicians from different musical traditions (Swedish folk music, Hindustani classical music, Japanese traditional music, and Western classical music) were instructed to perform short pieces of music to convey 11 emotions and related states to listeners. All musical stimuli were judged by Swedish, Indian, and Japanese participants in a balanced design, and a variety of acoustic and musical cues were extracted. Results first showed that the musicians' expressive intentions could be recognized with accuracy above chance both within and across musical cultures, but communication was, in general, more accurate for culturally familiar versus unfamiliar music, and for basic emotions versus nonbasic affective states. We further used a lens-model approach to describe the relations between the strategies that musicians use to convey various expressions and listeners' perceptions of the affective content of the music. Many acoustic and musical cues were similarly correlated with both the musicians' expressive intentions and the listeners' affective judgments across musical cultures, but the match between musicians' and listeners' uses of cues was better in within-cultural versus cross-cultural conditions. We conclude that affective expression in music may depend on a combination of universal and culture-specific factors.},
  language = {eng},
  number = {3},
  journal = {Emotion (Washington, D.C.)},
  doi = {10.1037/a0031388},
  author = {Laukka, Petri and Eerola, Tuomas and Thingujam, Nutankumar S. and Yamasaki, Teruo and Beller, Gr{\'e}gory},
  month = jun,
  year = {2013},
  keywords = {Humans,Female,Auditory Perception,Male,Music,Cues,Adult,Emotions,Judgment,Cross-Cultural Comparison,Culture,India,Japan,Social Perception,Sweden},
  pages = {434-449},
  pmid = {23398579}
}

@article{lippiMusicMedicine2010,
  title = {Music and Medicine},
  volume = {3},
  issn = {1178-2390},
  abstract = {Healing sounds have always been considered in the past an important aid in medical practice, and nowadays, medicine has confirmed the efficacy of music therapy in many diseases. The aim of this study is to assess the curative power of music, in the frame of the current clinical relationship.},
  urldate = {2019-07-18},
  journal = {Journal of multidisciplinary healthcare},
  doi = {10.2147/JMDH.S11378},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3004608/},
  author = {Lippi, Donatella and {Roberti di Sarsina}, Paolo and D'Elios, John Patrick},
  month = aug,
  year = {2010},
  pages = {137-141},
  file = {/Users/jonny/Papers/LippiD/2010/Lippi_2010_Music and medicine.pdf},
  pmid = {21197362},
  pmcid = {PMC3004608}
}

@article{fruhholzRoleMedialTemporal2014,
  title = {The Role of the Medial Temporal Limbic System in Processing Emotions in Voice and Music},
  volume = {123},
  issn = {0301-0082},
  abstract = {Subcortical brain structures of the limbic system, such as the amygdala, are thought to decode the emotional value of sensory information. Recent neuroimaging studies, as well as lesion studies in patients, have shown that the amygdala is sensitive to emotions in voice and music. Similarly, the hippocampus, another part of the temporal limbic system (TLS), is responsive to vocal and musical emotions, but its specific roles in emotional processing from music and especially from voices have been largely neglected. Here we review recent research on vocal and musical emotions, and outline commonalities and differences in the neural processing of emotions in the TLS in terms of emotional valence, emotional intensity and arousal, as well as in terms of acoustic and structural features of voices and music. We summarize the findings in a neural framework including several subcortical and cortical functional pathways between the auditory system and the TLS. This framework proposes that some vocal expressions might already receive a fast emotional evaluation via a subcortical pathway to the amygdala, whereas cortical pathways to the TLS are thought to be equally used for vocal and musical emotions. While the amygdala might be specifically involved in a coarse decoding of the emotional value of voices and music, the hippocampus might process more complex vocal and musical emotions, and might have an important role especially for the decoding of musical emotions by providing memory-based and contextual associations.},
  urldate = {2019-07-24},
  journal = {Progress in Neurobiology},
  doi = {10.1016/j.pneurobio.2014.09.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0301008214001038},
  author = {Fr{\"u}hholz, Sascha and Trost, Wiebke and Grandjean, Didier},
  month = dec,
  year = {2014},
  keywords = {Hippocampus,Emotion,Amygdala,Music,Voice,Neuroimaging},
  pages = {1-17},
  file = {/Users/jonny/Zotero/storage/K5KP7J3P/FrÃ¼hholz et al. - 2014 - The role of the medial temporal limbic system in p.pdf;/Users/jonny/Zotero/storage/QPTAMVLX/S0301008214001038.html}
}

@article{litovskyDevelopmentAuditorySystem2015,
  title = {Development of the Auditory System},
  volume = {129},
  issn = {0072-9752},
  abstract = {Auditory development involves changes in the peripheral and central nervous system along the auditory pathways, and these occur naturally, and in response to stimulation. Human development occurs along a trajectory that can last decades, and is studied using behavioral psychophysics, as well as physiologic measurements with neural imaging. The auditory system constructs a perceptual space that takes information from objects and groups, segregates sounds, and provides meaning and access to communication tools such as language. Auditory signals are processed in a series of analysis stages, from peripheral to central. Coding of information has been studied for features of sound, including frequency, intensity, loudness, and location, in quiet and in the presence of maskers. In the latter case, the ability of the auditory system to perform an analysis of the scene becomes highly relevant. While some basic abilities are well developed at birth, there is a clear prolonged maturation of auditory development well into the teenage years. Maturation involves auditory pathways. However, non-auditory changes (attention, memory, cognition) play an important role in auditory development. The ability of the auditory system to adapt in response to novel stimuli is a key feature of development throughout the nervous system, known as neural plasticity.},
  urldate = {2019-07-24},
  journal = {Handbook of clinical neurology},
  doi = {10.1016/B978-0-444-62630-1.00003-2},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4612629/},
  author = {Litovsky, Ruth},
  year = {2015},
  pages = {55-72},
  file = {/Users/jonny/Zotero/storage/BD462ZZV/Litovsky - 2015 - Development of the auditory system.pdf},
  pmid = {25726262},
  pmcid = {PMC4612629}
}


